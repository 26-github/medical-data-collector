import os
import uuid
import logging
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
from PIL import Image
import fitz
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
import base64
import boto3
from botocore.exceptions import ClientError, NoCredentialsError
import json
import hashlib
from datetime import datetime, timedelta
import pickle
import io

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("MedicalAttachmentProcessor")

DASHSCOPE_API_KEY=os.getenv("DASHSCOPE_API_KEY")

# åŒ»ç–—é™„ä»¶ç¼“å­˜ç®¡ç†å™¨
class MedicalAttachmentCache:
    def __init__(self, cache_dir: str = "medical_cache", max_cache_size: int = 100, cache_expiry_hours: int = 24):
        """
        åŒ»ç–—é™„ä»¶åˆ†æç»“æœç¼“å­˜ç®¡ç†å™¨
        
        Args:
            cache_dir: ç¼“å­˜ç›®å½•è·¯å¾„
            max_cache_size: æœ€å¤§ç¼“å­˜æ¡ç›®æ•°é‡
            cache_expiry_hours: ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆå°æ—¶ï¼‰
        """
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.max_cache_size = max_cache_size
        self.cache_expiry_hours = cache_expiry_hours
        self.cache_index_file = self.cache_dir / "cache_index.json"
        self.cache_index = self._load_cache_index()
        
        # å¯åŠ¨æ—¶æ¸…ç†è¿‡æœŸç¼“å­˜
        self._cleanup_expired_cache()
        
    def _load_cache_index(self) -> Dict[str, Dict[str, Any]]:
        """åŠ è½½ç¼“å­˜ç´¢å¼•"""
        try:
            if self.cache_index_file.exists():
                with open(self.cache_index_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            logger.warning(f"åŠ è½½ç¼“å­˜ç´¢å¼•å¤±è´¥: {e}")
        return {}
    
    def _save_cache_index(self):
        """ä¿å­˜ç¼“å­˜ç´¢å¼•"""
        try:
            with open(self.cache_index_file, 'w', encoding='utf-8') as f:
                json.dump(self.cache_index, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"ä¿å­˜ç¼“å­˜ç´¢å¼•å¤±è´¥: {e}")
    
    def _generate_cache_key(self, user_ip: str, filename: str, file_size: int = None) -> str:
        """
        ç”Ÿæˆç¼“å­˜é”®å€¼
        
        Args:
            user_ip: ç”¨æˆ·IP
            filename: æ–‡ä»¶å
            file_size: æ–‡ä»¶å¤§å°ï¼ˆå¯é€‰ï¼Œç”¨äºæ›´ç²¾ç¡®çš„ç¼“å­˜é”®ï¼‰
            
        Returns:
            ç¼“å­˜é”®å€¼çš„MD5å“ˆå¸Œ
        """
        # ä½¿ç”¨ç”¨æˆ·IPã€æ–‡ä»¶åå’Œæ–‡ä»¶å¤§å°ç”Ÿæˆå”¯ä¸€é”®å€¼
        cache_string = f"{user_ip}_{filename}"
        if file_size:
            cache_string += f"_{file_size}"
        return hashlib.md5(cache_string.encode('utf-8')).hexdigest()
    
    def _cleanup_expired_cache(self):
        """æ¸…ç†è¿‡æœŸçš„ç¼“å­˜æ¡ç›®"""
        current_time = datetime.now()
        expired_keys = []
        
        for cache_key, cache_info in self.cache_index.items():
            cache_time = datetime.fromisoformat(cache_info['created_at'])
            if current_time - cache_time > timedelta(hours=self.cache_expiry_hours):
                expired_keys.append(cache_key)
        
        for key in expired_keys:
            self._remove_cache_entry(key)
        
        if expired_keys:
            logger.info(f"æ¸…ç†äº† {len(expired_keys)} ä¸ªè¿‡æœŸç¼“å­˜æ¡ç›®")
    
    def _remove_cache_entry(self, cache_key: str):
        """åˆ é™¤ç¼“å­˜æ¡ç›®"""
        try:
            if cache_key in self.cache_index:
                cache_file = self.cache_dir / f"{cache_key}.pkl"
                if cache_file.exists():
                    cache_file.unlink()
                del self.cache_index[cache_key]
                logger.debug(f"åˆ é™¤ç¼“å­˜æ¡ç›®: {cache_key}")
        except Exception as e:
            logger.warning(f"åˆ é™¤ç¼“å­˜æ¡ç›®å¤±è´¥ {cache_key}: {e}")
    
    def _enforce_cache_size_limit(self):
        """å¼ºåˆ¶æ‰§è¡Œç¼“å­˜å¤§å°é™åˆ¶"""
        if len(self.cache_index) <= self.max_cache_size:
            return
        
        # æŒ‰åˆ›å»ºæ—¶é—´æ’åºï¼Œåˆ é™¤æœ€æ—§çš„æ¡ç›®
        sorted_entries = sorted(
            self.cache_index.items(),
            key=lambda x: x[1]['created_at']
        )
        
        entries_to_remove = len(self.cache_index) - self.max_cache_size
        for i in range(entries_to_remove):
            cache_key = sorted_entries[i][0]
            self._remove_cache_entry(cache_key)
        
        logger.info(f"å¼ºåˆ¶æ¸…ç†äº† {entries_to_remove} ä¸ªæ—§ç¼“å­˜æ¡ç›®ä»¥æ§åˆ¶ç¼“å­˜å¤§å°")
    
    def get_cached_analysis(self, user_ip: str, filename: str, file_size: int = None) -> Optional[Dict[str, Any]]:
        """
        è·å–ç¼“å­˜çš„åˆ†æç»“æœ
        
        Args:
            user_ip: ç”¨æˆ·IP
            filename: æ–‡ä»¶å
            file_size: æ–‡ä»¶å¤§å°
            
        Returns:
            ç¼“å­˜çš„åˆ†æç»“æœæˆ–None
        """
        cache_key = self._generate_cache_key(user_ip, filename, file_size)
        
        if cache_key not in self.cache_index:
            return None
        
        try:
            cache_file = self.cache_dir / f"{cache_key}.pkl"
            if not cache_file.exists():
                # ç´¢å¼•å­˜åœ¨ä½†æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ¸…ç†æ— æ•ˆç´¢å¼•
                del self.cache_index[cache_key]
                self._save_cache_index()
                return None
            
            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            cache_info = self.cache_index[cache_key]
            cache_time = datetime.fromisoformat(cache_info['created_at'])
            if datetime.now() - cache_time > timedelta(hours=self.cache_expiry_hours):
                self._remove_cache_entry(cache_key)
                self._save_cache_index()
                return None
            
            # åŠ è½½ç¼“å­˜æ•°æ®
            with open(cache_file, 'rb') as f:
                cached_data = pickle.load(f)
            
            # æ›´æ–°è®¿é—®æ—¶é—´
            self.cache_index[cache_key]['last_accessed'] = datetime.now().isoformat()
            self._save_cache_index()
            
            logger.info(f"âœ… å‘½ä¸­åŒ»ç–—é™„ä»¶ç¼“å­˜: {filename} (ç”¨æˆ·: {user_ip})")
            return cached_data
            
        except Exception as e:
            logger.warning(f"è¯»å–ç¼“å­˜å¤±è´¥ {cache_key}: {e}")
            self._remove_cache_entry(cache_key)
            self._save_cache_index()
            return None
    
    def cache_analysis_result(self, user_ip: str, filename: str, analysis_result: Dict[str, Any], file_size: int = None):
        """
        ç¼“å­˜åˆ†æç»“æœ
        
        Args:
            user_ip: ç”¨æˆ·IP
            filename: æ–‡ä»¶å
            analysis_result: åˆ†æç»“æœ
            file_size: æ–‡ä»¶å¤§å°
        """
        try:
            cache_key = self._generate_cache_key(user_ip, filename, file_size)
            cache_file = self.cache_dir / f"{cache_key}.pkl"
            
            # ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶
            with open(cache_file, 'wb') as f:
                pickle.dump(analysis_result, f)
            
            # æ›´æ–°ç´¢å¼•
            self.cache_index[cache_key] = {
                'user_ip': user_ip,
                'filename': filename,
                'file_size': file_size,
                'created_at': datetime.now().isoformat(),
                'last_accessed': datetime.now().isoformat(),
                'cache_file': f"{cache_key}.pkl"
            }
            
            # å¼ºåˆ¶æ‰§è¡Œç¼“å­˜å¤§å°é™åˆ¶
            self._enforce_cache_size_limit()
            
            # ä¿å­˜ç´¢å¼•
            self._save_cache_index()
            
            logger.info(f"ğŸ“ åŒ»ç–—é™„ä»¶åˆ†æç»“æœå·²ç¼“å­˜: {filename} (ç”¨æˆ·: {user_ip})")
            
        except Exception as e:
            logger.error(f"ç¼“å­˜åˆ†æç»“æœå¤±è´¥: {e}")
    
    def clear_user_cache(self, user_ip: str):
        """æ¸…ç†ç‰¹å®šç”¨æˆ·çš„æ‰€æœ‰ç¼“å­˜"""
        removed_count = 0
        keys_to_remove = []
        
        for cache_key, cache_info in self.cache_index.items():
            if cache_info.get('user_ip') == user_ip:
                keys_to_remove.append(cache_key)
        
        for key in keys_to_remove:
            self._remove_cache_entry(key)
            removed_count += 1
        
        self._save_cache_index()
        logger.info(f"æ¸…ç†ç”¨æˆ· {user_ip} çš„ {removed_count} ä¸ªç¼“å­˜æ¡ç›®")
        return removed_count
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        total_entries = len(self.cache_index)
        total_size = sum(
            (self.cache_dir / f"{key}.pkl").stat().st_size 
            for key in self.cache_index.keys()
            if (self.cache_dir / f"{key}.pkl").exists()
        )
        
        return {
            'total_entries': total_entries,
            'total_size_mb': round(total_size / (1024 * 1024), 2),
            'cache_dir': str(self.cache_dir),
            'max_cache_size': self.max_cache_size,
            'cache_expiry_hours': self.cache_expiry_hours
        }

# S3å®¢æˆ·ç«¯é…ç½®
class S3Config:
    def __init__(self):
        # ä»ç¯å¢ƒå˜é‡è¯»å–S3é…ç½®ï¼Œå¦‚æœæ²¡æœ‰è®¾ç½®åˆ™ä½¿ç”¨é»˜è®¤å€¼
        self.endpoint_url = os.getenv("S3_ENDPOINT_URL", "http://154.89.148.156:9000")
        self.access_key = os.getenv("S3_ACCESS_KEY", )
        self.secret_key = os.getenv("S3_SECRET_KEY",)
        self.region_name = os.getenv("S3_REGION", "us-east-1")  # MinIOé€šå¸¸ä½¿ç”¨è¿™ä¸ªé»˜è®¤å€¼
        
    def get_s3_client(self):
        """è·å–é…ç½®å¥½çš„S3å®¢æˆ·ç«¯"""
        return boto3.client(
            's3',
            endpoint_url=self.endpoint_url,
            aws_access_key_id=self.access_key,
            aws_secret_access_key=self.secret_key,
            region_name=self.region_name
        )

# åŒ»ç–—é™„ä»¶å¤„ç†å™¨
class MedicalAttachmentProcessor:
    def __init__(self):
        # åˆå§‹åŒ–é˜¿é‡Œäº‘DashScope qwen-vl-maxæ¨¡å‹ç”¨äºåŒ»ç–—å›¾åƒåˆ†æï¼ˆæ·»åŠ è¶…æ—¶å’Œé‡è¯•é…ç½®ï¼‰
        self.vision_model = ChatOpenAI(
            model="qwen-vl-max",  # é˜¿é‡Œäº‘è§†è§‰ç†è§£æ¨¡å‹
            temperature=0.1,
            request_timeout=90,  # å›¾åƒåˆ†æéœ€è¦æ›´é•¿è¶…æ—¶æ—¶é—´ï¼ˆ90ç§’ï¼‰
            max_retries=2,       # æœ€å¤§é‡è¯•2æ¬¡
            openai_api_base="https://dashscope.aliyuncs.com/compatible-mode/v1",
            openai_api_key=os.getenv("DASHSCOPE_API_KEY"),
        )
        # åˆ›å»ºä¸Šä¼ ç›®å½•
        self.upload_dir = Path("uploads")
        self.upload_dir.mkdir(exist_ok=True)
        
        # æ”¯æŒçš„æ–‡ä»¶ç±»å‹
        self.supported_image_types = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.webp'}
        self.supported_doc_types = {'.pdf', '.txt', '.docx'}
        
        # åˆå§‹åŒ–S3é…ç½®
        self.s3_config = S3Config()
        
        # åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
        self.cache_manager = MedicalAttachmentCache(
            cache_dir="medical_cache",
            max_cache_size=100,  # æœ€å¤šç¼“å­˜100ä¸ªæ–‡ä»¶çš„åˆ†æç»“æœ
            cache_expiry_hours=24  # ç¼“å­˜24å°æ—¶åè¿‡æœŸ
        )
        
        logger.info(f"åŒ»ç–—é™„ä»¶å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆï¼Œç¼“å­˜é…ç½®: {self.cache_manager.get_cache_stats()}")
        
    async def download_from_s3(self, bucket_name: str, file_key: str) -> Tuple[bytes, str]:
        """
        ä»S3ä¸‹è½½æ–‡ä»¶
        
        Args:
            bucket_name: S3æ¡¶å
            file_key: æ–‡ä»¶çš„S3é”®å
            
        Returns:
            æ–‡ä»¶å†…å®¹å­—èŠ‚å’Œæ–‡ä»¶åçš„å…ƒç»„
        """
        try:
            s3_client = self.s3_config.get_s3_client()
            
            # ä¸‹è½½æ–‡ä»¶åˆ°å†…å­˜
            response = s3_client.get_object(Bucket=bucket_name, Key=file_key)
            file_content = response['Body'].read()
            
            # ä»file_keyä¸­æå–æ–‡ä»¶å
            filename = os.path.basename(file_key)
            
            logger.info(f"æˆåŠŸä»S3ä¸‹è½½æ–‡ä»¶: {bucket_name}/{file_key}")
            return file_content, filename
            
        except ClientError as e:
            error_code = e.response['Error']['Code']
            if error_code == 'NoSuchKey':
                logger.error(f"S3æ–‡ä»¶ä¸å­˜åœ¨: {bucket_name}/{file_key}")
                raise FileNotFoundError(f"S3æ–‡ä»¶ä¸å­˜åœ¨: {bucket_name}/{file_key}")
            elif error_code == 'NoSuchBucket':
                logger.error(f"S3æ¡¶ä¸å­˜åœ¨: {bucket_name}")
                raise FileNotFoundError(f"S3æ¡¶ä¸å­˜åœ¨: {bucket_name}")
            else:
                logger.error(f"ä¸‹è½½S3æ–‡ä»¶æ—¶å‡ºé”™: {e}")
                raise Exception(f"ä¸‹è½½S3æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        except NoCredentialsError:
            logger.error("S3å‡­è¯é…ç½®é”™è¯¯")
            raise Exception("S3å‡­è¯é…ç½®é”™è¯¯")
        except Exception as e:
            logger.error(f"ä¸‹è½½S3æ–‡ä»¶æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
            raise Exception(f"ä¸‹è½½S3æ–‡ä»¶æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")

    async def process_medical_attachment_from_s3(self, user_ip: str, filename: str) -> Dict[str, Any]:
        """
        ä»S3å¤„ç†åŒ»ç–—é™„ä»¶çš„ä¸»å‡½æ•°ï¼ˆæ”¯æŒç¼“å­˜ï¼‰
        
        Args:
            user_ip: ç”¨æˆ·IPï¼Œç”¨ä½œS3æ¡¶åçš„ä¸€éƒ¨åˆ†
            filename: æ–‡ä»¶å
            
        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        try:
            # 1. é¦–å…ˆæ£€æŸ¥ç¼“å­˜ï¼ˆå¢åŠ å¿«é€Ÿè·¯å¾„ï¼‰
            logger.info(f"ğŸ” æ£€æŸ¥åŒ»ç–—é™„ä»¶ç¼“å­˜: {filename} (ç”¨æˆ·: {user_ip})")
            cached_result = self.cache_manager.get_cached_analysis(user_ip, filename)
            
            if cached_result:
                logger.info(f"âš¡ ç¼“å­˜å‘½ä¸­ï¼Œç›´æ¥è¿”å›: {filename}")
                # è¿”å›ç¼“å­˜çš„ç»“æœï¼Œä½†æ›´æ–°ä¸€äº›å…ƒæ•°æ®
                cached_result["status"] = "success_from_cache"
                cached_result["original_filename"] = filename
                return cached_result
            
            # 2. ç¼“å­˜æœªå‘½ä¸­ï¼Œè¿›è¡Œå®Œæ•´å¤„ç†
            logger.info(f"ğŸ’¾ ç¼“å­˜æœªå‘½ä¸­ï¼Œå¼€å§‹å¤„ç†åŒ»ç–—é™„ä»¶: {filename}")
            
            # ç”Ÿæˆç¬¦åˆS3å‘½åè§„èŒƒçš„æ¡¶å
            safe_ip = user_ip.replace(".", "-").replace(":", "-").lower()
            bucket_prefix = os.getenv("S3_USER_BUCKET_PREFIX", "user")
            bucket_name = f"{bucket_prefix}"
            
            logger.info(f"å°è¯•ä»S3æ¡¶ {bucket_name} ä¸‹è½½æ–‡ä»¶: {filename}")
            
            # ä»S3ä¸‹è½½æ–‡ä»¶
            file_content, actual_filename = await self.download_from_s3(bucket_name, filename)
            file_size = len(file_content)
            
            # ä½¿ç”¨ç°æœ‰çš„å¤„ç†é€»è¾‘
            analysis_result = await self.process_medical_attachment(
                file_content=file_content,
                filename=actual_filename,
                file_type="s3_medical_attachment",
                user_ip=user_ip
            )
            
            # 3. å¦‚æœå¤„ç†æˆåŠŸï¼Œç¼“å­˜ç»“æœ
            if analysis_result.get("status") == "success":
                logger.info(f"ğŸ’¾ ç¼“å­˜åŒ»ç–—é™„ä»¶åˆ†æç»“æœ: {filename}")
                self.cache_manager.cache_analysis_result(
                    user_ip=user_ip,
                    filename=filename,
                    analysis_result=analysis_result,
                    file_size=file_size
                )
            
            return analysis_result
            
        except Exception as e:
            logger.error(f"ä»S3å¤„ç†åŒ»ç–—é™„ä»¶æ—¶å‡ºé”™: {e}")
            return {
                "file_id": "",
                "file_path": "",
                "extracted_content": f"ä»S3å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}",
                "file_type": "s3_medical_attachment",
                "original_filename": filename,
                "status": "error"
            }

    async def test_s3_connection(self) -> bool:
        """
        æµ‹è¯•S3è¿æ¥æ˜¯å¦æ­£å¸¸
        
        Returns:
            è¿æ¥æ˜¯å¦æˆåŠŸ
        """
        try:
            s3_client = self.s3_config.get_s3_client()
            # å°è¯•åˆ—å‡ºæ¡¶æ¥æµ‹è¯•è¿æ¥
            response = s3_client.list_buckets()
            logger.info("S3è¿æ¥æµ‹è¯•æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"S3è¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def cleanup_temp_file(self, file_path: str):
        """
        æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        
        Args:
            file_path: è¦æ¸…ç†çš„æ–‡ä»¶è·¯å¾„
        """
        try:
            if os.path.exists(file_path):
                os.remove(file_path)
                logger.info(f"å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {file_path}")
        except Exception as e:
            logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
    
    async def _compress_image(self, file_path: Path, max_size: tuple = (1600, 1600), quality: int = 85, target_base64_size: int = None, is_medical: bool = True) -> bytes:
        """
        å‹ç¼©å›¾åƒä»¥å‡å°‘æ–‡ä»¶å¤§å°ï¼Œé¿å…APIå­—ç¬¦é™åˆ¶
        
        Args:
            file_path: å›¾åƒæ–‡ä»¶è·¯å¾„
            max_size: æœ€å¤§å°ºå¯¸ (width, height) - åŒ»ç–—å›¾åƒå¢å¤§é»˜è®¤å°ºå¯¸
            quality: JPEGè´¨é‡ (1-100) - åŒ»ç–—å›¾åƒæé«˜é»˜è®¤è´¨é‡
            target_base64_size: ç›®æ ‡base64ç¼–ç å¤§å°ï¼ˆå­—èŠ‚ï¼‰
            is_medical: æ˜¯å¦ä¸ºåŒ»ç–—å›¾åƒï¼Œå¯ç”¨ç‰¹æ®Šä¼˜åŒ–
            
        Returns:
            å‹ç¼©åçš„å›¾åƒæ•°æ®
        """
        try:
            with Image.open(file_path) as img:
                # è½¬æ¢ä¸ºRGBæ¨¡å¼ï¼ˆç¡®ä¿å…¼å®¹æ€§ï¼‰
                if img.mode != 'RGB':
                    img = img.convert('RGB')
                
                # åŒ»ç–—å›¾åƒé¢„å¤„ç† - å¢å¼ºå¯¹æ¯”åº¦å’Œé”åŒ–
                if is_medical:
                    img = self._enhance_medical_image(img)
                
                # å¦‚æœæŒ‡å®šäº†ç›®æ ‡base64å¤§å°ï¼Œä½¿ç”¨è‡ªé€‚åº”å‹ç¼©
                if target_base64_size:
                    return await self._adaptive_compress(img, target_base64_size, is_medical)
                
                # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
                img.thumbnail(max_size, Image.Resampling.LANCZOS)
                
                # ä¿å­˜åˆ°å­—èŠ‚æµ
                import io
                output = io.BytesIO()
                
                # ç»Ÿä¸€è½¬ä¸ºJPEGæ ¼å¼ä»¥è·å¾—æœ€ä½³å‹ç¼©ç‡
                img.save(output, format='JPEG', quality=quality, optimize=True)
                
                compressed_data = output.getvalue()
                original_size = os.path.getsize(file_path)
                compressed_size = len(compressed_data)
                
                logger.info(f"å›¾åƒå‹ç¼©å®Œæˆ: {original_size} bytes â†’ {compressed_size} bytes "
                          f"(å‹ç¼©ç‡: {(1 - compressed_size/original_size)*100:.1f}%)")
                
                return compressed_data
                
        except Exception as e:
            logger.error(f"å›¾åƒå‹ç¼©å¤±è´¥: {e}")
            # å¦‚æœå‹ç¼©å¤±è´¥ï¼Œè¿”å›åŸå§‹æ•°æ®
            with open(file_path, 'rb') as f:
                return f.read()
    
    async def _adaptive_compress(self, img: Image.Image, target_base64_size: int, is_medical: bool = True) -> bytes:
        """
        è‡ªé€‚åº”å‹ç¼©å›¾åƒåˆ°æŒ‡å®šçš„base64å¤§å°
        
        Args:
            img: PILå›¾åƒå¯¹è±¡
            target_base64_size: ç›®æ ‡base64ç¼–ç å¤§å°ï¼ˆå­—èŠ‚ï¼‰
            is_medical: æ˜¯å¦ä¸ºåŒ»ç–—å›¾åƒï¼Œå½±å“å‹ç¼©ç­–ç•¥
            
        Returns:
            å‹ç¼©åçš„å›¾åƒæ•°æ®
        """
        import io
        
        # ç”±äºbase64ç¼–ç ä¼šå¢åŠ çº¦33%çš„å¤§å°ï¼Œè®¡ç®—ç›®æ ‡åŸå§‹æ•°æ®å¤§å°
        target_raw_size = int(target_base64_size * 0.75)
        
        # ä¸ºåŒ»ç–—å›¾åƒè®¾ç½®æ›´ä¿å®ˆçš„åˆå§‹å‚æ•°
        if is_medical:
            max_width = 1800  # åŒ»ç–—å›¾åƒä¿æŒæ›´é«˜åˆ†è¾¨ç‡
            quality = 95      # åŒ»ç–—å›¾åƒä¿æŒæ›´é«˜è´¨é‡
            min_quality = 70  # åŒ»ç–—å›¾åƒæœ€ä½è´¨é‡è¿›ä¸€æ­¥æé«˜
            min_width = 800   # åŒ»ç–—å›¾åƒæœ€å°å®½åº¦æé«˜
        else:
            max_width = 1000  # éåŒ»ç–—å›¾åƒä¹Ÿé€‚å½“æé«˜
            quality = 85
            min_quality = 20  # éåŒ»ç–—å›¾åƒæœ€ä½è´¨é‡ä¹Ÿé€‚å½“æé«˜
            min_width = 200
        
        for attempt in range(10):  # æœ€å¤šå°è¯•10æ¬¡
            # åˆ›å»ºå›¾åƒå‰¯æœ¬ç”¨äºå‹ç¼©
            img_copy = img.copy()
            
            # æŒ‰æ¯”ä¾‹ç¼©æ”¾
            if max_width < img_copy.width:
                ratio = max_width / img_copy.width
                new_height = int(img_copy.height * ratio)
                img_copy = img_copy.resize((max_width, new_height), Image.Resampling.LANCZOS)
            
            # å‹ç¼©ä¸ºJPEG
            output = io.BytesIO()
            img_copy.save(output, format='JPEG', quality=quality, optimize=True)
            compressed_data = output.getvalue()
            
            logger.info(f"è‡ªé€‚åº”å‹ç¼©å°è¯• {attempt + 1}: å°ºå¯¸={img_copy.width}x{img_copy.height}, "
                       f"è´¨é‡={quality}, å¤§å°={len(compressed_data)} bytes")
            
            # æ£€æŸ¥æ˜¯å¦æ»¡è¶³ç›®æ ‡å¤§å°
            if len(compressed_data) <= target_raw_size:
                return compressed_data
            
            # è°ƒæ•´å‹ç¼©å‚æ•° - åŒ»ç–—å›¾åƒä¼˜å…ˆä¿æŒè´¨é‡
            if is_medical:
                if len(compressed_data) > target_raw_size * 3:
                    # å¤§å¹…è¶…å‡ºï¼Œä¼˜å…ˆç¼©å°å°ºå¯¸ï¼Œå°‘é™è´¨é‡
                    max_width = int(max_width * 0.85)
                    quality = max(min_quality, quality - 3)
                elif len(compressed_data) > target_raw_size * 2:
                    # ä¸­ç­‰è¶…å‡ºï¼Œå¹³è¡¡è°ƒæ•´ï¼Œä½†æ›´ä¿å®ˆ
                    max_width = int(max_width * 0.9)
                    quality = max(min_quality, quality - 5)
                else:
                    # ç•¥å¾®è¶…å‡ºï¼Œè½»å¾®è°ƒæ•´è´¨é‡
                    quality = max(min_quality, quality - 8)
            else:
                # éåŒ»ç–—å›¾åƒä½¿ç”¨åŸæœ‰ç­–ç•¥
                if len(compressed_data) > target_raw_size * 2:
                    max_width = int(max_width * 0.7)
                    quality = max(min_quality, quality - 10)
                elif len(compressed_data) > target_raw_size * 1.5:
                    max_width = int(max_width * 0.8)
                    quality = max(min_quality, quality - 15)
                else:
                    quality = max(min_quality, quality - 20)
            
            # é˜²æ­¢æ— é™å¾ªç¯
            if max_width < min_width or quality < min_quality:
                break
        
        # å¦‚æœä»ç„¶è¿‡å¤§ï¼Œä½¿ç”¨ä¿å®ˆçš„æœ€å°è®¾ç½®
        logger.warning("è‡ªé€‚åº”å‹ç¼©æœªèƒ½è¾¾åˆ°ç›®æ ‡å¤§å°ï¼Œä½¿ç”¨åŒ»ç–—å›¾åƒä¿å®ˆè®¾ç½®")
        img_copy = img.copy()
        
        if is_medical:
            # åŒ»ç–—å›¾åƒä¿æŒæ›´å¤§çš„æœ€å°å°ºå¯¸å’Œè´¨é‡ - æé«˜æœ€å°æ ‡å‡†
            img_copy.thumbnail((1200, 1200), Image.Resampling.LANCZOS)
            final_quality = 75  # æé«˜æœ€ç»ˆè´¨é‡
        else:
            img_copy.thumbnail((400, 400), Image.Resampling.LANCZOS)  # éåŒ»ç–—å›¾åƒä¹Ÿé€‚å½“æé«˜
            final_quality = 30
            
        output = io.BytesIO()
        img_copy.save(output, format='JPEG', quality=final_quality, optimize=True)
        
        return output.getvalue()

    def _enhance_medical_image(self, img: Image.Image) -> Image.Image:
        """
        åŒ»ç–—å›¾åƒå¢å¼ºå¤„ç†ï¼Œæé«˜å¯¹æ¯”åº¦å’Œç»†èŠ‚æ¸…æ™°åº¦
        
        Args:
            img: PILå›¾åƒå¯¹è±¡
            
        Returns:
            å¢å¼ºåçš„å›¾åƒ
        """
        try:
            from PIL import ImageEnhance, ImageFilter
            
            # 1. å¯¹æ¯”åº¦å¢å¼º - å¯¹åŒ»ç–—å½±åƒç‰¹åˆ«é‡è¦
            contrast_enhancer = ImageEnhance.Contrast(img)
            img = contrast_enhancer.enhance(1.3)  # é€‚åº¦å¢å¼ºå¯¹æ¯”åº¦
            
            # 2. é”åŒ–å¤„ç† - å¢å¼ºè¾¹ç¼˜å’Œç»†èŠ‚
            sharpness_enhancer = ImageEnhance.Sharpness(img)
            img = sharpness_enhancer.enhance(1.2)  # é€‚åº¦é”åŒ–
            
            # 3. äº®åº¦è°ƒæ•´ - ç¡®ä¿ç»†èŠ‚å¯è§
            brightness_enhancer = ImageEnhance.Brightness(img)
            img = brightness_enhancer.enhance(1.1)  # è½»å¾®å¢äº®
            
            # 4. åº”ç”¨è½»å¾®çš„é”åŒ–æ»¤é•œ
            img = img.filter(ImageFilter.UnsharpMask(radius=1, percent=20, threshold=3))
            
            logger.info("âœ… åŒ»ç–—å›¾åƒå¢å¼ºå¤„ç†å®Œæˆ")
            return img
            
        except Exception as e:
            logger.warning(f"åŒ»ç–—å›¾åƒå¢å¼ºå¤±è´¥ï¼Œä½¿ç”¨åŸå›¾: {e}")
            return img

    async def process_medical_attachment(self, file_content: bytes, filename: str, 
                                       file_type: str, user_ip: str) -> Dict[str, Any]:
        """
        å¤„ç†åŒ»ç–—é™„ä»¶çš„ä¸»å‡½æ•°
        
        Args:
            file_content: æ–‡ä»¶äºŒè¿›åˆ¶å†…å®¹
            filename: æ–‡ä»¶å
            file_type: æ–‡ä»¶ç±»å‹æ ‡è¯†
            user_ip: ç”¨æˆ·IP
            
        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        temp_file_path = None
        try:
            # ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶ID
            file_id = str(uuid.uuid4())
            file_extension = Path(filename).suffix.lower()
            
            # ä¿å­˜æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
            safe_filename = f"{file_id}_{filename}"
            file_path = self.upload_dir / safe_filename
            temp_file_path = str(file_path)
            
            with open(file_path, 'wb') as f:
                f.write(file_content)
            
            logger.info(f"æ–‡ä»¶å·²ä¿å­˜: {file_path}")
            
            # æ ¹æ®æ–‡ä»¶ç±»å‹å¤„ç†
            if file_extension in self.supported_image_types:
                extracted_content = await self._process_medical_image(file_path)
            elif file_extension == '.pdf':
                extracted_content = await self._process_medical_pdf(file_path)
            elif file_extension in {'.txt', '.docx'}:
                extracted_content = await self._process_medical_document(file_path)
            else:
                extracted_content = f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_extension}"
            
            result = {
                "file_id": file_id,
                "file_path": str(file_path),
                "extracted_content": extracted_content,
                "file_type": file_type,
                "original_filename": filename,
                "status": "success"
            }
            
            return result
            
        except Exception as e:
            logger.error(f"å¤„ç†åŒ»ç–—é™„ä»¶æ—¶å‡ºé”™: {e}")
            return {
                "file_id": "",
                "file_path": "",
                "extracted_content": f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}",
                "file_type": file_type,
                "original_filename": filename,
                "status": "error"
            }
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if temp_file_path:
                self.cleanup_temp_file(temp_file_path)
    
    async def process_medical_image_base64(self, base64_image_data: str, filename: str = "image.jpg", user_ip: str = "unknown") -> Dict[str, Any]:
        """ç›´æ¥å¤„ç†Base64ç¼–ç çš„åŒ»ç–—å›¾åƒã€‚
        
        æ”¯æŒå¸¦æœ‰æˆ–ä¸å¸¦æœ‰ data URL å‰ç¼€çš„Base64å­—ç¬¦ä¸²ï¼Œä¾‹å¦‚ï¼š
        - data:image/jpeg;base64,/9j/4AAQSkZJRgABAQ...
        - /9j/4AAQSkZJRgABAQ...
        """
        try:
            # å»é™¤ data URL å‰ç¼€ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if base64_image_data.startswith("data:"):
                try:
                    base64_image_data = base64_image_data.split(",", 1)[1]
                except Exception:
                    pass
            # è§£ç ä¸ºäºŒè¿›åˆ¶
            image_bytes = base64.b64decode(base64_image_data)
            # å¤ç”¨ç°æœ‰å…¥å£ï¼Œç»Ÿä¸€å¤„ç†ä¸ç¼“å­˜é€»è¾‘
            return await self.process_medical_attachment(
                file_content=image_bytes,
                filename=filename,
                file_type="base64_medical_image",
                user_ip=user_ip,
            )
        except Exception as e:
            logger.error(f"å¤„ç†Base64åŒ»ç–—å›¾åƒå¤±è´¥: {e}")
            return {
                "file_id": "",
                "file_path": "",
                "extracted_content": f"å¤„ç†Base64å›¾åƒæ—¶å‡ºé”™: {str(e)}",
                "file_type": "base64_medical_image",
                "original_filename": filename,
                "status": "error",
            }
    
    async def _process_medical_image(self, file_path: Path) -> str:
        """å¤„ç†åŒ»ç–—å›¾åƒï¼ˆXå…‰ç‰‡ã€CTã€MRIã€è¡€æ£€æŠ¥å‘Šç­‰ï¼‰"""
        try:
            # åŒ»ç–—å›¾åƒä½¿ç”¨æ›´å¤§çš„ç›®æ ‡å°ºå¯¸ï¼Œç¡®ä¿å…³é”®ä¿¡æ¯ä¸ä¸¢å¤±
            target_base64_size = 40000  # å¤§å¹…æé«˜ç›®æ ‡å¤§å°ï¼Œçº¦30KBåŸå§‹æ•°æ®ï¼Œç¡®ä¿åŒ»ç–—å›¾åƒè´¨é‡
            compressed_image_data = await self._compress_image(
                file_path, 
                target_base64_size=target_base64_size,
                is_medical=True  # å¯ç”¨åŒ»ç–—å›¾åƒç‰¹æ®Šå¤„ç†
            )
            base64_image = base64.b64encode(compressed_image_data).decode('utf-8')
            data_url = f"data:image/jpeg;base64,{base64_image}"

            # æ„å»ºåŒ»ç–—å›¾åƒåˆ†ææç¤ºè¯ï¼ˆæ–‡æœ¬+å›¾åƒæ•°æ®URLï¼‰
            text_prompt = (
                "åˆ†æåŒ»ç–—å›¾åƒï¼Œæä¾›ï¼š\n"
                "1. æ£€æŸ¥ç±»å‹ï¼ˆXå…‰/CT/MRI/è¡€æ£€ç­‰ï¼‰\n"
                "2. å…³é”®ä¿¡æ¯å’Œå¼‚å¸¸å‘ç°\n"
                "3. ä¸“ä¸šå»ºè®®"
            )

            # ä½¿ç”¨é˜¿é‡Œäº‘DashScopeçš„OpenAIå…¼å®¹å¤šæ¨¡æ€æ ¼å¼ï¼štext + image_url(data URL)
            messages = [
                HumanMessage(
                    content=[
                        {"type": "text", "text": text_prompt},
                        {"type": "image_url", "image_url": {"url": data_url}},
                    ]
                )
            ]
            
            response = await self.vision_model.ainvoke(messages)
            
            # é™åˆ¶å›¾åƒåˆ†æç»“æœé•¿åº¦ï¼Œä¸ºä¸‹æ¸¸RAGæ•´åˆé¢„ç•™Tokenç©ºé—´
            analysis_result = response.content
            max_analysis_length = 2000  # å¢åŠ åˆ†æç»“æœé•¿åº¦é™åˆ¶
            
            if len(analysis_result) > max_analysis_length:
                # æ™ºèƒ½æˆªæ–­ï¼šä¿ç•™å‰éƒ¨åˆ†å…³é”®ä¿¡æ¯
                truncated_result = analysis_result[:max_analysis_length]
                # ç¡®ä¿åœ¨å¥å­è¾¹ç•Œæˆªæ–­
                last_period = truncated_result.rfind('ã€‚')
                last_newline = truncated_result.rfind('\n')
                if last_period > -1 and last_period > last_newline:
                    truncated_result = truncated_result[:last_period + 1]
                elif last_newline > -1:
                    truncated_result = truncated_result[:last_newline]
                
                truncated_result += "\n\n[æ³¨ï¼šåˆ†æç»“æœå·²æˆªæ–­ä»¥ç¡®ä¿ç³»ç»Ÿç¨³å®šæ€§]"
                logger.info(f"å›¾åƒåˆ†æç»“æœå·²æˆªæ–­ï¼š{len(analysis_result)} â†’ {len(truncated_result)} å­—ç¬¦")
                return truncated_result
            
            logger.info(f"âœ… å›¾åƒåˆ†ææˆåŠŸå®Œæˆï¼Œç»“æœé•¿åº¦: {len(analysis_result)} å­—ç¬¦")
            return analysis_result
            
        except Exception as e:
            logger.error(f"å¤„ç†åŒ»ç–—å›¾åƒæ—¶å‡ºé”™: {e}")
            # å¦‚æœAIåˆ†æå¤±è´¥ï¼Œå°è¯•OCRæ–‡æœ¬æå–ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ
            logger.info("å°è¯•OCRæ–‡æœ¬æå–ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ...")
            try:
                ocr_result = await self._extract_text_from_image(file_path)
                return f"å›¾åƒAIåˆ†æå¤±è´¥ï¼ŒOCRæ–‡æœ¬æå–ç»“æœï¼š\n{ocr_result}"
            except Exception as ocr_e:
                logger.error(f"OCRæ–‡æœ¬æå–ä¹Ÿå¤±è´¥: {ocr_e}")
                return f"å›¾åƒåˆ†æå¤±è´¥: {str(e)}ï¼ŒOCRæå–ä¹Ÿå¤±è´¥: {str(ocr_e)}"
    
    async def _extract_text_from_image(self, file_path: Path) -> str:
        """ä»å›¾åƒä¸­æå–æ–‡æœ¬ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆï¼ˆOCRï¼‰"""
        try:
            # å°è¯•å¯¼å…¥OCRåº“
            try:
                import pytesseract  # type: ignore[import]
                # PIL Imageå·²åœ¨æ–‡ä»¶é¡¶éƒ¨å¯¼å…¥
            except ImportError:
                logger.warning("OCRåº“æœªå®‰è£…ï¼Œæ— æ³•è¿›è¡Œæ–‡æœ¬æå–")
                return f"å›¾åƒè¿‡å¤§æ— æ³•åˆ†æï¼Œä¸”OCRåŠŸèƒ½æœªé…ç½®ã€‚å»ºè®®ï¼š\n1. ç¼©å°å›¾åƒæ–‡ä»¶\n2. è½¬æ¢ä¸ºæ–‡æ¡£æ ¼å¼\n3. æ‰‹åŠ¨æå–å…³é”®ä¿¡æ¯\n\næ–‡ä»¶ï¼š{file_path.name}"
            
            # æ‰“å¼€å›¾åƒå¹¶è¿›è¡ŒOCR
            with Image.open(file_path) as img:
                # è½¬æ¢ä¸ºç°åº¦ä»¥æé«˜OCRå‡†ç¡®æ€§
                if img.mode != 'L':
                    img = img.convert('L')
                
                # ä½¿ç”¨ä¸­æ–‡OCRé…ç½®
                extracted_text = pytesseract.image_to_string(img, lang='chi_sim+eng')
                
                if extracted_text.strip():
                    # æ¸…ç†å’Œæ ¼å¼åŒ–æå–çš„æ–‡æœ¬
                    lines = extracted_text.strip().split('\n')
                    cleaned_lines = [line.strip() for line in lines if line.strip()]
                    cleaned_text = '\n'.join(cleaned_lines)
                    
                    # é™åˆ¶æ–‡æœ¬é•¿åº¦
                    max_text_length = 1500
                    if len(cleaned_text) > max_text_length:
                        cleaned_text = cleaned_text[:max_text_length] + "\n\n[æ³¨ï¼šæ–‡æœ¬å·²æˆªæ–­]"
                    
                    logger.info(f"OCRæ–‡æœ¬æå–æˆåŠŸï¼Œæå–æ–‡æœ¬é•¿åº¦: {len(cleaned_text)} å­—ç¬¦")
                    return f"å›¾åƒOCRæ–‡æœ¬æå–ç»“æœï¼š\n{cleaned_text}\n\n[æ³¨ï¼šè¿™æ˜¯ä»å›¾åƒä¸­æå–çš„æ–‡æœ¬ï¼Œå¯èƒ½å­˜åœ¨è¯†åˆ«é”™è¯¯]"
                else:
                    return f"å›¾åƒOCRæœªèƒ½æå–åˆ°æ–‡æœ¬å†…å®¹ã€‚å»ºè®®ï¼š\n1. ç¡®ä¿å›¾åƒæ¸…æ™°åº¦\n2. å¢åŠ å›¾åƒå¯¹æ¯”åº¦\n3. æ‰‹åŠ¨è¾“å…¥å…³é”®ä¿¡æ¯\n\næ–‡ä»¶ï¼š{file_path.name}"
                    
        except Exception as e:
            logger.error(f"OCRæ–‡æœ¬æå–å¤±è´¥: {e}")
            return f"å›¾åƒå†…å®¹è¿‡å¤§ä¸”OCRæå–å¤±è´¥: {str(e)}ã€‚å»ºè®®ï¼š\n1. ä½¿ç”¨æ›´å°çš„å›¾åƒæ–‡ä»¶\n2. è½¬æ¢ä¸ºæ–‡æ¡£æ ¼å¼\n3. æ‰‹åŠ¨æå–å…³é”®ä¿¡æ¯\n\næ–‡ä»¶ï¼š{file_path.name}"
    
    async def _process_medical_pdf(self, file_path: Path) -> str:
        """å¤„ç†åŒ»ç–—PDFæ–‡æ¡£ï¼Œæ”¯æŒæ–‡æœ¬æå–å’ŒOCRç­–ç•¥"""
        try:
            doc = fitz.open(file_path)
            full_text = ""
            ocr_pages = []  # è®°å½•éœ€è¦OCRçš„é¡µé¢
            
            # ç¬¬ä¸€æ­¥ï¼šå°è¯•ç›´æ¥æ–‡æœ¬æå–
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                text = page.get_text().strip()
                
                if text:
                    # æœ‰æ–‡æœ¬å†…å®¹ï¼Œç›´æ¥ä½¿ç”¨
                    full_text += f"\n--- ç¬¬{page_num + 1}é¡µ ---\n{text}"
                    logger.info(f"ç¬¬{page_num + 1}é¡µï¼šç›´æ¥æ–‡æœ¬æå–æˆåŠŸ ({len(text)} å­—ç¬¦)")
                else:
                    # æ— æ–‡æœ¬å†…å®¹ï¼Œæ ‡è®°ä¸ºéœ€è¦OCR
                    ocr_pages.append(page_num)
                    logger.info(f"ç¬¬{page_num + 1}é¡µï¼šæ— æ–‡æœ¬å†…å®¹ï¼Œéœ€è¦OCRå¤„ç†")
            
            # ç¬¬äºŒæ­¥ï¼šå¯¹éœ€è¦OCRçš„é¡µé¢è¿›è¡Œå¤„ç†
            if ocr_pages:
                logger.info(f"å¼€å§‹OCRå¤„ç† {len(ocr_pages)} ä¸ªé¡µé¢...")
                ocr_text = await self._ocr_pdf_pages(doc, ocr_pages)
                if ocr_text:
                    full_text += f"\n--- OCRæå–å†…å®¹ ---\n{ocr_text}"
            
            doc.close()
            
            # ç¬¬ä¸‰æ­¥ï¼šå¦‚æœå®Œå…¨æ²¡æœ‰å†…å®¹ï¼Œå°è¯•å›¾åƒåˆ†æ
            if not full_text.strip():
                logger.info("PDFæ— æ–‡æœ¬å†…å®¹ï¼Œå°è¯•å›¾åƒåˆ†æ...")
                return await self._analyze_pdf_as_images(file_path)
            
            # ç¬¬å››æ­¥ï¼šåˆ†ææå–çš„æ–‡æœ¬
            return await self._analyze_pdf_text(full_text, len(ocr_pages) > 0)
            
        except Exception as e:
            logger.error(f"å¤„ç†PDFæ–‡æ¡£æ—¶å‡ºé”™: {e}")
            return f"PDFå¤„ç†å¤±è´¥: {str(e)}"
    
    async def _ocr_pdf_pages(self, doc, page_numbers: List[int]) -> str:
        """å¯¹PDFé¡µé¢è¿›è¡ŒOCRæ–‡æœ¬æå–"""
        try:
            # æ£€æŸ¥OCRä¾èµ–
            try:
                import pytesseract  # type: ignore[import]
            except ImportError:
                logger.warning("OCRåº“æœªå®‰è£…ï¼Œè·³è¿‡OCRå¤„ç†")
                return "OCRåŠŸèƒ½æœªé…ç½®ï¼Œæ— æ³•æå–æ‰«æé¡µé¢æ–‡æœ¬"
            
            ocr_text = ""
            max_pages_to_ocr = 5  # é™åˆ¶OCRé¡µé¢æ•°é‡ä»¥æ§åˆ¶å¤„ç†æ—¶é—´
            
            for i, page_num in enumerate(page_numbers[:max_pages_to_ocr]):
                try:
                    page = doc.load_page(page_num)
                    
                    # å°†PDFé¡µé¢è½¬æ¢ä¸ºå›¾åƒ
                    mat = fitz.Matrix(2.0, 2.0)  # 2å€ç¼©æ”¾æé«˜OCRè´¨é‡
                    pix = page.get_pixmap(matrix=mat)
                    img_data = pix.tobytes("png")
                    
                    # ä½¿ç”¨PILå¤„ç†å›¾åƒ
                    img = Image.open(io.BytesIO(img_data))
                    
                    # å›¾åƒé¢„å¤„ç†ä»¥æé«˜OCRå‡†ç¡®æ€§
                    img = self._preprocess_image_for_ocr(img)
                    
                    # æ‰§è¡ŒOCR
                    page_text = pytesseract.image_to_string(
                        img, 
                        lang='chi_sim+eng',  # ä¸­è‹±æ–‡è¯†åˆ«
                        config='--psm 6'     # å‡è®¾å•ä¸€æ–‡æœ¬å—
                    ).strip()
                    
                    if page_text:
                        ocr_text += f"\n--- ç¬¬{page_num + 1}é¡µ (OCR) ---\n{page_text}"
                        logger.info(f"ç¬¬{page_num + 1}é¡µOCRæˆåŠŸ ({len(page_text)} å­—ç¬¦)")
                    else:
                        logger.info(f"ç¬¬{page_num + 1}é¡µOCRæœªæå–åˆ°æ–‡æœ¬")
                        
                except Exception as page_e:
                    logger.warning(f"ç¬¬{page_num + 1}é¡µOCRå¤±è´¥: {page_e}")
                    continue
            
            if len(page_numbers) > max_pages_to_ocr:
                ocr_text += f"\n[æ³¨ï¼šä»…å¤„ç†å‰{max_pages_to_ocr}é¡µOCRï¼Œå‰©ä½™{len(page_numbers) - max_pages_to_ocr}é¡µå·²è·³è¿‡]"
            
            return ocr_text
            
        except Exception as e:
            logger.error(f"OCRå¤„ç†å¤±è´¥: {e}")
            return f"OCRå¤„ç†å¤±è´¥: {str(e)}"
    
    def _preprocess_image_for_ocr(self, img: Image.Image) -> Image.Image:
        """å›¾åƒé¢„å¤„ç†ä»¥æé«˜OCRå‡†ç¡®æ€§"""
        try:
            from PIL import ImageEnhance, ImageFilter
            
            # è½¬æ¢ä¸ºç°åº¦
            if img.mode != 'L':
                img = img.convert('L')
            
            # å¢å¼ºå¯¹æ¯”åº¦
            contrast_enhancer = ImageEnhance.Contrast(img)
            img = contrast_enhancer.enhance(1.5)
            
            # é”åŒ–
            img = img.filter(ImageFilter.SHARPEN)
            
            # å»å™ª
            img = img.filter(ImageFilter.MedianFilter(size=3))
            
            return img
            
        except Exception as e:
            logger.warning(f"å›¾åƒé¢„å¤„ç†å¤±è´¥: {e}")
            return img
    
    async def _analyze_pdf_as_images(self, file_path: Path) -> str:
        """å°†PDFä½œä¸ºå›¾åƒè¿›è¡ŒAIåˆ†æï¼ˆæœ€åçš„å¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        try:
            doc = fitz.open(file_path)
            
            # åªåˆ†æå‰å‡ é¡µä»¥æ§åˆ¶æˆæœ¬å’Œæ—¶é—´
            max_pages = 3
            analysis_results = []
            
            for page_num in range(min(len(doc), max_pages)):
                page = doc.load_page(page_num)
                
                # è½¬æ¢ä¸ºå›¾åƒ
                mat = fitz.Matrix(1.5, 1.5)  # é€‚ä¸­çš„åˆ†è¾¨ç‡
                pix = page.get_pixmap(matrix=mat)
                img_data = pix.tobytes("png")
                
                # å‹ç¼©å›¾åƒ
                img = Image.open(io.BytesIO(img_data))
                compressed_data = await self._compress_image_data(img, target_size=20000)
                base64_image = base64.b64encode(compressed_data).decode('utf-8')
                data_url = f"data:image/png;base64,{base64_image}"
                
                # AIåˆ†æ
                messages = [
                    HumanMessage(
                        content=[
                            {"type": "text", "text": f"åˆ†æPDFç¬¬{page_num + 1}é¡µçš„åŒ»ç–—å†…å®¹ï¼Œæå–å…³é”®ä¿¡æ¯ï¼š"},
                            {"type": "image_url", "image_url": {"url": data_url}},
                        ]
                    )
                ]
                
                response = await self.vision_model.ainvoke(messages)
                analysis_results.append(f"ç¬¬{page_num + 1}é¡µåˆ†æï¼š\n{response.content}")
            
            doc.close()
            
            combined_analysis = "\n\n".join(analysis_results)
            if len(doc) > max_pages:
                combined_analysis += f"\n\n[æ³¨ï¼šä»…åˆ†æå‰{max_pages}é¡µï¼Œæ€»å…±{len(doc)}é¡µ]"
            
            return f"PDFå›¾åƒåˆ†æç»“æœï¼š\n{combined_analysis}"
            
        except Exception as e:
            logger.error(f"PDFå›¾åƒåˆ†æå¤±è´¥: {e}")
            return f"PDFå›¾åƒåˆ†æå¤±è´¥: {str(e)}"
    
    async def _compress_image_data(self, img: Image.Image, target_size: int) -> bytes:
        """å‹ç¼©PILå›¾åƒåˆ°æŒ‡å®šå¤§å°"""
        import io
        
        # è½¬æ¢ä¸ºRGB
        if img.mode != 'RGB':
            img = img.convert('RGB')
        
        # è‡ªé€‚åº”å‹ç¼©
        quality = 85
        for _ in range(5):
            output = io.BytesIO()
            img.save(output, format='JPEG', quality=quality, optimize=True)
            data = output.getvalue()
            
            if len(data) <= target_size:
                return data
            
            quality -= 15
            if quality < 20:
                break
        
        return data
    
    async def _analyze_pdf_text(self, full_text: str, has_ocr_content: bool) -> str:
        """åˆ†æPDFæå–çš„æ–‡æœ¬å†…å®¹"""
        try:
            # é™åˆ¶æ–‡æœ¬é•¿åº¦ä»¥é¿å…è¶…è¿‡APIå­—ç¬¦é™åˆ¶
            max_text_length = 3000
            text_for_analysis = full_text[:max_text_length]
            
            # æ„å»ºåˆ†ææç¤ºè¯
            ocr_note = " (åŒ…å«OCRæå–å†…å®¹)" if has_ocr_content else ""
            analysis_prompt = f"""åˆ†æåŒ»ç–—PDFæ–‡æ¡£{ocr_note}ï¼Œæå–å…³é”®ä¿¡æ¯ï¼š

æ–‡æ¡£å†…å®¹ï¼š
{text_for_analysis}

è¯·ç®€è¦æä¾›ï¼š
1. æ–‡æ¡£ç±»å‹ï¼ˆæ£€æŸ¥æŠ¥å‘Š/ç—…å†/å¤„æ–¹ç­‰ï¼‰
2. å…³é”®åŒ»ç–—ä¿¡æ¯
3. å¼‚å¸¸å‘ç°æˆ–é‡è¦æŒ‡æ ‡
4. åŒ»ç”Ÿå»ºè®®æˆ–è¯Šæ–­ç»“è®º"""

            # æ£€æŸ¥æ€»å­—ç¬¦æ•°
            total_chars = len(analysis_prompt)
            logger.info(f"PDFåˆ†ææç¤ºè¯æ€»å­—ç¬¦æ•°: {total_chars}")
            
            if total_chars > 40000:
                # è¿›ä¸€æ­¥ç¼©çŸ­æ–‡æœ¬
                text_for_analysis = full_text[:2500]
                analysis_prompt = f"""åˆ†æåŒ»ç–—æ–‡æ¡£{ocr_note}ï¼š

{text_for_analysis}

ç®€è¦è¯´æ˜ï¼šæ–‡æ¡£ç±»å‹å’Œä¸»è¦å‘ç°ã€‚"""
                logger.info(f"ç¼©çŸ­åå­—ç¬¦æ•°: {len(analysis_prompt)}")

            messages = [HumanMessage(content=analysis_prompt)]
            response = await self.vision_model.ainvoke(messages)
            
            # è¿”å›åˆ†æç»“æœå’Œéƒ¨åˆ†åŸå§‹æ–‡æœ¬
            result = f"PDFåˆ†æ{ocr_note}ï¼š\n{response.content}"
            
            # æ·»åŠ åŸå§‹æ–‡æœ¬æ‘˜è¦
            if len(full_text) > 1000:
                result += f"\n\nåŸå§‹æ–‡æœ¬æ‘˜è¦ï¼š\n{full_text[:1000]}..."
            else:
                result += f"\n\nåŸå§‹æ–‡æœ¬ï¼š\n{full_text}"
            
            return result
            
        except Exception as e:
            logger.error(f"PDFæ–‡æœ¬åˆ†æå¤±è´¥: {e}")
            return f"PDFæ–‡æœ¬åˆ†æå¤±è´¥: {str(e)}\n\nåŸå§‹æ–‡æœ¬ï¼š\n{full_text[:1000]}..."
    
    async def _process_medical_document(self, file_path: Path) -> str:
        """å¤„ç†å…¶ä»–åŒ»ç–—æ–‡æ¡£"""
        try:
            if file_path.suffix.lower() == '.txt':
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                    
                # é™åˆ¶å†…å®¹é•¿åº¦ï¼Œé¿å…è¶…è¿‡APIå­—ç¬¦é™åˆ¶
                max_content_length = 2000
                if len(content) > max_content_length:
                    content = content[:max_content_length]
                    logger.info(f"æ–‡æ¡£å†…å®¹å·²æˆªæ–­è‡³{max_content_length}å­—ç¬¦")
                    
                return f"æ–‡æ¡£å†…å®¹ï¼š\n{content}..."
            else:
                # å¯¹äºdocxç­‰æ ¼å¼ï¼Œå¯ä»¥ä½¿ç”¨python-docxåº“
                return f"æš‚ä¸æ”¯æŒ {file_path.suffix} æ ¼å¼çš„è¯¦ç»†è§£æï¼Œå»ºè®®è½¬æ¢ä¸ºPDFæˆ–TXTæ ¼å¼"
            
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡æ¡£æ—¶å‡ºé”™: {e}")
            return f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}"
    
    def get_attachment_summary(self, attachment_info: Dict[str, Any]) -> str:
        """è·å–é™„ä»¶çš„ç®€è¦æ‘˜è¦ç”¨äºå¯¹è¯"""
        if attachment_info["status"] == "error":
            return f"é™„ä»¶ {attachment_info['original_filename']} å¤„ç†å¤±è´¥"
        
        # æ£€æŸ¥æ•°æ®æ¥æºå¹¶æ·»åŠ æŒ‡ç¤ºå™¨
        cache_indicator = ""
        status = attachment_info.get("status", "")
        if status == "success_from_cache":
            cache_indicator = " [æ¥è‡ªç¼“å­˜]"
        elif status == "success_from_user_data":
            cache_indicator = " [æ¥è‡ªç”¨æˆ·æ•°æ®]"
        elif status == "success_from_previous_data":
            cache_indicator = " [ç»§ç»­ä½¿ç”¨ä¹‹å‰çš„åˆ†æ]"
        
        content = attachment_info["extracted_content"]
        # æå–å‰200ä¸ªå­—ç¬¦ä½œä¸ºæ‘˜è¦
        summary = content[:200] + "..." if len(content) > 200 else content
        
        return f"åŒ»ç–—é™„ä»¶ '{attachment_info['original_filename']}' åˆ†æç»“æœ{cache_indicator}ï¼š{summary}"
    
    def clear_user_medical_cache(self, user_ip: str) -> int:
        """
        æ¸…ç†ç‰¹å®šç”¨æˆ·çš„åŒ»ç–—é™„ä»¶ç¼“å­˜
        
        Args:
            user_ip: ç”¨æˆ·IP
            
        Returns:
            æ¸…ç†çš„ç¼“å­˜æ¡ç›®æ•°é‡
        """
        return self.cache_manager.clear_user_cache(user_ip)
    
    def get_cache_statistics(self) -> Dict[str, Any]:
        """
        è·å–åŒ»ç–—é™„ä»¶ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        return self.cache_manager.get_cache_stats()
    
    def is_attachment_cached(self, user_ip: str, filename: str) -> bool:
        """
        æ£€æŸ¥åŒ»ç–—é™„ä»¶æ˜¯å¦å·²ç¼“å­˜
        
        Args:
            user_ip: ç”¨æˆ·IP
            filename: æ–‡ä»¶å
            
        Returns:
            æ˜¯å¦å·²ç¼“å­˜
        """
        cached_result = self.cache_manager.get_cached_analysis(user_ip, filename)
        return cached_result is not None

# å…¨å±€å¤„ç†å™¨å®ä¾‹
medical_processor = MedicalAttachmentProcessor()