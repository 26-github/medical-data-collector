import os
import time

import httpx
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, SystemMessage
from langchain_openai import ChatOpenAI
from pydantic import BaseModel, Field
from langchain_core.prompts import PromptTemplate, MessagesPlaceholder, ChatPromptTemplate
from langchain_core.output_parsers import PydanticOutputParser
import logging
from mcp import ClientSession

# å¢å¼ºçš„å…¨å±€ç¼“å­˜ç³»ç»Ÿ
_global_cache = {}
_rag_cache = {}  # RAGæŸ¥è¯¢ç»“æœç¼“å­˜
_language_cache = {}  # è¯­è¨€æ£€æµ‹ç¼“å­˜
_response_cache = {}  # å®Œæ•´å“åº”ç¼“å­˜

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("Agent-Server")

def count_tokens(text: str) -> int:
    """
    è®¡ç®—æ–‡æœ¬çš„tokenæ•°é‡
    å¯¹äºé˜¿é‡Œäº‘DashScopeæ¨¡å‹ï¼Œä¼˜å…ˆä½¿ç”¨æ›´å‡†ç¡®çš„è®¡ç®—æ–¹æ³•
    """
    if not text:
        return 0
    
    text = str(text)
    
    # æ–¹æ³•1: å°è¯•ä½¿ç”¨tiktokenï¼ˆOpenAIæ ‡å‡†ï¼‰ä½œä¸ºå‚è€ƒ
    try:
        import tiktoken
        encoding = tiktoken.get_encoding("cl100k_base")
        tiktoken_count = len(encoding.encode(text))
        # å¯¹äºé˜¿é‡Œäº‘DashScopeæ¨¡å‹ï¼Œæ ¹æ®ç»éªŒè°ƒæ•´ç³»æ•°ï¼ˆé˜¿é‡Œäº‘æ¨¡å‹tokenè®¡ç®—å¯èƒ½ç•¥æœ‰ä¸åŒï¼‰
        adjusted_count = int(tiktoken_count * 1.1)  # ä¿å®ˆä¼°è®¡ï¼Œå¢åŠ 10%
        return adjusted_count
    except ImportError:
        logger.info("tiktokenåº“æœªå®‰è£…ï¼Œä½¿ç”¨ä¼˜åŒ–çš„å­—ç¬¦æ•°ä¼°ç®—ã€‚å»ºè®®å®‰è£…: pip install tiktoken")
    except Exception as e:
        logger.warning(f"tiktokenè®¡ç®—å¤±è´¥: {e}")
    
    # æ–¹æ³•2: ä½¿ç”¨æ›´ç²¾ç¡®çš„å­—ç¬¦æ•°ä¼°ç®—ï¼ˆé’ˆå¯¹ä¸­è‹±æ–‡æ··åˆæ–‡æœ¬ä¼˜åŒ–ï¼‰
    try:
        # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦ã€è‹±æ–‡å•è¯ã€æ•°å­—å’Œç¬¦å·
        import re
        
        # ä¸­æ–‡å­—ç¬¦ï¼ˆåŒ…æ‹¬ä¸­æ–‡æ ‡ç‚¹ï¼‰
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff\u3000-\u303f\uff00-\uffef]', text))
        
        # è‹±æ–‡å•è¯ï¼ˆæŒ‰ç©ºæ ¼åˆ†å‰²ï¼‰
        english_text = re.sub(r'[\u4e00-\u9fff\u3000-\u303f\uff00-\uffef]', ' ', text)
        english_words = len([word for word in english_text.split() if word.strip()])
        
        # æ•°å­—å’Œç‰¹æ®Šç¬¦å·
        remaining_chars = len(re.sub(r'[\u4e00-\u9fff\u3000-\u303f\uff00-\uffef\w\s]', '', text))
        
        # æ ¹æ®é˜¿é‡Œäº‘DashScopeæ¨¡å‹çš„tokenè®¡ç®—è§„å¾‹ä¼°ç®—
        # ä¸­æ–‡: 1å­—ç¬¦ â‰ˆ 1token
        # è‹±æ–‡: 1å•è¯ â‰ˆ 1.3token (è€ƒè™‘subword)
        # ç¬¦å·: 1å­—ç¬¦ â‰ˆ 1token
        estimated_tokens = chinese_chars + int(english_words * 1.3) + remaining_chars
        
        # æ·»åŠ åŸºç¡€overheadï¼ˆæ¶ˆæ¯æ ¼å¼ç­‰ï¼‰
        if estimated_tokens > 0:
            estimated_tokens += 10  # åŸºç¡€æ ¼å¼token
        
        return max(estimated_tokens, 1)  # è‡³å°‘è¿”å›1
        
    except Exception as e:
        logger.warning(f"ä¼˜åŒ–tokenè®¡ç®—å¤±è´¥: {e}")
        
    # æ–¹æ³•3: æœ€ç®€å•çš„fallback
    # ä¿å®ˆä¼°è®¡: å¹³å‡æ¯2ä¸ªå­—ç¬¦1ä¸ªtoken
    return max(len(text) // 2, 1)

def calculate_prompt_tokens(messages: list, user_data: dict = None) -> dict:
    """
    è®¡ç®—æç¤ºè¯çš„tokenæ¶ˆè€—
    è¿”å›è¯¦ç»†çš„tokenä½¿ç”¨ç»Ÿè®¡
    """
    token_stats = {
        "system_prompt_tokens": 0,
        "conversation_history_tokens": 0,
        "user_data_tokens": 0,
        "current_message_tokens": 0,
        "tool_message_tokens": 0,
        "ai_message_tokens": 0,
        "total_input_tokens": 0,
        "message_count": 0
    }
    
    if not messages:
        return token_stats
    
    for i, message in enumerate(messages):
        try:
            content = ""
            message_type = "unknown"
            
            # æå–æ¶ˆæ¯å†…å®¹å’Œç±»å‹
            if hasattr(message, 'content') and hasattr(message, '__class__'):
                content = str(message.content) if message.content else ""
                message_type = message.__class__.__name__
            elif isinstance(message, dict):
                content = str(message.get('content', ''))
                message_type = message.get('type', message.get('role', 'unknown'))
            else:
                content = str(message)
                
            # è®¡ç®—tokenæ•°é‡
            tokens = count_tokens(content)
            token_stats["message_count"] += 1
            
            # æ ¹æ®æ¶ˆæ¯ç±»å‹åˆ†ç±»ç»Ÿè®¡
            if isinstance(message, SystemMessage) or message_type in ['system', 'SystemMessage']:
                token_stats["system_prompt_tokens"] += tokens
            elif isinstance(message, HumanMessage) or message_type in ['human', 'user', 'HumanMessage']:
                # æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ä½œä¸ºå½“å‰æ¶ˆæ¯
                if i == len(messages) - 1 or not any(
                    isinstance(m, HumanMessage) or (isinstance(m, dict) and m.get('type') in ['human', 'user'])
                    for m in messages[i+1:]
                ):
                    token_stats["current_message_tokens"] = tokens
                token_stats["conversation_history_tokens"] += tokens
            elif isinstance(message, AIMessage) or message_type in ['ai', 'assistant', 'AIMessage']:
                token_stats["ai_message_tokens"] += tokens
                token_stats["conversation_history_tokens"] += tokens
            elif isinstance(message, ToolMessage) or message_type in ['tool', 'function', 'ToolMessage']:
                token_stats["tool_message_tokens"] += tokens
                token_stats["conversation_history_tokens"] += tokens
            else:
                # å…¶ä»–ç±»å‹æ¶ˆæ¯å½’å…¥å¯¹è¯å†å²
                token_stats["conversation_history_tokens"] += tokens
                
        except Exception as e:
            logger.warning(f"å¤„ç†æ¶ˆæ¯æ—¶å‡ºé”™ (ç´¢å¼• {i}): {e}")
            # å¯¹å‡ºé”™çš„æ¶ˆæ¯ä½¿ç”¨é»˜è®¤tokenä¼°ç®—
            fallback_tokens = len(str(message)) // 3  # ä¿å®ˆä¼°è®¡
            token_stats["conversation_history_tokens"] += fallback_tokens
    
    # è®¡ç®—ç”¨æˆ·æ•°æ®çš„tokenæ•°é‡
    if user_data:
        try:
            user_data_text = str(user_data)
            token_stats["user_data_tokens"] = count_tokens(user_data_text)
        except Exception as e:
            logger.warning(f"è®¡ç®—ç”¨æˆ·æ•°æ®tokenæ—¶å‡ºé”™: {e}")
            token_stats["user_data_tokens"] = len(str(user_data)) // 3
    
    # è®¡ç®—æ€»è¾“å…¥tokenæ•°
    token_stats["total_input_tokens"] = (
        token_stats["system_prompt_tokens"] + 
        token_stats["conversation_history_tokens"] + 
        token_stats["user_data_tokens"]
    )
    
    # æ·»åŠ è°ƒè¯•ä¿¡æ¯
    logger.debug(f"Tokenç»Ÿè®¡è¯¦æƒ…: {token_stats}")
    
    return token_stats


def test_token_calculation():
    """
    æµ‹è¯•tokenè®¡ç®—åŠŸèƒ½
    """
    test_cases = [
        ("Hello world", "ç®€å•è‹±æ–‡"),
        ("ä½ å¥½ä¸–ç•Œ", "ç®€å•ä¸­æ–‡"),
        ("Hello ä½ å¥½ world ä¸–ç•Œ!", "ä¸­è‹±æ–‡æ··åˆ"),
        ("", "ç©ºå­—ç¬¦ä¸²"),
        ("è¿™æ˜¯ä¸€ä¸ªå¾ˆé•¿çš„æµ‹è¯•æ–‡æœ¬ï¼ŒåŒ…å«ä¸­æ–‡å’ŒEnglish mixed content, with numbers 123 and symbols @#$%^&*()", "å¤æ‚æ··åˆæ–‡æœ¬"),
    ]
    
    logger.info("ğŸ§ª å¼€å§‹æµ‹è¯•tokenè®¡ç®—åŠŸèƒ½...")
    
    for text, description in test_cases:
        try:
            tokens = count_tokens(text)
            char_count = len(text)
            ratio = tokens / max(char_count, 1)
            logger.info(f"  {description}: '{text[:30]}...' -> {tokens} tokens (å­—ç¬¦æ•°: {char_count}, æ¯”ä¾‹: {ratio:.2f})")
        except Exception as e:
            logger.error(f"  {description}: è®¡ç®—å¤±è´¥ - {e}")
    
    # æµ‹è¯•æ¶ˆæ¯åˆ—è¡¨è®¡ç®—
    test_messages = [
        HumanMessage(content="ä½ å¥½ï¼Œæˆ‘æƒ³å’¨è¯¢ä¸€äº›é—®é¢˜"),
        AIMessage(content="ä½ å¥½ï¼æˆ‘å¾ˆä¹æ„å¸®åŠ©æ‚¨ã€‚è¯·é—®æœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿ"),
        HumanMessage(content="æˆ‘æƒ³äº†è§£é˜¿é‡Œäº‘DashScopeæ¨¡å‹çš„tokenè®¡ç®—æ–¹å¼")
    ]
    
    try:
        stats = calculate_prompt_tokens(test_messages)
        logger.info(f"ğŸ“Š æ¶ˆæ¯åˆ—è¡¨tokenç»Ÿè®¡: {stats}")
    except Exception as e:
        logger.error(f"æ¶ˆæ¯åˆ—è¡¨tokenè®¡ç®—å¤±è´¥: {e}")
    
    logger.info("âœ… Tokenè®¡ç®—æµ‹è¯•å®Œæˆ")


class data(BaseModel):          #ç”Ÿæˆç”¨æˆ·æ•°æ®æ¨¡å‹
    """ç”¨æˆ·æ•°æ®æ¨¡å‹ï¼ŒåŒ…å«è¦æ”¶é›†çš„ç”¨æˆ·ä¿¡æ¯å’ŒAIåŠ©æ‰‹å›å¤"""
    name: str = Field(default="0", description="ç”¨æˆ·çš„å§“å")
    age: str = Field(default="0", description="ç”¨æˆ·çš„å¹´é¾„")
    phone: str = Field(default="0", description="ç”¨æˆ·çš„ç”µè¯")
    email: str = Field(default="0", description="ç”¨æˆ·çš„é‚®ç®±")
    country: str = Field(default="0", description="ç”¨æˆ·çš„å›½å®¶")
    language: str = Field(default="English", description="ç”¨æˆ·çš„åå¥½è¯­è¨€")
    firstMedicalOpinion: str = Field(default="0", description="ç”¨æˆ·çš„ç¬¬ä¸€åŒ»ç–—æ„è§")
    medicalAttachments: str = Field(default="0", description="ç”¨æˆ·ä¸Šä¼ çš„åŒ»ç–—é™„ä»¶")
    medicalAttachmentAnalysis: str = Field(default="0", description="å®Œæ•´çš„åŒ»ç–—é™„ä»¶åˆ†æç»“æœ")
    medicalAttachmentFilename: str = Field(default="0", description="åŒ»ç–—é™„ä»¶æ–‡ä»¶å")
    preferredCity: str = Field(default="0", description="ç”¨æˆ·å€¾å‘å°±åŒ»çš„åŸå¸‚")
    preferredHospital: str = Field(default="0", description="ç”¨æˆ·å€¾å‘å°±åŒ»çš„åŒ»é™¢")
    treatmentBudget: str = Field(default="0", description="ç”¨æˆ·çš„æ²»ç–—é¢„ç®—")
    treatmentPreference: str = Field(default="0", description="æ²»ç–—æ–¹æ¡ˆå€¾å‘: standard-æ ‡å‡†, advanced-å…ˆè¿›, clinical-ä¸´åºŠ")
    urgencyLevel: str = Field(default="0", description="ç´§æ€¥ç¨‹åº¦: low-ä½, medium-ä¸­, high-é«˜, emergency-ç´§æ€¥")
    consultationType: str = Field(default="0", description="å’¨è¯¢ç±»å‹: online_only-ä»…å’¨è¯¢, offline_required-éœ€è¦çº¿ä¸‹å°±åŒ»")
    remark: str = Field(default="0", description="ç”¨æˆ·å¤‡æ³¨")
    reply: str = Field(default="", description="AIåŠ©æ‰‹çš„å›å¤å†…å®¹")

class prefabricated_words(BaseModel):           #ç”Ÿæˆé¢„åˆ¶è¯æ¨¡å‹
    """æ ¹æ®aiçš„å›å¤ä¸ºç”¨æˆ·ç”Ÿæˆä¸‰åˆ°äº”ä¸ªé¢„åˆ¶è¯"""
    words: list[str] = Field(default_factory=list, description="é¢„åˆ¶è¯åˆ—è¡¨ï¼Œå†…éƒ¨åŒ…å«ä¸‰åˆ°äº”ä¸ªé¢„åˆ¶è¯")

# ä½¿ç”¨ PydanticOutputParser å°† data æ¨¡å‹è§£æä¸ºè¾“å‡ºæ ¼å¼
parser_data = PydanticOutputParser(pydantic_object=data)
# ä½¿ç”¨ PydanticOutputParser å°† prefabricated_words æ¨¡å‹è§£æä¸ºè¾“å‡ºæ ¼å¼
parser_prefabricated_words = PydanticOutputParser(pydantic_object=prefabricated_words)

def get_prompt_for_collecting_data():  #ç”ŸæˆåŒ»ç–—ç§‘æ™®åŠ©æ‰‹çš„æç¤ºæ¨¡æ¿
    prompt = PromptTemplate(
        template="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¤šè¯­è¨€åŒ»ç–—ç§‘æ™®åŠ©æ‰‹ï¼Œèƒ½å¤Ÿæµåˆ©ä½¿ç”¨å„ç§è¯­è¨€ä¸ºç”¨æˆ·æä¾›åŒ»ç–—å’¨è¯¢æœåŠ¡ã€‚

ç”¨æˆ·æŸ¥è¯¢ï¼š{query}

### è¯­è¨€å¤„ç†è§„åˆ™ï¼š
1. **ä¸¥æ ¼ä½¿ç”¨ç”¨æˆ·åå¥½è¯­è¨€å›å¤**ï¼š{language}
2. **å°è¯­ç§æ”¯æŒ**ï¼šå¦‚æœç”¨æˆ·ä½¿ç”¨å°è¯­ç§ï¼Œè¯·ç¡®ä¿ï¼š
   - ä½¿ç”¨å‡†ç¡®çš„è¯­æ³•å’Œè¡¨è¾¾
   - é‡‡ç”¨å½“åœ°å¸¸ç”¨çš„åŒ»ç–—æœ¯è¯­
   - è€ƒè™‘æ–‡åŒ–èƒŒæ™¯å’Œè¯­è¨€ä¹ æƒ¯
   - ä¿æŒä¸“ä¸šæ€§å’Œå‡†ç¡®æ€§
3. **è¯­è¨€ä»£ç å¯¹åº”**ï¼š
   - zh: ä¸­æ–‡ï¼ˆç®€ä½“/ç¹ä½“ï¼‰
   - English: è‹±æ–‡
   - ja: æ—¥æ–‡  
   - ko: éŸ©æ–‡
   - ar: é˜¿æ‹‰ä¼¯æ–‡
   - th: æ³°æ–‡
   - vi: è¶Šå—æ–‡
   - hi: å°åœ°æ–‡
   - bn: å­ŸåŠ æ‹‰æ–‡
   - ta: æ³°ç±³å°”æ–‡
   - te: æ³°å¢å›ºæ–‡
   - ml: é©¬æ‹‰é›…æ‹‰å§†æ–‡
   - kn: å¡çº³è¾¾æ–‡
   - gu: å¤å‰æ‹‰ç‰¹æ–‡
   - pa: æ—é®æ™®æ–‡
   - ur: ä¹Œå°”éƒ½æ–‡
   - fa: æ³¢æ–¯æ–‡
   - ru: ä¿„æ–‡
   - es: è¥¿ç­ç‰™æ–‡
   - fr: æ³•æ–‡
   - de: å¾·æ–‡
   - å…¶ä»–ISO 639-1è¯­è¨€ä»£ç 

### åŒ»ç–—å¤„ç†è§„åˆ™ï¼š
å¯¹äºåŒ»ç–—ç›¸å…³é—®é¢˜ï¼Œå¿…é¡»ä½¿ç”¨medical_qa_searchå·¥å…·è·å–ä¸“ä¸šåŒ»å­¦çŸ¥è¯†ã€‚

### ç”¨æˆ·æ•°æ®ï¼š
{data}

### å¯¹è¯å†å²ï¼š
{record}

### å›å¤è¦æ±‚ï¼š
1. ä½¿ç”¨{language}è¯­è¨€å›å¤
2. æä¾›ä¸“ä¸šã€å‡†ç¡®çš„åŒ»ç–—ä¿¡æ¯
3. ä¿æŒæ–‡åŒ–æ•æ„Ÿæ€§å’Œè¯­è¨€é€‚åº”æ€§
4. å¦‚æœæ˜¯å°è¯­ç§ï¼Œç¡®ä¿åŒ»ç–—æœ¯è¯­çš„å‡†ç¡®æ€§

è¯·å¤„ç†ç”¨æˆ·çš„åŒ»ç–—é—®é¢˜ã€‚""",
        # queryæ˜¯ç”¨æˆ·æœ€æ–°æ¶ˆæ¯ï¼Œdataæ˜¯ç”¨æˆ·å·²æ”¶é›†çš„ä¿¡æ¯ï¼Œrecordæ˜¯å¯¹è¯è®°å½•ï¼Œlanguageæ˜¯ç”¨æˆ·åå¥½è¯­è¨€
        input_variables=["query", "data", "record","language"],
    )
    return prompt


def extract_reply_from_ai_message(ai_content: str) -> str:
    """ä»AIæ¶ˆæ¯ä¸­æå–replyå­—æ®µï¼Œå¦‚æœæ˜¯JSONæ ¼å¼çš„è¯"""
    try:
        import json
        # å°è¯•è§£æJSON
        if ai_content.strip().startswith('{') and ai_content.strip().endswith('}'):
            data = json.loads(ai_content)
            return data.get('reply', ai_content)
    except:
        pass
    return ai_content

def optimize_conversation_history(conversation_history: list, max_entries: int = 4, max_tokens: int = 1500) -> list:
    """ä¼˜åŒ–å¯¹è¯å†å²ï¼Œå¤§å¹…å‡å°‘tokenæ¶ˆè€—"""
    from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, ToolMessage
    
    if not conversation_history:
        return conversation_history
    
    # 1. ç§»é™¤å·¥å…·æ¶ˆæ¯å’ŒåŒ…å«å·¥å…·è°ƒç”¨çš„AIæ¶ˆæ¯
    cleaned_messages = []
    for msg in conversation_history:
        if isinstance(msg, ToolMessage):
            continue
        if isinstance(msg, AIMessage):
            has_tool_calls = (
                hasattr(msg, 'tool_calls') and msg.tool_calls or
                hasattr(msg, 'additional_kwargs') and msg.additional_kwargs.get('tool_calls')
            )
            if has_tool_calls:
                continue
        cleaned_messages.append(msg)
    
    # 2. æ™ºèƒ½é€‰æ‹©æœ€é‡è¦çš„æ¶ˆæ¯
    if len(cleaned_messages) <= max_entries:
        selected_messages = cleaned_messages
    else:
        # ä¿ç•™æœ€è¿‘çš„æ¶ˆæ¯å’ŒåŒ…å«åŒ»ç–—å…³é”®è¯çš„é‡è¦æ¶ˆæ¯
        important_messages = []
        total_count = len(cleaned_messages)
        preserve_recent = min(2, max_entries // 2)  # ä¿ç•™æœ€è¿‘1-2æ¡æ¶ˆæ¯
        
        # å…ˆæ·»åŠ æœ€è¿‘çš„æ¶ˆæ¯
        for i in range(max(0, total_count - preserve_recent), total_count):
            important_messages.append(cleaned_messages[i])
        
        # å†ä»å†å²ä¸­é€‰æ‹©åŒ…å«åŒ»ç–—å…³é”®è¯çš„æ¶ˆæ¯
        medical_keywords = ['ç—‡çŠ¶', 'è¯Šæ–­', 'æ²»ç–—', 'è¯ç‰©', 'æ£€æŸ¥', 'æŠ¥å‘Š', 'ç—…æƒ…', 'åŒ»é™¢', 'ç–¼ç—›', 'å‘çƒ­']
        for i, msg in enumerate(cleaned_messages[:-preserve_recent]):
            if len(important_messages) >= max_entries:
                break
            if hasattr(msg, 'content'):
                content = str(msg.content).lower()
                if any(keyword in content for keyword in medical_keywords):
                    # é¿å…é‡å¤æ·»åŠ 
                    if msg not in important_messages:
                        important_messages.insert(-preserve_recent, msg)
        
        selected_messages = important_messages[:max_entries]
    
    # 3. æ¿€è¿›çš„å†…å®¹å‹ç¼©
    final_messages = []
    total_tokens = 0
    
    for msg in selected_messages:
        if hasattr(msg, 'content'):
            content = str(msg.content)
            # è®¡ç®—å½“å‰tokenæ•°
            content_tokens = count_tokens(content)
            
            # å¦‚æœå•æ¡æ¶ˆæ¯è¿‡é•¿ï¼Œè¿›è¡Œå‹ç¼©
            if content_tokens > 300:  # å•æ¡æ¶ˆæ¯æœ€å¤š300 tokens
                # ä¿ç•™å¼€å¤´å’Œç»“å°¾ï¼Œä¸­é—´ç”¨çœç•¥å·
                max_chars = 200
                if len(content) > max_chars:
                    start_part = content[:max_chars//2]
                    end_part = content[-max_chars//2:]
                    content = f"{start_part}...[å·²å‹ç¼©]...{end_part}"
                    content_tokens = count_tokens(content)
            
            # æ£€æŸ¥æ€»tokené™åˆ¶
            if total_tokens + content_tokens > max_tokens:
                # å¦‚æœä¼šè¶…è¿‡é™åˆ¶ï¼Œè¿›ä¸€æ­¥å‹ç¼©æˆ–è·³è¿‡
                remaining_tokens = max_tokens - total_tokens
                if remaining_tokens > 50:  # è‡³å°‘ä¿ç•™50ä¸ªtokençš„ç©ºé—´
                    # è¿›ä¸€æ­¥å‹ç¼©å†…å®¹
                    max_chars = min(100, remaining_tokens * 2)  # ç²—ç•¥ä¼°ç®—
                    content = content[:max_chars] + "...[å·²å‹ç¼©]"
                    content_tokens = count_tokens(content)
                else:
                    break  # è·³è¿‡å‰©ä½™æ¶ˆæ¯
            
            # åˆ›å»ºå‹ç¼©åçš„æ¶ˆæ¯
            if isinstance(msg, HumanMessage):
                final_messages.append(HumanMessage(content=content))
            elif isinstance(msg, AIMessage):
                final_messages.append(AIMessage(content=content))
            else:
                final_messages.append(msg)
            
            total_tokens += content_tokens
        else:
            final_messages.append(msg)
    
    logger.info(f"âš¡ å¯¹è¯å†å²æ¿€è¿›ä¼˜åŒ–: {len(conversation_history)} -> {len(final_messages)} æ¡æ¶ˆæ¯, é¢„ä¼°tokens: {total_tokens}")
    return final_messages

def clean_conversation_history(conversation_history: list) -> str:
    """å½»åº•æ¸…ç†å¯¹è¯å†å²ï¼Œåªä¿ç•™æ ¸å¿ƒå¯¹è¯å†…å®¹ï¼Œå¤§å¹…å‡å°‘tokenæ¶ˆè€—"""
    from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, ToolMessage
    
    # ä½¿ç”¨ä¼˜åŒ–åçš„å†å²
    optimized_history = optimize_conversation_history(conversation_history)
    
    history_text = ""
    
    for msg in optimized_history:
        if isinstance(msg, HumanMessage):
            content = str(msg.content)
            history_text += f"ç”¨æˆ·: {content}\n"
        elif isinstance(msg, AIMessage):
            # å¦‚æœcontentä¸ºç©ºä½†æœ‰å·¥å…·è°ƒç”¨ï¼Œè·³è¿‡è¿™æ¡æ¶ˆæ¯
            if not msg.content.strip():
                continue
            # æå–AIå›å¤çš„æ ¸å¿ƒå†…å®¹
            clean_reply = extract_reply_from_ai_message(msg.content)
            if clean_reply.strip():
                history_text += f"åŠ©æ‰‹: {clean_reply}\n"
        elif isinstance(msg, ToolMessage):
            # è·³è¿‡å·¥å…·æ¶ˆæ¯ï¼Œå› ä¸ºå®ƒä»¬æ˜¯æŠ€æœ¯ç»†èŠ‚ï¼Œå ç”¨å¤§é‡token
            continue
        elif isinstance(msg, SystemMessage):
            # è·³è¿‡ç³»ç»Ÿæ¶ˆæ¯
            continue
    
    return history_text

def build_prompt_text(user_message: str, user_data_dict: dict, conversation_history: list) -> str:
    """æ„å»ºå®Œæ•´çš„æç¤ºè¯æ–‡æœ¬"""
    # ä½¿ç”¨æ–°çš„æ¸…ç†å‡½æ•°è·å–å¹²å‡€çš„å†å²è®°å½•
    history_text = clean_conversation_history(conversation_history)

    # è·å–åŸå§‹æç¤ºè¯æ¨¡æ¿
    prompt_template = get_prompt_for_collecting_data()

    # æ„å»ºå®Œæ•´æç¤ºè¯
    full_prompt = prompt_template.format(
        query=user_message,
        data=user_data_dict,
        record=history_text,
        format_instructions=parser_data.get_format_instructions()
    )

    return full_prompt



def get_prompt_for_generating_preset():    #ç”Ÿæˆé¢„åˆ¶è¯çš„æç¤ºæ¨¡æ¿
    """ç”Ÿæˆé¢„åˆ¶è¯çš„æç¤ºæ¨¡æ¿ï¼Œå¢å¼ºå°è¯­ç§æ”¯æŒ"""
    prompt = PromptTemplate(
        template="""ç”¨æˆ·å½“å‰æ•°æ®ï¼š{data_and_ai_message}

                 æ•°æ®è¯´æ˜ï¼š
                 - data: åŒ…å«ç”¨æˆ·å·²æ”¶é›†çš„ä¿¡æ¯
                 - reply: AIæœ€æ–°çš„å›å¤å†…å®¹
                 - language: ç”¨æˆ·åå¥½è¯­è¨€

                 è¦æ±‚ï¼š
                 1. æ ¹æ®AIçš„replyå†…å®¹ï¼Œç”Ÿæˆ3-5ä¸ªé¢„åˆ¶è¯é€‰é¡¹
                 2. **ä¸¥æ ¼ä½¿ç”¨languageå­—æ®µæŒ‡å®šçš„è¯­è¨€**ç”Ÿæˆé¢„åˆ¶è¯ï¼Œæ”¯æŒä»»ä½•è¯­ç§åŒ…æ‹¬å°è¯­ç§
                 3. é¢„åˆ¶è¯åº”è¯¥æ˜¯ç”¨æˆ·å¯¹AIé—®é¢˜çš„åˆç†å›ç­”é€‰é¡¹
                 4. å¿…é¡»åŒ…æ‹¬ä¸€ä¸ªç¤¼è²Œçš„æ‹’ç»é€‰é¡¹ï¼Œä½¿ç”¨å¯¹åº”è¯­è¨€çš„æ ‡å‡†è¡¨è¾¾ï¼š
                    - ä¸­æ–‡(zh): "ä¸æ„¿é€éœ²"
                    - è‹±æ–‡(English): "Prefer not to say"
                    - æ—¥æ–‡(ja): "ãŠç­”ãˆã§ãã¾ã›ã‚“"
                    - éŸ©æ–‡(ko): "ë‹µë³€í•˜ê³  ì‹¶ì§€ ì•ŠìŠµë‹ˆë‹¤"
                    - é˜¿æ‹‰ä¼¯æ–‡(ar): "Ø£ÙØ¶Ù„ Ø¹Ø¯Ù… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©"
                    - æ³°æ–‡(th): "à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¸•à¸­à¸š"
                    - è¶Šå—æ–‡(vi): "KhÃ´ng muá»‘n tráº£ lá»i"
                    - å°åœ°æ–‡(hi): "à¤œà¤µà¤¾à¤¬ à¤¨à¤¹à¥€à¤‚ à¤¦à¥‡à¤¨à¤¾ à¤šà¤¾à¤¹à¤¤à¥‡"
                    - å­ŸåŠ æ‹‰æ–‡(bn): "à¦‰à¦¤à§à¦¤à¦° à¦¦à¦¿à¦¤à§‡ à¦šà¦¾à¦‡ à¦¨à¦¾"
                    - æ³°ç±³å°”æ–‡(ta): "à®ªà®¤à®¿à®²à¯ à®šà¯Šà®²à¯à®² à®µà®¿à®°à¯à®®à¯à®ªà®µà®¿à®²à¯à®²à¯ˆ"
                    - ä¿„æ–‡(ru): "ĞŸÑ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ğ¸Ñ‚Ğ°Ñ Ğ½Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°Ñ‚ÑŒ"
                    - è¥¿ç­ç‰™æ–‡(es): "Prefiero no decir"
                    - æ³•æ–‡(fr): "Je prÃ©fÃ¨re ne pas dire"
                    - å¾·æ–‡(de): "MÃ¶chte ich nicht sagen"
                    - å…¶ä»–è¯­è¨€ï¼šä½¿ç”¨ç›¸åº”çš„ç¤¼è²Œæ‹’ç»è¡¨è¾¾
                 5. é¢„åˆ¶è¯è¦ç®€çŸ­æ˜äº†ï¼Œä¾¿äºç”¨æˆ·å¿«é€Ÿé€‰æ‹©
                 6. æ™ºèƒ½ç”Ÿæˆé€‰é¡¹ï¼ˆä½¿ç”¨å¯¹åº”è¯­è¨€ï¼‰ï¼š
                    - é€‰æ‹©é¢˜ï¼šæä¾›æ ‡å‡†é€‰é¡¹
                    - ä¸ªäººä¿¡æ¯ï¼šæä¾›"è¾“å…¥"å’Œ"è·³è¿‡"é€‰é¡¹  
                    - å¹´é¾„ï¼šæä¾›å¹´é¾„æ®µé€‰é¡¹
                    - åœ°ç†ä½ç½®ï¼šæä¾›å¸¸è§é€‰é¡¹
                    - é¢„ç®—ï¼šæä¾›åˆç†èŒƒå›´é€‰é¡¹
                    - åŒ»ç–—ç›¸å…³ï¼šæä¾›å¸¸è§ç—‡çŠ¶ã€æ²»ç–—åå¥½ç­‰é€‰é¡¹
                 7. **è¯­è¨€ä¸€è‡´æ€§**ï¼šæ‰€æœ‰é¢„åˆ¶è¯å¿…é¡»ä½¿ç”¨ä¼ å…¥çš„languageå¯¹åº”çš„è¯­è¨€è¿›è¡Œè¾“å‡ºï¼Œä¸å¾—æ··ç”¨
                 8. **å°è¯­ç§ç‰¹æ®Šå¤„ç†**ï¼š
                    - ç¡®ä¿å°è¯­ç§çš„è¯­æ³•å’Œè¡¨è¾¾ä¹ æƒ¯æ­£ç¡®
                    - ä½¿ç”¨å½“åœ°å¸¸ç”¨çš„è¡¨è¾¾æ–¹å¼
                    - è€ƒè™‘æ–‡åŒ–èƒŒæ™¯å’Œè¯­è¨€ä¹ æƒ¯
                 
                 é‡è¦ï¼šè¯·ç›´æ¥è¾“å‡ºJSONæ•°æ®ï¼Œä¸è¦è¾“å‡ºschemaæˆ–descriptionï¼
                 
                 è¾“å‡ºç¤ºä¾‹ï¼š
                 {{"words": ["é€‰é¡¹1", "é€‰é¡¹2", "é€‰é¡¹3", "é€‰é¡¹4"]}}

                 {format_instructions}""",
        # data_and_ai_messageåŒ…å«ç”¨æˆ·æ•°æ®å’ŒAIæœ€æ–°å›å¤
        input_variables=["data_and_ai_message"],
        partial_variables={"format_instructions": parser_prefabricated_words.get_format_instructions()},
    )
    return prompt



# æ£€æŸ¥ç”¨æˆ·æ˜¯å¦å·²å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºæ–°ç”¨æˆ·
def check_user(user_ip,database):
    # å¦‚æœç”¨æˆ·ä¸å­˜åœ¨ï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„ç”¨æˆ·è®°å½•
    if user_ip not in database:
        database[user_ip] = {'record': [], 'data': {}}
        return False
    return True

# è·å–åç»­å¯¹è¯çš„æç¤ºæ¨¡æ¿
def get_follow_up_prompt(token_stats: dict = None):
    """ç”Ÿæˆåç»­å¯¹è¯çš„æç¤ºæ¨¡æ¿ï¼Œå¢å¼ºå¤šè¯­è¨€æ”¯æŒ"""
    
    follow_up_prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¤šè¯­è¨€åŒ»ç–—ç§‘æ™®åŠ©æ‰‹ã€‚è¯·æ ¹æ®å·¥å…·è¿”å›çš„ç»“æœæä¾›å‡†ç¡®çš„åŒ»ç–—å›å¤ã€‚

## å¤šè¯­è¨€å›å¤è§„èŒƒï¼š
1. **ä¸¥æ ¼æŒ‰ç…§ç”¨æˆ·æ•°æ®ä¸­çš„languageå­—æ®µä½¿ç”¨å¯¹åº”è¯­è¨€å›å¤**
2. **å°è¯­ç§ç‰¹æ®Šå¤„ç†**ï¼š
   - ç¡®ä¿åŒ»ç–—æœ¯è¯­çš„å‡†ç¡®æ€§å’Œä¸“ä¸šæ€§
   - ä½¿ç”¨å½“åœ°å¸¸ç”¨çš„è¡¨è¾¾æ–¹å¼
   - è€ƒè™‘æ–‡åŒ–èƒŒæ™¯å’Œè¯­è¨€ä¹ æƒ¯
   - ä¿æŒè¯­æ³•æ­£ç¡®å’Œè¡¨è¾¾è‡ªç„¶
3. **æ”¯æŒçš„è¯­è¨€åŒ…æ‹¬ä½†ä¸é™äº**ï¼š
   - ä¸»è¦è¯­è¨€ï¼šä¸­æ–‡(zh)ã€è‹±æ–‡(English)ã€æ—¥æ–‡(ja)ã€éŸ©æ–‡(ko)ã€é˜¿æ‹‰ä¼¯æ–‡(ar)
   - å—äºšè¯­è¨€ï¼šå°åœ°æ–‡(hi)ã€å­ŸåŠ æ‹‰æ–‡(bn)ã€æ³°ç±³å°”æ–‡(ta)ã€æ³°å¢å›ºæ–‡(te)ã€é©¬æ‹‰é›…æ‹‰å§†æ–‡(ml)ã€å¡çº³è¾¾æ–‡(kn)ã€å¤å‰æ‹‰ç‰¹æ–‡(gu)ã€æ—é®æ™®æ–‡(pa)ã€ä¹Œå°”éƒ½æ–‡(ur)
   - ä¸œå—äºšè¯­è¨€ï¼šæ³°æ–‡(th)ã€è¶Šå—æ–‡(vi)ã€ç¼…ç”¸æ–‡(my)ã€é«˜æ£‰æ–‡(km)ã€è€æŒæ–‡(lo)
   - ä¸­ä¸œè¯­è¨€ï¼šæ³¢æ–¯æ–‡(fa)ã€å¸Œä¼¯æ¥æ–‡(he)
   - æ¬§æ´²è¯­è¨€ï¼šä¿„æ–‡(ru)ã€è¥¿ç­ç‰™æ–‡(es)ã€æ³•æ–‡(fr)ã€å¾·æ–‡(de)ç­‰
   - å…¶ä»–å°è¯­ç§ï¼šæ ¼é²å‰äºšæ–‡(ka)ã€äºšç¾å°¼äºšæ–‡(hy)ã€é˜¿å§†å“ˆæ‹‰æ–‡(am)ç­‰
4. **å›å¤è´¨é‡è¦æ±‚**ï¼š
   - åŒ»ç–—ä¿¡æ¯å‡†ç¡®ä¸“ä¸š
   - è¯­è¨€è¡¨è¾¾è‡ªç„¶æµç•…
   - æ–‡åŒ–é€‚åº”æ€§å¼º
   - ç”¨æˆ·å‹å¥½æ˜“æ‡‚"""),
            MessagesPlaceholder(variable_name="history"),
        ])
    return follow_up_prompt

# æ‰§è¡Œå·¥å…·è°ƒç”¨
async def execute_tools(tool_calls: list, session: ClientSession, tools: list) -> list:
    """æ‰§è¡Œå·¥å…·è°ƒç”¨å¹¶ç”ŸæˆToolMessage"""
    tool_messages = []
    for tool_call in tool_calls:
        func_name = "unknown_tool"  # é»˜è®¤å€¼ï¼Œç¡®ä¿åœ¨å¼‚å¸¸å¤„ç†ä¸­å¯ç”¨
        try:
            # ä¿®å¤å·¥å…·è°ƒç”¨æ ¼å¼ï¼šOpenAIçš„å·¥å…·è°ƒç”¨æ ¼å¼
            if "function" in tool_call:
                func_name = tool_call["function"]["name"]
                args_str = tool_call["function"]["arguments"]
                # è§£æå‚æ•°å­—ç¬¦ä¸²ä¸ºå­—å…¸
                import json
                args = json.loads(args_str) if args_str else {}
            else:
                # å¤‡ç”¨æ ¼å¼ï¼ˆå¦‚æœæ˜¯å…¶ä»–æ ¼å¼ï¼‰
                func_name = tool_call.get("name", "unknown_tool")
                args = tool_call.get("args", {})

            # æŸ¥æ‰¾å¹¶æ‰§è¡Œå¯¹åº”çš„å·¥å…·
            tool_found = False
            for t in tools:
                if t.name == func_name:
                    logger.info(f"æ‰§è¡Œå·¥å…·: {func_name} å‚æ•°: {args}")
                    # æ‰§è¡ŒMCPå·¥å…·ï¼Œä¼ é€’sessionå‚æ•°
                    result = await t.ainvoke(args, session=session)
                    logger.info(f"å·¥å…·æ‰§è¡Œç»“æœ: {result}")

                    tool_messages.append(
                        ToolMessage(
                            content=str(result),
                            tool_call_id=tool_call.get("id", "unknown"),
                            name=func_name,  # æ·»åŠ nameå­—æ®µï¼Œé˜¿é‡Œäº‘AIè¦æ±‚functionæ¶ˆæ¯å¿…é¡»æœ‰name
                        )
                    )
                    tool_found = True
                    break

            if not tool_found:
                logger.warning(f"å·¥å…·æœªæ‰¾åˆ°: {func_name}")
                tool_messages.append(
                    ToolMessage(
                        content=f"Tool {func_name} not found",
                        tool_call_id=tool_call.get("id", "unknown"),
                        name=func_name,  # æ·»åŠ nameå­—æ®µï¼Œé˜¿é‡Œäº‘AIè¦æ±‚functionæ¶ˆæ¯å¿…é¡»æœ‰name
                    )
                )
        except Exception as e:
            import traceback
            error_msg = f"å·¥å…·æ‰§è¡Œé”™è¯¯: {type(e).__name__}: {str(e)}"
            logger.error(error_msg)
            logger.error(f"å®Œæ•´é”™è¯¯æ ˆ: {traceback.format_exc()}")
            tool_messages.append(
                ToolMessage(
                    content=f"Error executing tool: {error_msg}",
                    tool_call_id=tool_call.get("id", "unknown"),
                    name=func_name,  # æ·»åŠ nameå­—æ®µï¼Œé˜¿é‡Œäº‘AIè¦æ±‚functionæ¶ˆæ¯å¿…é¡»æœ‰name
                )
            )
    return tool_messages

# ä¿ç•™è¯­è¨€æ£€æµ‹AIï¼ˆç”¨æˆ·éœ€æ±‚ï¼‰
llm_for_language_detection = ChatOpenAI(
    model="qwen-max",  # æ”¹ç”¨æ›´å¿«çš„qwen-maxè€Œä¸æ˜¯qwen-vl-max
    temperature=0.1,
    request_timeout=30,  # ç¼©çŸ­è¶…æ—¶æ—¶é—´
    max_retries=2,       # å‡å°‘é‡è¯•æ¬¡æ•°
    openai_api_base="https://dashscope.aliyuncs.com/compatible-mode/v1",
    openai_api_key=os.getenv("DASHSCOPE_API_KEY"),
)
def should_detect_language(user_ip: str, message: str, current_language: str = None) -> bool:
    """æ™ºèƒ½åˆ¤æ–­æ˜¯å¦éœ€è¦è¿›è¡Œè¯­è¨€æ£€æµ‹ï¼Œå¢å¼ºå°è¯­ç§æ”¯æŒ"""
    # å¦‚æœæ¶ˆæ¯å¾ˆçŸ­ï¼ˆå°‘äº10ä¸ªå­—ç¬¦ï¼‰ï¼Œä½†åŒ…å«ç‰¹æ®Šå­—ç¬¦ï¼Œä»éœ€æ£€æµ‹
    if len(message.strip()) < 10:
        # æ£€æŸ¥æ˜¯å¦åŒ…å«éæ‹‰ä¸å­—ç¬¦ï¼Œå¦‚æœæœ‰åˆ™éœ€è¦æ£€æµ‹
        import re
        has_special_chars = bool(re.search(r'[^\x00-\x7F]', message))
        if not has_special_chars:
            return False
    
    # å¦‚æœæ˜¯çº¯æ•°å­—æˆ–ç¬¦å·ï¼Œè·³è¿‡æ£€æµ‹
    if message.strip().isdigit() or not any(c.isalpha() for c in message):
        return False
    
    # å¦‚æœå½“å‰è¯­è¨€ä¸ºç©ºæˆ–é»˜è®¤å€¼ï¼Œéœ€è¦æ£€æµ‹
    if not current_language or current_language in ['English']:
        return True
    
    # å¢å¼ºçš„è¯­è¨€ç‰¹å¾æ£€æµ‹ï¼Œæ”¯æŒæ›´å¤šå°è¯­ç§
    if current_language and current_language != 'English':
        import re
        
        # æ‰©å±•çš„è¯­è¨€ç‰¹å¾æ£€æµ‹
        language_patterns = {
            'chinese': r'[\u4e00-\u9fff]',
            'japanese': r'[\u3040-\u309f\u30a0-\u30ff]',
            'korean': r'[\uac00-\ud7af]',
            'arabic': r'[\u0600-\u06ff]',
            'thai': r'[\u0e00-\u0e7f]',
            'vietnamese': r'[Ã Ã¡áº¡áº£Ã£Ã¢áº§áº¥áº­áº©áº«Äƒáº±áº¯áº·áº³áºµÃ¨Ã©áº¹áº»áº½Ãªá»áº¿á»‡á»ƒá»…Ã¬Ã­á»‹á»‰Ä©Ã²Ã³á»á»ÃµÃ´á»“á»‘á»™á»•á»—Æ¡á»á»›á»£á»Ÿá»¡Ã¹Ãºá»¥á»§Å©Æ°á»«á»©á»±á»­á»¯á»³Ã½á»µá»·á»¹Ä‘]',
            'hindi': r'[\u0900-\u097f]',
            'bengali': r'[\u0980-\u09ff]',
            'tamil': r'[\u0b80-\u0bff]',
            'telugu': r'[\u0c00-\u0c7f]',
            'malayalam': r'[\u0d00-\u0d7f]',
            'kannada': r'[\u0c80-\u0cff]',
            'gujarati': r'[\u0a80-\u0aff]',
            'punjabi': r'[\u0a00-\u0a7f]',
            'urdu': r'[\u0600-\u06ff\u0750-\u077f]',
            'persian': r'[\u0600-\u06ff\u0750-\u077f]',
            'russian': r'[Ğ°-ÑÑ‘]',
            'greek': r'[\u0370-\u03ff]',
            'hebrew': r'[\u0590-\u05ff]',
            'myanmar': r'[\u1000-\u109f]',
            'khmer': r'[\u1780-\u17ff]',
            'lao': r'[\u0e80-\u0eff]',
            'georgian': r'[\u10a0-\u10ff]',
            'armenian': r'[\u0530-\u058f]',
            'ethiopic': r'[\u1200-\u137f]',
        }
        
        # æ£€æµ‹å½“å‰æ¶ˆæ¯ä¸­çš„è¯­è¨€ç‰¹å¾
        detected_features = []
        for lang, pattern in language_patterns.items():
            if re.search(pattern, message, re.IGNORECASE):
                detected_features.append(lang)
        
        # å¦‚æœæ£€æµ‹åˆ°ä¸å½“å‰è¯­è¨€ä¸åŒ¹é…çš„ç‰¹å¾ï¼Œéœ€è¦é‡æ–°æ£€æµ‹
        current_lang_mapping = {
            'zh': 'chinese',
            'ja': 'japanese', 
            'ko': 'korean',
            'ar': 'arabic',
            'th': 'thai',
            'vi': 'vietnamese',
            'hi': 'hindi',
            'bn': 'bengali',
            'ta': 'tamil',
            'te': 'telugu',
            'ml': 'malayalam',
            'kn': 'kannada',
            'gu': 'gujarati',
            'pa': 'punjabi',
            'ur': 'urdu',
            'fa': 'persian',
            'ru': 'russian',
            'el': 'greek',
            'he': 'hebrew',
            'my': 'myanmar',
            'km': 'khmer',
            'lo': 'lao',
            'ka': 'georgian',
            'hy': 'armenian',
            'am': 'ethiopic',
        }
        
        current_feature = current_lang_mapping.get(current_language)
        if detected_features and current_feature not in detected_features:
            return True
    
    # å…¶ä»–æƒ…å†µä¸‹ï¼Œå¦‚æœå·²æœ‰è¯­è¨€è®¾ç½®ï¼Œè·³è¿‡æ£€æµ‹ä»¥æå‡æ€§èƒ½
    return False

async def detect_and_update_language(user_message: str, current_user_data: dict, user_ip: str = "") -> dict:
    """
    ä½¿ç”¨ä¸“é—¨çš„AIæ£€æµ‹ç”¨æˆ·è¾“å…¥çš„è¯­è¨€å¹¶æ›´æ–°languageå­—æ®µï¼ˆæ”¯æŒæ™ºèƒ½ç¼“å­˜ï¼‰
    """
    current_language = current_user_data.get("language", "English")

    # æ£€æŸ¥æ˜¯å¦éœ€è¦è¿›è¡Œè¯­è¨€æ£€æµ‹
    if not should_detect_language(user_ip, user_message, current_language):
        logger.info(f"è·³è¿‡è¯­è¨€æ£€æµ‹: æ¶ˆæ¯è¿‡çŸ­æˆ–ä¸éœ€è¦æ£€æµ‹")
        return current_user_data
    
    # å¢å¼ºçš„è¯­è¨€ç¼“å­˜æ£€æŸ¥
    if user_ip:
        cache_key = f"lang_{user_ip}"
        # æ£€æŸ¥ä¸“ç”¨è¯­è¨€ç¼“å­˜
        if cache_key in _language_cache:
            cache_entry = _language_cache[cache_key]
            if time.time() - cache_entry['timestamp'] < 86400:  # 24å°æ—¶ç¼“å­˜
                cached_language = cache_entry['language']
                if cached_language and cached_language != "English":
                    if cached_language != current_language:
                        logger.info(f"âš¡ ä½¿ç”¨è¯­è¨€ç¼“å­˜: {current_language} -> {cached_language}")
                        current_user_data["language"] = cached_language
                    return current_user_data
            else:
                del _language_cache[cache_key]
        
        # æ£€æŸ¥æ—§ç‰ˆå…¨å±€ç¼“å­˜ï¼ˆå…¼å®¹æ€§ï¼‰
        if cache_key in _global_cache:
            cache_entry = _global_cache[cache_key]
            if time.time() - cache_entry['timestamp'] < 86400:  # 24å°æ—¶ç¼“å­˜
                cached_language = cache_entry['language']
                if cached_language and cached_language != "English":
                    if cached_language != current_language:
                        logger.info(f"ä½¿ç”¨ç¼“å­˜çš„è¯­è¨€è®¾ç½®: {current_language} -> {cached_language}")
                        current_user_data["language"] = cached_language
                    return current_user_data
            else:
                del _global_cache[cache_key]

    # åˆ›å»ºå¢å¼ºçš„è¯­è¨€æ£€æµ‹æç¤ºè¯ï¼Œæ”¯æŒæ›´å¤šå°è¯­ç§
    language_detection_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¤šè¯­è¨€æ£€æµ‹åŠ©æ‰‹ã€‚è¯·åˆ†æç”¨æˆ·è¾“å…¥æ–‡æœ¬ï¼Œè¯†åˆ«å…¶ä¸»è¦ä½¿ç”¨çš„è¯­è¨€ã€‚

ç”¨æˆ·è¾“å…¥ï¼š"{user_message}"
å½“å‰languageå­—æ®µï¼š{current_language}

æ£€æµ‹è§„åˆ™ï¼š
1. è¯†åˆ«ç”¨æˆ·è¾“å…¥çš„ä¸»è¦è¯­è¨€ï¼ˆåŒ…æ‹¬å„ç§å¤§è¯­ç§å’Œå°è¯­ç§ï¼‰
2. ä½¿ç”¨æ ‡å‡†çš„è¯­è¨€ä»£ç æ ¼å¼ï¼š
   - ä¸­æ–‡ï¼ˆç®€ä½“/ç¹ä½“ï¼‰â†’ "zh"
   - è‹±æ–‡ â†’ "English"  
   - æ—¥æ–‡ â†’ "ja"
   - éŸ©æ–‡ â†’ "ko"
   - é˜¿æ‹‰ä¼¯æ–‡ â†’ "ar"
   - æ³°æ–‡ â†’ "th"
   - è¶Šå—æ–‡ â†’ "vi"
   - å°åœ°æ–‡ â†’ "hi"
   - å­ŸåŠ æ‹‰æ–‡ â†’ "bn"
   - æ³°ç±³å°”æ–‡ â†’ "ta"
   - æ³°å¢å›ºæ–‡ â†’ "te"
   - é©¬æ‹‰é›…æ‹‰å§†æ–‡ â†’ "ml"
   - å¡çº³è¾¾æ–‡ â†’ "kn"
   - å¤å‰æ‹‰ç‰¹æ–‡ â†’ "gu"
   - æ—é®æ™®æ–‡ â†’ "pa"
   - ä¹Œå°”éƒ½æ–‡ â†’ "ur"
   - æ³¢æ–¯æ–‡ â†’ "fa"
   - ä¿„æ–‡ â†’ "ru"
   - å¸Œè…Šæ–‡ â†’ "el"
   - å¸Œä¼¯æ¥æ–‡ â†’ "he"
   - ç¼…ç”¸æ–‡ â†’ "my"
   - é«˜æ£‰æ–‡ â†’ "km"
   - è€æŒæ–‡ â†’ "lo"
   - æ ¼é²å‰äºšæ–‡ â†’ "ka"
   - äºšç¾å°¼äºšæ–‡ â†’ "hy"
   - é˜¿å§†å“ˆæ‹‰æ–‡ â†’ "am"
   - å…¶ä»–æ¬§æ´²è¯­è¨€ï¼šesã€frã€deã€itã€ptã€nlã€svã€noã€daã€fiã€plã€trã€csã€skã€slã€hrã€bgã€mkã€srã€bsã€isã€gaã€cyã€euã€caã€glã€mtã€lvã€ltã€etã€sqã€afç­‰

3. ç‰¹æ®Šå¤„ç†ï¼š
   - æ··åˆè¯­è¨€æ–‡æœ¬ï¼šé€‰æ‹©å ä¸»å¯¼åœ°ä½çš„è¯­è¨€
   - æ— æ³•ç¡®å®šæˆ–çº¯ç¬¦å·/æ•°å­—ï¼šä¿æŒå½“å‰languageè®¾ç½®
   - æ–‡æœ¬è¿‡çŸ­ä½†æœ‰æ˜ç¡®ç‰¹å¾ï¼šä¼˜å…ˆæ£€æµ‹ç»“æœ
   - å°è¯­ç§ä¼˜å…ˆï¼šå¦‚æœæ£€æµ‹åˆ°å°è¯­ç§ç‰¹å¾ï¼Œä¼˜å…ˆè¯†åˆ«ä¸ºå°è¯­ç§

4. å°è¯­ç§è¯†åˆ«è¦ç‚¹ï¼š
   - ä»”ç»†è¯†åˆ«å—äºšè¯­è¨€ï¼ˆå°åœ°æ–‡ã€å­ŸåŠ æ‹‰æ–‡ã€æ³°ç±³å°”æ–‡ç­‰ï¼‰
   - å‡†ç¡®åŒºåˆ†ä¸œå—äºšè¯­è¨€ï¼ˆæ³°æ–‡ã€è¶Šå—æ–‡ã€ç¼…ç”¸æ–‡ã€é«˜æ£‰æ–‡ç­‰ï¼‰
   - æ­£ç¡®è¯†åˆ«ä¸­ä¸œè¯­è¨€ï¼ˆé˜¿æ‹‰ä¼¯æ–‡ã€æ³¢æ–¯æ–‡ã€ä¹Œå°”éƒ½æ–‡ã€å¸Œä¼¯æ¥æ–‡ç­‰ï¼‰
   - ç²¾ç¡®åŒºåˆ†ä¸œäºšè¯­è¨€ï¼ˆä¸­æ–‡ã€æ—¥æ–‡ã€éŸ©æ–‡ï¼‰

è¾“å‡ºè¦æ±‚ï¼š
- ä»…è¾“å‡ºè¯­è¨€ä»£ç ï¼Œä¸è¦è§£é‡Š
- ç¤ºä¾‹ï¼šzh æˆ– English æˆ– es æˆ– ja æˆ– th æˆ– hi"""

    try:
        # è°ƒç”¨è¯­è¨€æ£€æµ‹AI - ä½¿ç”¨HumanMessageè€Œä¸æ˜¯SystemMessage
        detection_response = await llm_for_language_detection.ainvoke([
            HumanMessage(content=language_detection_prompt)
        ])

        detected_language = detection_response.content.strip()
        logger.info(f"è¯­è¨€æ£€æµ‹AIç»“æœ: '{detected_language}' (åŸè¯­è¨€: {current_language})")

        # éªŒè¯æ£€æµ‹ç»“æœæ˜¯å¦æœ‰æ•ˆï¼ˆç®€å•éªŒè¯ï¼šéç©ºä¸”åˆç†é•¿åº¦ï¼‰
        if detected_language and len(detected_language) <= 10 and detected_language.replace("-", "").replace("_",
                                                                                                             "").isalnum():
            # ç¼“å­˜æ£€æµ‹ç»“æœåˆ°ä¸“ç”¨è¯­è¨€ç¼“å­˜
            if user_ip and detected_language != "English":  # åªç¼“å­˜éé»˜è®¤è¯­è¨€
                cache_key = f"lang_{user_ip}"
                _language_cache[cache_key] = {
                    'language': detected_language,
                    'timestamp': time.time()
                }
                # é™åˆ¶è¯­è¨€ç¼“å­˜å¤§å°
                if len(_language_cache) > 1000:
                    oldest_key = min(_language_cache.keys(), 
                                   key=lambda k: _language_cache[k]['timestamp'])
                    del _language_cache[oldest_key]
            
            # å¦‚æœæ£€æµ‹åˆ°çš„è¯­è¨€ä¸å½“å‰ä¸åŒï¼Œæ›´æ–°æ•°æ®
            if detected_language != current_language:
                logger.info(f"è¯­è¨€æ›´æ–°: {current_language} -> {detected_language}")
                current_user_data["language"] = detected_language
                return current_user_data

        # å¦‚æœæ£€æµ‹ç»“æœæ— æ•ˆæˆ–ç›¸åŒï¼Œä¿æŒåŸçŠ¶
        logger.info(f"è¯­è¨€ä¿æŒä¸å˜: {current_language}")
        return current_user_data

    except Exception as e:
        logger.error(f"è¯­è¨€æ£€æµ‹AIè°ƒç”¨å¤±è´¥: {e}")
        # å‡ºé”™æ—¶ä¿æŒåŸæœ‰è¯­è¨€è®¾ç½®
        return current_user_data


async def send_customer_data_to_api(user_ip: str, user_data_dict: dict):
    """å‘é€ç”¨æˆ·æ•°æ®åˆ°å®¢æˆ·ä¿¡æ¯åˆ›å»ºAPI"""
    try:
        # æ˜ å°„ç”¨æˆ·æ•°æ®åˆ°APIè¦æ±‚çš„æ ¼å¼ï¼Œç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯å­—ç¬¦ä¸²ç±»å‹ä¸”å¤„ç†Noneå€¼
        def safe_get_str(value, default="0"):
            """å®‰å…¨è·å–å­—ç¬¦ä¸²å€¼ï¼Œå¤„ç†Noneæƒ…å†µ"""
            if value is None or value == "":
                return default
            return str(value)
        
        api_data = {
            "ipAddress": user_ip,
            "name": safe_get_str(user_data_dict.get("name"), ""),
            "age": safe_get_str(user_data_dict.get("age"), "0"),
            "phone": safe_get_str(user_data_dict.get("phone"), ""),
            "email": safe_get_str(user_data_dict.get("email"), ""),
            "country": safe_get_str(user_data_dict.get("country"), ""),
            "language": safe_get_str(user_data_dict.get("language"), "zh"),
            "firstMedicalOpinion": safe_get_str(user_data_dict.get("firstMedicalOpinion"), "0"),
            "medicalAttachments": safe_get_str(user_data_dict.get("medicalAttachments"), "0"),
            "preferredCity": safe_get_str(user_data_dict.get("preferredCity"), "0"),
            "preferredHospital": safe_get_str(user_data_dict.get("preferredHospital"), "0"),
            "treatmentBudget": safe_get_str(user_data_dict.get("treatmentBudget"), "0"),
            "treatmentPreference": safe_get_str(user_data_dict.get("treatmentPreference"), "0"),
            "urgencyLevel": safe_get_str(user_data_dict.get("urgencyLevel"), "0"),
            "consultationType": safe_get_str(user_data_dict.get("consultationType"), "0"),
            "remark": safe_get_str(user_data_dict.get("remark"), "0"),
        }

        logger.info(f"å‡†å¤‡å‘é€ç”¨æˆ·æ•°æ®åˆ°API: {api_data}")

        # å‘é€HTTP POSTè¯·æ±‚
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.post(
                "http://localhost:48080/admin-api/datamanagement/customer-info/create", #ä¼ å…¥æ•°æ®åœ°å€
                json=api_data,
                headers={
                    "Content-Type": "application/json",
                    "tenant-id": "1"
                }
            )

            if response.status_code == 200:
                logger.info(f"æˆåŠŸå‘é€ç”¨æˆ·æ•°æ®åˆ°API: ç”¨æˆ·IP={user_ip}, å“åº”={response.text}")
            else:
                logger.error(f"å‘é€ç”¨æˆ·æ•°æ®åˆ°APIå¤±è´¥: çŠ¶æ€ç ={response.status_code}, å“åº”={response.text}")

    except Exception as e:
        logger.error(f"å‘é€ç”¨æˆ·æ•°æ®åˆ°APIæ—¶å‘ç”Ÿå¼‚å¸¸: {e}")


def clean_tool_messages_from_record(record: list) -> list:
    """
    æ¸…ç†å¯¹è¯è®°å½•ä¸­çš„å·¥å…·ç›¸å…³æ¶ˆæ¯ï¼Œé¿å…DashScopeæŠ¥é”™ï¼š
    "An assistant message with tool_calls must be followed by tool messages"
    
    - åˆ é™¤æ‰€æœ‰ ToolMessage
    - åŒæ—¶åˆ é™¤åŒ…å« tool_calls çš„ AIMessageï¼ˆé˜²æ­¢å‡ºç°â€œç¼ºå°‘å¯¹åº”å·¥å…·å“åº”â€çš„å†å²æ¶ˆæ¯ï¼‰
    
    Args:
        record: åŒ…å«å„ç§æ¶ˆæ¯ç±»å‹çš„å¯¹è¯è®°å½•åˆ—è¡¨
        
    Returns:
        list: åˆ é™¤å·¥å…·æ¶ˆæ¯å’Œç›¸å…³assistantå·¥å…·è°ƒç”¨åçš„å¯¹è¯è®°å½•
    """
    cleaned_record = []
    for msg in record:
        # è·³è¿‡å·¥å…·æ¶ˆæ¯
        if isinstance(msg, ToolMessage):
            continue

        # åˆ é™¤åŒ…å«tool_callsçš„AIæ¶ˆæ¯ï¼Œé¿å…å†å²ä¸­å‡ºç°æœªé…å¯¹çš„å·¥å…·è°ƒç”¨
        try:
            from langchain_core.messages import AIMessage
            if isinstance(msg, AIMessage):
                has_native_tool_calls = bool(getattr(msg, "tool_calls", None))
                has_kw_tool_calls = bool(getattr(msg, "additional_kwargs", {}) and msg.additional_kwargs.get("tool_calls"))
                if has_native_tool_calls or has_kw_tool_calls:
                    continue
        except Exception:
            pass

        cleaned_record.append(msg)

    logger.info(f"æ¸…ç†å·¥å…·æ¶ˆæ¯: åŸå§‹è®°å½•{len(record)}æ¡ï¼Œæ¸…ç†å{len(cleaned_record)}æ¡")
    return cleaned_record


def cleanup_database_record(database: dict, user_ip: str) -> None:
    """
    æ¸…ç†æ•°æ®åº“ä¸­æŒ‡å®šç”¨æˆ·çš„å·¥å…·è®°å½•
    
    Args:
        database: ç”¨æˆ·æ•°æ®åº“
        user_ip: ç”¨æˆ·IPåœ°å€
    """
    if user_ip in database and "record" in database[user_ip]:
        original_count = len(database[user_ip]["record"])
        database[user_ip]["record"] = clean_tool_messages_from_record(database[user_ip]["record"])
        cleaned_count = len(database[user_ip]["record"])
        logger.info(f"ç”¨æˆ· {user_ip} çš„è®°å½•å·²æ¸…ç†: {original_count} -> {cleaned_count}")

