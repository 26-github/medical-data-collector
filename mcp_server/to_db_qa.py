import os
import glob
import re
import pandas as pd
import time
from typing import List, Dict, Any, Tuple
from dotenv import load_dotenv
from langchain_chroma import Chroma
from langchain_community.document_loaders import UnstructuredPDFLoader, PyMuPDFLoader
from langchain_openai import ChatOpenAI
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_core.prompts import PromptTemplate
import requests


# =============================================================================
# QAç”Ÿæˆç›¸å…³é…ç½®å’Œæç¤ºæ¨¡æ¿
# =============================================================================
load_dotenv()

# æ£€æŸ¥é˜¿é‡Œäº‘APIå¯†é’¥
dashscope_api_key = os.getenv("DASHSCOPE_API_KEY")
if not dashscope_api_key:
    print("âŒ é˜¿é‡Œäº‘DASHSCOPE_API_KEYæœªè®¾ç½®ï¼")
    print("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ï¼š")
    print("  DASHSCOPE_API_KEY = æ‚¨çš„é˜¿é‡Œäº‘DashScope API Key")
    print("\nè·å–æ–¹å¼ï¼š")
    print("1. è®¿é—® https://dashscope.console.aliyun.com/")
    print("2. åˆ›å»ºæˆ–é€‰æ‹©åº”ç”¨ï¼Œè·å–API Key")
    print("3. è®¾ç½®ç¯å¢ƒå˜é‡æˆ–åœ¨.envæ–‡ä»¶ä¸­é…ç½®")
    print("\nç¤ºä¾‹é…ç½®(.envæ–‡ä»¶)ï¼š")
    print("DASHSCOPE_API_KEY=sk-***********")
    import sys
    sys.exit(1)

print(f"âœ… é˜¿é‡Œäº‘APIå¯†é’¥é…ç½®æ£€æŸ¥é€šè¿‡")
print(f"   API Key: {dashscope_api_key[:8]}***ï¼ˆå·²è„±æ•æ˜¾ç¤ºï¼‰")

# åˆå§‹åŒ–é˜¿é‡Œäº‘DashScopeæ¨¡å‹
llm_for_qa_generation = ChatOpenAI(
    model="qwen-max",
    temperature=0.1,
    openai_api_base="https://dashscope.aliyuncs.com/compatible-mode/v1",
    openai_api_key=dashscope_api_key,
)

# QAç”Ÿæˆæç¤ºæ¨¡æ¿
QA_GENERATION_PROMPT = PromptTemplate(
    template="""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„åŒ»å­¦æ•™è‚²ä¸“å®¶ã€‚è¯·åŸºäºä»¥ä¸‹åŒ»å­¦æ–‡æ¡£å†…å®¹ï¼Œç”Ÿæˆé«˜è´¨é‡çš„é—®ç­”å¯¹ï¼Œç”¨äºåŒ»å­¦æ•™è‚²å’Œä¸´åºŠå‚è€ƒã€‚

æ–‡æ¡£å†…å®¹ï¼š
{content}

è¯·æ ¹æ®ä¸Šè¿°å†…å®¹ç”Ÿæˆ3-5ä¸ªé—®ç­”å¯¹ï¼Œè¦æ±‚ï¼š
1. é—®é¢˜åº”è¯¥æ˜¯ä¸´åºŠå®è·µä¸­å¸¸è§çš„ã€æœ‰å®é™…æ„ä¹‰çš„é—®é¢˜
2. ç­”æ¡ˆå¿…é¡»åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹ï¼Œå‡†ç¡®ä¸”è¯¦ç»†
3. é—®é¢˜ç±»å‹è¦å¤šæ ·åŒ–ï¼šè¯Šæ–­ã€æ²»ç–—ã€ç—‡çŠ¶ã€æ£€æŸ¥ã€é¢„åç­‰
4. ç­”æ¡ˆè¦ä¸“ä¸šä½†æ˜“æ‡‚ï¼Œé€‚åˆåŒ»å­¦å­¦ä¹ å’Œä¸´åºŠå‚è€ƒ
5. é¿å…ç”Ÿæˆè¿‡äºç®€å•æˆ–è¿‡äºå¤æ‚çš„é—®é¢˜

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š
é—®é¢˜1ï¼š[é—®é¢˜å†…å®¹]
ç­”æ¡ˆ1ï¼š[ç­”æ¡ˆå†…å®¹]

é—®é¢˜2ï¼š[é—®é¢˜å†…å®¹]
ç­”æ¡ˆ2ï¼š[ç­”æ¡ˆå†…å®¹]

é—®é¢˜3ï¼š[é—®é¢˜å†…å®¹]
ç­”æ¡ˆ3ï¼š[ç­”æ¡ˆå†…å®¹]

[ç»§ç»­æŒ‰éœ€ç”Ÿæˆæ›´å¤šQAå¯¹...]
""",
    input_variables=["content"]
)

# é¢„å®šä¹‰çš„ç›®æ ‡QAé›†åˆï¼ˆå¸¸è§åŒ»å­¦é—®é¢˜ï¼‰
TARGET_QA_PAIRS = [
    {
        "question": "å† å¿ƒç—…çš„å…¸å‹ç—‡çŠ¶æœ‰å“ªäº›ï¼Ÿ",
        "answer": "å† å¿ƒç—…çš„å…¸å‹ç—‡çŠ¶ä¸»è¦åŒ…æ‹¬ï¼š1) èƒ¸ç—›æˆ–èƒ¸é—·ï¼šå…¸å‹çš„å¿ƒç»ç—›è¡¨ç°ä¸ºèƒ¸éª¨åå‹æ¦¨æ€§ç–¼ç—›ï¼Œå¸¸åœ¨æ´»åŠ¨æ—¶å‡ºç°ï¼Œä¼‘æ¯åç¼“è§£ï¼›2) æ´»åŠ¨åæ°”ä¿ƒï¼šè¿åŠ¨è€åŠ›ä¸‹é™ï¼Œè½»åº¦æ´»åŠ¨å³æ„Ÿæ°”çŸ­ï¼›3) å¿ƒæ‚¸ï¼šå¿ƒç‡ä¸é½æˆ–å¿ƒåŠ¨è¿‡é€Ÿçš„æ„Ÿè§‰ï¼›4) ç–²åŠ³ä¹åŠ›ï¼šæ—¥å¸¸æ´»åŠ¨èƒ½åŠ›ä¸‹é™ï¼›5) æ”¾å°„ç—›ï¼šç–¼ç—›å¯æ”¾å°„è‡³å·¦è‚©ã€å·¦è‡‚ã€é¢ˆéƒ¨æˆ–ä¸‹é¢Œã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œéƒ¨åˆ†æ‚£è€…å¯èƒ½å‡ºç°ä¸å…¸å‹ç—‡çŠ¶ï¼Œå¦‚ä¸Šè…¹ç—›ã€æ¶ˆåŒ–ä¸è‰¯ç­‰ã€‚",
        "category": "ç—‡çŠ¶è¯†åˆ«"
    },
    {
        "question": "å¦‚ä½•è¯Šæ–­ç¨³å®šå‹å¿ƒç»ç—›ï¼Ÿ",
        "answer": "ç¨³å®šå‹å¿ƒç»ç—›çš„è¯Šæ–­ä¸»è¦åŸºäºï¼š1) ä¸´åºŠç—‡çŠ¶ï¼šå…¸å‹çš„èƒ¸ç—›ç‰¹ç‚¹ï¼ˆéƒ¨ä½ã€æ€§è´¨ã€è¯±å‘å› ç´ ã€æŒç»­æ—¶é—´ã€ç¼“è§£æ–¹å¼ï¼‰ï¼›2) å¿ƒç”µå›¾æ£€æŸ¥ï¼šé™æ¯å¿ƒç”µå›¾å¯èƒ½æ­£å¸¸ï¼Œè¿åŠ¨è´Ÿè·è¯•éªŒå¯è¯±å‘STæ®µæ”¹å˜ï¼›3) è¶…å£°å¿ƒåŠ¨å›¾ï¼šè¯„ä¼°å¿ƒè„ç»“æ„å’ŒåŠŸèƒ½ï¼›4) å† çŠ¶åŠ¨è„‰é€ å½±ï¼šé‡‘æ ‡å‡†ï¼Œç›´æ¥æ˜¾ç¤ºå† è„‰ç‹­çª„æƒ…å†µï¼›5) å† è„‰CTï¼šæ— åˆ›æ€§æ£€æŸ¥ï¼Œé€‚ç”¨äºä¸­ä½é£é™©æ‚£è€…ï¼›6) æ ¸åŒ»å­¦æ£€æŸ¥ï¼šå¿ƒè‚ŒçŒæ³¨æ˜¾åƒç­‰ã€‚è¯Šæ–­éœ€è¦ç»¼åˆä¸´åºŠè¡¨ç°å’Œè¾…åŠ©æ£€æŸ¥ç»“æœã€‚",
        "category": "è¯Šæ–­æ–¹æ³•"
    },
    {
        "question": "å† å¿ƒç—…çš„è¯ç‰©æ²»ç–—åŸåˆ™æ˜¯ä»€ä¹ˆï¼Ÿ",
        "answer": "å† å¿ƒç—…çš„è¯ç‰©æ²»ç–—éµå¾ªä»¥ä¸‹åŸåˆ™ï¼š1) æŠ—è¡€å°æ¿æ²»ç–—ï¼šé˜¿å¸åŒ¹æ—ä¸ºåŸºç¡€ï¼Œæ€¥æ€§æœŸå¯è”åˆæ°¯å¡æ ¼é›·ï¼›2) ä»–æ±€ç±»è¯ç‰©ï¼šé™è„‚ç¨³æ–‘ï¼Œç›®æ ‡LDL-C<1.8mmol/Lï¼›3) ACEI/ARBï¼šæ”¹å–„é¢„åï¼Œç‰¹åˆ«é€‚ç”¨äºåˆå¹¶ç³–å°¿ç—…æˆ–å¿ƒåŠŸèƒ½ä¸å…¨æ‚£è€…ï¼›4) Î²å—ä½“é˜»æ»å‰‚ï¼šå‡æ…¢å¿ƒç‡ï¼Œå‡å°‘å¿ƒè‚Œè€—æ°§é‡ï¼›5) ç¡é…¸é…¯ç±»ï¼šç¼“è§£å¿ƒç»ç—›ç—‡çŠ¶ï¼›6) é’™é€šé“é˜»æ»å‰‚ï¼šé€‚ç”¨äºå˜å¼‚æ€§å¿ƒç»ç—›æˆ–ä¸è€å—Î²å—ä½“é˜»æ»å‰‚çš„æ‚£è€…ã€‚æ²»ç–—éœ€è¦ä¸ªä½“åŒ–ï¼Œæ ¹æ®æ‚£è€…å…·ä½“æƒ…å†µè°ƒæ•´ç”¨è¯æ–¹æ¡ˆã€‚",
        "category": "æ²»ç–—æ–¹æ¡ˆ"
    },
    {
        "question": "å† çŠ¶åŠ¨è„‰é€ å½±çš„é€‚åº”ç—‡æœ‰å“ªäº›ï¼Ÿ",
        "answer": "å† çŠ¶åŠ¨è„‰é€ å½±çš„é€‚åº”ç—‡åŒ…æ‹¬ï¼š1) æ€¥æ€§å† è„‰ç»¼åˆå¾ï¼šæ€¥æ€§å¿ƒè‚Œæ¢—æ­»ã€ä¸ç¨³å®šæ€§å¿ƒç»ç—›ï¼›2) ç¨³å®šæ€§å¿ƒç»ç—›è¯ç‰©æ²»ç–—æ•ˆæœä¸ä½³ï¼›3) éä¾µå…¥æ€§æ£€æŸ¥æç¤ºå­˜åœ¨æ˜¾è‘—å¿ƒè‚Œç¼ºè¡€ï¼›4) å¿ƒè„çŒæ­»å¤è‹åï¼›5) æœ¯å‰è¯„ä¼°ï¼šå¿ƒè„æ‰‹æœ¯å‰çš„å† è„‰è¯„ä¼°ï¼›6) æ€€ç–‘å† è„‰å¼‚å¸¸ï¼šå…ˆå¤©æ€§å† è„‰å¼‚å¸¸ã€å† è„‰ç˜˜ç­‰ï¼›7) å¿ƒåŠŸèƒ½ä¸å…¨åŸå› ä¸æ˜éœ€è¦æ’é™¤ç¼ºè¡€æ€§å¿ƒè‚Œç—…ï¼›8) èŒä¸šéœ€è¦ï¼šé£è¡Œå‘˜ã€å¸æœºç­‰ç‰¹æ®ŠèŒä¸šçš„å† è„‰è¯„ä¼°ã€‚é€ å½±å‰éœ€è¦å……åˆ†è¯„ä¼°é£é™©æ•ˆç›Šæ¯”ã€‚",
        "category": "æ£€æŸ¥é€‚åº”ç—‡"
    },
    {
        "question": "æ€¥æ€§å¿ƒè‚Œæ¢—æ­»çš„æ€¥æ•‘å¤„ç†åŸåˆ™æ˜¯ä»€ä¹ˆï¼Ÿ",
        "answer": "æ€¥æ€§å¿ƒè‚Œæ¢—æ­»çš„æ€¥æ•‘å¤„ç†åŸåˆ™ï¼š1) ç«‹å³è¯„ä¼°ï¼š12å¯¼è”å¿ƒç”µå›¾ï¼Œè¯„ä¼°è¡€æµåŠ¨åŠ›å­¦çŠ¶æ€ï¼›2) è¿…é€Ÿå†çŒæ³¨ï¼šå‘ç—…12å°æ—¶å†…é¦–é€‰PCIï¼Œä¸å…·å¤‡æ¡ä»¶æ—¶æº¶æ “æ²»ç–—ï¼›3) æŠ—æ “æ²»ç–—ï¼šåŒé‡æŠ—è¡€å°æ¿+æŠ—å‡æ²»ç–—ï¼›4) å¯¹ç—‡å¤„ç†ï¼šé•‡ç—›ï¼ˆå—å•¡ï¼‰ã€å¸æ°§ã€ç¡é…¸ç”˜æ²¹ç­‰ï¼›5) å¹¶å‘ç—‡ç›‘æµ‹ï¼šå¿ƒå¾‹å¤±å¸¸ã€å¿ƒåŠ›è¡°ç«­ã€æœºæ¢°å¹¶å‘ç—‡ç­‰ï¼›6) æ—©æœŸé£é™©åˆ†å±‚ï¼šè¯„ä¼°æ¢—æ­»é¢ç§¯ã€å·¦å¿ƒåŠŸèƒ½ç­‰ï¼›7) äºŒçº§é¢„é˜²ï¼šä»–æ±€ã€ACEI/ARBã€Î²å—ä½“é˜»æ»å‰‚ç­‰ã€‚å…³é”®æ˜¯'æ—¶é—´å°±æ˜¯å¿ƒè‚Œ'ï¼Œå°½å¿«æ¢å¤è¡€æµã€‚",
        "category": "æ€¥æ•‘å¤„ç†"
    },
    {
        "question": "å† å¿ƒç—…æ‚£è€…çš„ç”Ÿæ´»æ–¹å¼ç®¡ç†åŒ…æ‹¬å“ªäº›æ–¹é¢ï¼Ÿ",
        "answer": "å† å¿ƒç—…æ‚£è€…çš„ç”Ÿæ´»æ–¹å¼ç®¡ç†åŒ…æ‹¬ï¼š1) æˆ’çƒŸé™é…’ï¼šå®Œå…¨æˆ’çƒŸï¼Œé™åˆ¶é…’ç²¾æ‘„å…¥ï¼›2) é¥®é£Ÿæ§åˆ¶ï¼šä½ç›ä½è„‚é¥®é£Ÿï¼Œå¢åŠ è”¬èœæ°´æœæ‘„å…¥ï¼Œæ§åˆ¶æ€»çƒ­é‡ï¼›3) è§„å¾‹è¿åŠ¨ï¼šæœ‰æ°§è¿åŠ¨ä¸ºä¸»ï¼Œæ¯å‘¨150åˆ†é’Ÿä¸­ç­‰å¼ºåº¦è¿åŠ¨ï¼›4) ä½“é‡æ§åˆ¶ï¼šç»´æŒç†æƒ³ä½“é‡ï¼ŒBMI<24kg/mÂ²ï¼›5) è¡€å‹ç®¡ç†ï¼šç›®æ ‡è¡€å‹<130/80mmHgï¼›6) è¡€ç³–æ§åˆ¶ï¼šç³–å°¿ç—…æ‚£è€…HbA1c<7%ï¼›7) è¡€è„‚ç®¡ç†ï¼šLDL-C<1.8mmol/Lï¼›8) å‹åŠ›ç®¡ç†ï¼šä¿æŒè‰¯å¥½å¿ƒæ€ï¼Œé¿å…è¿‡åº¦ç´§å¼ ï¼›9) è§„å¾‹ä½œæ¯ï¼šå……è¶³ç¡çœ ï¼Œé¿å…ç†¬å¤œã€‚ç”Ÿæ´»æ–¹å¼å¹²é¢„æ˜¯å† å¿ƒç—…é˜²æ²»çš„åŸºç¡€ã€‚",
        "category": "ç”Ÿæ´»ç®¡ç†"
    }
]


# =============================================================================
# æ–‡æœ¬å‘é‡åŒ–å¤„ç†
# =============================================================================

class DashScopeEmbeddings:
    def __init__(self, api_key: str, model: str = "text-embedding-v4",
                 api_base: str = "https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings",
                 request_timeout_seconds: int = 30,
                 sleep_between_requests_seconds: float = 0.05):
        self.api_key = api_key
        self.model = model
        self.api_base = api_base.rstrip("/")
        self.timeout = request_timeout_seconds
        self.sleep_seconds = sleep_between_requests_seconds

    def embed_query(self, text: str) -> List[float]:
        payload = {"model": self.model, "input": text}
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }
        response = requests.post(self.api_base, headers=headers, json=payload, timeout=self.timeout)
        response.raise_for_status()
        data = response.json()
        if "data" in data and len(data["data"]) > 0 and "embedding" in data["data"][0]:
            return data["data"][0]["embedding"]
        raise ValueError(f"æ— æ•ˆçš„Embeddingå“åº”: {data}")

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        embeddings: List[List[float]] = []
        for text in texts:
            try:
                vector = self.embed_query(text)
            except Exception:
                vector = [0.0] * 1024
            embeddings.append(vector)
            if self.sleep_seconds and self.sleep_seconds > 0:
                time.sleep(self.sleep_seconds)
        return embeddings

# =============================================================================
# é€šç”¨æ–‡æœ¬å¤„ç†å’ŒéªŒè¯å‡½æ•°
# =============================================================================

def clean_text(text: str) -> str:
    """æ¸…ç†æ–‡æœ¬ï¼Œç§»é™¤æ— ç”¨å­—ç¬¦å’Œæ ¼å¼"""
    # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
    text = re.sub(r'\s+', ' ', text)
    # ç§»é™¤å•ç‹¬çš„ç‰¹æ®Šå­—ç¬¦
    text = re.sub(r'\s[^\w\u4e00-\u9fff]{1,3}\s', ' ', text)
    # ç§»é™¤è¿‡çŸ­çš„è‹±æ–‡å•è¯ç‰‡æ®µï¼ˆå¯èƒ½æ˜¯OCRé”™è¯¯ï¼‰
    text = re.sub(r'\b[a-zA-Z]{1,2}\b', '', text)
    # ç§»é™¤è¿ç»­çš„ç‰¹æ®Šå­—ç¬¦
    text = re.sub(r'[^\w\s\u4e00-\u9fffï¼Œã€‚ï¼›ï¼šï¼ï¼Ÿã€""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹]{2,}', ' ', text)
    # å†æ¬¡æ¸…ç†å¤šä½™ç©ºç™½
    text = re.sub(r'\s+', ' ', text)
    return text.strip()


def is_valid_content(content: str, min_length: int = 50) -> bool:
    """åˆ¤æ–­å†…å®¹æ˜¯å¦æœ‰æ•ˆ"""
    # å…ˆæ¸…ç†ä¸€ä¸‹å†…å®¹å†åˆ¤æ–­
    cleaned = clean_text(content)

    if len(cleaned) < min_length:
        return False

    # è®¡ç®—ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', cleaned))
    total_chars = len(cleaned)
    chinese_ratio = chinese_chars / total_chars if total_chars > 0 else 0

    # è®¡ç®—æœ‰æ„ä¹‰å•è¯çš„æ¯”ä¾‹
    words = cleaned.split()
    meaningful_words = [w for w in words if len(w) > 2 and re.search(r'[a-zA-Z\u4e00-\u9fff]', w)]
    word_ratio = len(meaningful_words) / len(words) if len(words) > 0 else 0

    # ä¸­æ–‡å†…å®¹æˆ–é«˜è´¨é‡è‹±æ–‡å†…å®¹éƒ½è®¤ä¸ºæœ‰æ•ˆ
    return chinese_ratio > 0.1 or (word_ratio > 0.7 and len(meaningful_words) > 5)


def validate_document_before_embedding(doc: Document, min_length: int = 100) -> bool:
    """åœ¨å‘é€åˆ°embeddings APIä¹‹å‰è¿›è¡Œæœ€ç»ˆéªŒè¯"""
    content = doc.page_content.strip()

    # åŸºæœ¬é•¿åº¦æ£€æŸ¥
    if len(content) < min_length:
        print(f"    æ–‡æ¡£å†…å®¹è¿‡çŸ­({len(content)}å­—ç¬¦)ï¼Œè·³è¿‡")
        return False

    # æ£€æŸ¥æ˜¯å¦åªåŒ…å«ç©ºç™½æˆ–ç‰¹æ®Šå­—ç¬¦
    meaningful_chars = re.sub(r'[\s\W]', '', content)
    if len(meaningful_chars) < min_length // 2:
        print(f"    æ–‡æ¡£ç¼ºä¹æœ‰æ„ä¹‰å†…å®¹ï¼Œè·³è¿‡")
        return False

    # æ£€æŸ¥æ˜¯å¦åŒ…å«è¶³å¤Ÿçš„ä¸­æ–‡æˆ–è‹±æ–‡å†…å®¹
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', content))
    english_chars = len(re.findall(r'[a-zA-Z]', content))

    if chinese_chars < 20 and english_chars < 50:
        print(f"    æ–‡æ¡£è¯­è¨€å†…å®¹ä¸è¶³ï¼Œè·³è¿‡")
        return False

    return True


# =============================================================================
# QAç”ŸæˆåŠŸèƒ½
# =============================================================================

def generate_qa_pairs_from_document(doc: Document) -> List[Document]:
    """ä»å•ä¸ªæ–‡æ¡£ç”ŸæˆQAå¯¹"""
    qa_documents = []

    try:
        # ç¡®ä¿æ–‡æ¡£å†…å®¹è¶³å¤Ÿé•¿ä¸”æœ‰æ•ˆ
        if len(doc.page_content) < 200 or not is_valid_content(doc.page_content, min_length=100):
            print(f"    æ–‡æ¡£å†…å®¹ä¸è¶³ï¼Œè·³è¿‡QAç”Ÿæˆ")
            return []

        print(f"    æ­£åœ¨ä¸ºæ–‡æ¡£ç”ŸæˆQAå¯¹...")

        # è°ƒç”¨LLMç”ŸæˆQAå¯¹
        prompt_input = {"content": doc.page_content[:4000]}  # é™åˆ¶é•¿åº¦é¿å…tokenè¶…é™
        response = llm_for_qa_generation.invoke(QA_GENERATION_PROMPT.format(**prompt_input))

        if not response or not response.content:
            print(f"    QAç”Ÿæˆå¤±è´¥ï¼šæ— æœ‰æ•ˆå“åº”")
            return []

        # è§£æç”Ÿæˆçš„QAå¯¹
        qa_pairs = parse_generated_qa_pairs(response.content)

        if not qa_pairs:
            print(f"    QAè§£æå¤±è´¥ï¼šæœªæ‰¾åˆ°æœ‰æ•ˆQAå¯¹")
            return []

        print(f"    æˆåŠŸç”Ÿæˆ {len(qa_pairs)} ä¸ªQAå¯¹")

        # ä¸ºæ¯ä¸ªQAå¯¹åˆ›å»ºDocumentå¯¹è±¡
        for i, (question, answer) in enumerate(qa_pairs):
            # åˆ›å»ºQAæ–‡æ¡£å†…å®¹
            qa_content = f"é—®é¢˜ï¼š{question}\n\nç­”æ¡ˆï¼š{answer}"

            # åˆ›å»ºQAæ–‡æ¡£çš„å…ƒæ•°æ®
            qa_metadata = {
                **doc.metadata,  # ç»§æ‰¿åŸæ–‡æ¡£å…ƒæ•°æ®
                'content_type': 'generated_qa',
                'qa_pair_index': i + 1,
                'question': question,
                'answer': answer,
                'source_content_length': len(doc.page_content),
                'generation_method': 'llm_generated',
                'file_type': 'qa_pair'
            }

            qa_doc = Document(
                page_content=qa_content,
                metadata=qa_metadata
            )

            qa_documents.append(qa_doc)

    except Exception as e:
        print(f"    QAç”Ÿæˆè¿‡ç¨‹å‡ºé”™: {e}")

    return qa_documents


def parse_generated_qa_pairs(qa_text: str) -> List[Tuple[str, str]]:
    """è§£æLLMç”Ÿæˆçš„QAæ–‡æœ¬"""
    qa_pairs = []

    try:
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–QAå¯¹
        # åŒ¹é… "é—®é¢˜Xï¼š...ç­”æ¡ˆXï¼š..." çš„æ¨¡å¼
        pattern = r'é—®é¢˜\d*[ï¼š:]\s*([^\n]+)\s*\n\s*ç­”æ¡ˆ\d*[ï¼š:]\s*([^é—®]+?)(?=é—®é¢˜\d*[ï¼š:]|$)'
        matches = re.findall(pattern, qa_text, re.DOTALL)

        for question, answer in matches:
            question = question.strip()
            answer = answer.strip()

            # éªŒè¯QAå¯¹çš„è´¨é‡
            if (len(question) > 10 and len(answer) > 20 and
                    question.endswith(('ï¼Ÿ', '?', 'å—', 'å‘¢')) and
                    is_valid_content(answer, min_length=20)):
                qa_pairs.append((question, answer))

        # å¦‚æœæ­£åˆ™è¡¨è¾¾å¼åŒ¹é…å¤±è´¥ï¼Œå°è¯•å…¶ä»–è§£ææ–¹æ³•
        if not qa_pairs:
            lines = qa_text.split('\n')
            current_question = None
            current_answer = ""

            for line in lines:
                line = line.strip()
                if not line:
                    continue

                if line.startswith(('é—®é¢˜', 'é¢˜ç›®')) and ('ï¼š' in line or ':' in line):
                    # ä¿å­˜ä¸Šä¸€ä¸ªQAå¯¹
                    if current_question and current_answer:
                        qa_pairs.append((current_question, current_answer.strip()))

                    # å¼€å§‹æ–°çš„é—®é¢˜
                    current_question = line.split('ï¼š', 1)[-1].split(':', 1)[-1].strip()
                    current_answer = ""

                elif line.startswith(('ç­”æ¡ˆ', 'å›ç­”')) and ('ï¼š' in line or ':' in line):
                    current_answer = line.split('ï¼š', 1)[-1].split(':', 1)[-1].strip()

                elif current_question and current_answer:
                    # ç»§ç»­æ”¶é›†ç­”æ¡ˆå†…å®¹
                    current_answer += " " + line

            # ä¿å­˜æœ€åä¸€ä¸ªQAå¯¹
            if current_question and current_answer:
                qa_pairs.append((current_question, current_answer.strip()))

    except Exception as e:
        print(f"    QAè§£æå‡ºé”™: {e}")

    return qa_pairs


def create_target_qa_documents() -> List[Document]:
    """åˆ›å»ºé¢„å®šä¹‰çš„ç›®æ ‡QAæ–‡æ¡£"""
    qa_documents = []

    print(f"ğŸ¯ åˆ›å»ºé¢„å®šä¹‰ç›®æ ‡QAæ–‡æ¡£é›†åˆ...")

    for i, qa_pair in enumerate(TARGET_QA_PAIRS):
        # åˆ›å»ºQAæ–‡æ¡£å†…å®¹
        qa_content = f"é—®é¢˜ï¼š{qa_pair['question']}\n\nç­”æ¡ˆï¼š{qa_pair['answer']}"

        # åˆ›å»ºå…ƒæ•°æ®
        qa_metadata = {
            'source': 'predefined_target_qa',
            'source_file': 'target_qa_collection',
            'content_type': 'target_qa',
            'qa_pair_index': i + 1,
            'question': qa_pair['question'],
            'answer': qa_pair['answer'],
            'category': qa_pair['category'],
            'generation_method': 'predefined',
            'file_type': 'qa_pair',
            'content_length': len(qa_content)
        }

        qa_doc = Document(
            page_content=qa_content,
            metadata=qa_metadata
        )

        qa_documents.append(qa_doc)

    print(f"âœ… æˆåŠŸåˆ›å»º {len(qa_documents)} ä¸ªç›®æ ‡QAæ–‡æ¡£")
    return qa_documents


# =============================================================================
# PDFå¤„ç†ç›¸å…³å‡½æ•°
# =============================================================================

def detect_chapter_patterns(text: str) -> List[str]:
    """æ£€æµ‹æ–‡æœ¬ä¸­çš„ç« èŠ‚æ ‡é¢˜æ¨¡å¼"""
    patterns = [
        # æ•°å­—ç¼–å·æ¨¡å¼ (1. 2. 3. æˆ– 1ã€2ã€3ã€)
        r'^[\s]*(\d+[.ã€][\s]*[^\n]{5,50})$',
        # å¤šçº§ç¼–å· (1.1 1.2 2.1 ç­‰)
        r'^[\s]*(\d+\.\d+[\s]*[^\n]{5,50})$',
        # ä¸­æ–‡æ•°å­— (ä¸€ã€äºŒã€ä¸‰ã€)
        r'^[\s]*([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡]+[ã€.][\s]*[^\n]{5,50})$',
        # æ‹¬å·ç¼–å· ((1) (2) (3))
        r'^[\s]*(\([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+\)[\s]*[^\n]{5,50})$',
        # è‹±æ–‡ç½—é©¬æ•°å­— (I. II. III.)
        r'^[\s]*([IVX]+[.][\s]*[^\n]{5,50})$',
        # å…¶ä»–å¯èƒ½çš„æ ‡é¢˜æ ¼å¼ï¼ˆå…¨å¤§å†™æˆ–åŒ…å«å…³é”®è¯ï¼‰
        r'^[\s]*([^\n]{5,30}[è¯Šæ–­|æ²»ç–—|ç®¡ç†|æŒ‡å—|å»ºè®®|åŸåˆ™|æ–¹æ³•|ç­–ç•¥|åˆ†æ])[\s]*$',
    ]

    detected_patterns = []
    lines = text.split('\n')

    for pattern in patterns:
        matches = []
        for line in lines:
            if re.match(pattern, line.strip(), re.MULTILINE):
                matches.append(line.strip())
        if len(matches) >= 2:  # è‡³å°‘è¦æœ‰2ä¸ªåŒ¹é…æ‰è®¤ä¸ºæ˜¯æœ‰æ•ˆæ¨¡å¼
            detected_patterns.append(pattern)

    return detected_patterns


def split_by_chapters(text: str, metadata: dict) -> List[Document]:
    """æŒ‰ç« èŠ‚åˆ‡åˆ†æ–‡æ¡£"""
    # æ£€æµ‹ç« èŠ‚æ¨¡å¼
    patterns = detect_chapter_patterns(text)

    if not patterns:
        print(f"    æœªæ£€æµ‹åˆ°æ˜æ˜¾çš„ç« èŠ‚æ¨¡å¼ï¼Œä½¿ç”¨ä¼ ç»Ÿåˆ‡åˆ†æ–¹å¼")
        # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°ç« èŠ‚æ¨¡å¼ï¼Œå›é€€åˆ°ä¼ ç»Ÿåˆ‡åˆ†
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=2000,
            chunk_overlap=400,
            separators=["\n\n", "\n", "ã€‚", "ï¼›", "ï¼", "ï¼Ÿ", ".", "!", "?", " ", ""]
        )
        temp_doc = Document(page_content=text, metadata=metadata)
        return text_splitter.split_documents([temp_doc])

    print(f"    æ£€æµ‹åˆ° {len(patterns)} ç§ç« èŠ‚æ¨¡å¼ï¼ŒæŒ‰ç« èŠ‚åˆ‡åˆ†")

    # ä½¿ç”¨ç¬¬ä¸€ä¸ªæœ€æœ‰æ•ˆçš„æ¨¡å¼è¿›è¡Œåˆ‡åˆ†
    primary_pattern = patterns[0]
    lines = text.split('\n')

    chapters = []
    current_chapter = []
    current_title = "å¼•è¨€"

    for line in lines:
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç« èŠ‚æ ‡é¢˜
        if re.match(primary_pattern, line.strip(), re.MULTILINE):
            # ä¿å­˜ä¸Šä¸€ç« èŠ‚
            if current_chapter:
                chapter_content = '\n'.join(current_chapter).strip()
                if is_valid_content(chapter_content, min_length=100):
                    chapter_doc = Document(
                        page_content=chapter_content,
                        metadata={
                            **metadata,
                            'chapter_title': current_title,
                            'split_method': 'chapter_based',
                            'file_type': 'pdf'
                        }
                    )
                    chapters.append(chapter_doc)

            # å¼€å§‹æ–°ç« èŠ‚
            current_title = line.strip()
            current_chapter = [line]
        else:
            current_chapter.append(line)

    # å¤„ç†æœ€åä¸€ç« 
    if current_chapter:
        chapter_content = '\n'.join(current_chapter).strip()
        if is_valid_content(chapter_content, min_length=100):
            chapter_doc = Document(
                page_content=chapter_content,
                metadata={
                    **metadata,
                    'chapter_title': current_title,
                    'split_method': 'chapter_based',
                    'file_type': 'pdf'
                }
            )
            chapters.append(chapter_doc)

    # å¦‚æœç« èŠ‚å¤ªå¤§ï¼Œè¿›ä¸€æ­¥ç»†åˆ†
    final_chapters = []
    for chapter in chapters:
        if len(chapter.page_content) > 3000:  # å¦‚æœç« èŠ‚è¿‡å¤§
            print(
                f"      ç« èŠ‚ '{chapter.metadata['chapter_title'][:20]}...' è¿‡å¤§({len(chapter.page_content)}å­—ç¬¦)ï¼Œè¿›ä¸€æ­¥ç»†åˆ†")
            # åœ¨ç« èŠ‚å†…ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•è¿›ä¸€æ­¥åˆ‡åˆ†
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=2000,
                chunk_overlap=200,
                separators=["\n\n", "\n", "ã€‚", "ï¼›", "ï¼", "ï¼Ÿ", ".", "!", "?", " ", ""]
            )
            sub_docs = text_splitter.split_documents([chapter])

            # ä¸ºå­æ–‡æ¡£æ·»åŠ ç« èŠ‚ä¿¡æ¯
            for i, sub_doc in enumerate(sub_docs):
                sub_doc.metadata.update({
                    'chapter_title': chapter.metadata['chapter_title'],
                    'sub_section': i + 1,
                    'split_method': 'chapter_then_size',
                    'file_type': 'pdf'
                })
            final_chapters.extend(sub_docs)
        else:
            final_chapters.append(chapter)

    return final_chapters


def load_pdf_with_multiple_strategies(pdf_path: str) -> List[Document]:
    """ä½¿ç”¨å¤šç§ç­–ç•¥åŠ è½½PDFï¼Œé€‰æ‹©æœ€ä½³ç»“æœ"""
    documents = []

    # ç­–ç•¥1ï¼šå°è¯•PyMuPDFï¼ˆé€šå¸¸å¯¹æ–‡æœ¬PDFæ•ˆæœæ›´å¥½ï¼‰
    try:
        print(f"  å°è¯•ç­–ç•¥1ï¼šPyMuPDF")
        loader = PyMuPDFLoader(pdf_path)
        docs = loader.load()
        if docs:
            combined_text = "\n".join([doc.page_content for doc in docs])
            cleaned_text = clean_text(combined_text)
            if is_valid_content(cleaned_text, min_length=100):
                print(f"  ç­–ç•¥1æˆåŠŸï¼Œæå–äº† {len(cleaned_text)} ä¸ªå­—ç¬¦")
                documents.append(Document(
                    page_content=cleaned_text,
                    metadata={"source": pdf_path, "extraction_method": "pymupdf", "file_type": "pdf"}
                ))
                return documents
    except Exception as e:
        print(f"  ç­–ç•¥1å¤±è´¥: {e}")

    # ç­–ç•¥2ï¼šä½¿ç”¨UnstructuredPDFLoaderçš„autoç­–ç•¥
    try:
        print(f"  å°è¯•ç­–ç•¥2ï¼šUnstructured auto")
        loader = UnstructuredPDFLoader(
            file_path=pdf_path,
            mode="elements",
            strategy="auto",  # æ”¹ä¸ºautoï¼Œè®©å®ƒè‡ªåŠ¨é€‰æ‹©æœ€ä½³ç­–ç•¥
        )
        docs = loader.load()
        if docs:
            combined_text = "\n".join([doc.page_content for doc in docs])
            cleaned_text = clean_text(combined_text)
            if is_valid_content(cleaned_text, min_length=100):
                print(f"  ç­–ç•¥2æˆåŠŸï¼Œæå–äº† {len(cleaned_text)} ä¸ªå­—ç¬¦")
                documents.append(Document(
                    page_content=cleaned_text,
                    metadata={"source": pdf_path, "extraction_method": "unstructured_auto", "file_type": "pdf"}
                ))
                return documents
    except Exception as e:
        print(f"  ç­–ç•¥2å¤±è´¥: {e}")

    # ç­–ç•¥3ï¼šæœ€åå°è¯•OCRï¼ˆä½œä¸ºå¤‡é€‰ï¼‰
    try:
        print(f"  å°è¯•ç­–ç•¥3ï¼šOCR")
        loader = UnstructuredPDFLoader(
            file_path=pdf_path,
            mode="elements",
            strategy="ocr_only",
        )
        docs = loader.load()
        if docs:
            combined_text = "\n".join([doc.page_content for doc in docs])
            cleaned_text = clean_text(combined_text)
            if is_valid_content(cleaned_text, min_length=50):  # OCRçš„è¦æ±‚é™ä½ä¸€äº›
                print(f"  ç­–ç•¥3æˆåŠŸï¼Œæå–äº† {len(cleaned_text)} ä¸ªå­—ç¬¦")
                documents.append(Document(
                    page_content=cleaned_text,
                    metadata={"source": pdf_path, "extraction_method": "ocr", "file_type": "pdf"}
                ))
                return documents
    except Exception as e:
        print(f"  ç­–ç•¥3å¤±è´¥: {e}")

    print(f"  æ‰€æœ‰ç­–ç•¥éƒ½å¤±è´¥")
    return []


# =============================================================================
# Excelå¤„ç†ç›¸å…³å‡½æ•°
# =============================================================================

def analyze_excel_structure(df: pd.DataFrame) -> Dict[str, Any]:
    """åˆ†æExcelè¡¨æ ¼ç»“æ„ï¼Œè¯†åˆ«æ•°æ®ç±»å‹å’Œæ¨¡å¼"""
    structure_info = {
        'column_types': {},
        'numeric_columns': [],
        'text_columns': [],
        'date_columns': [],
        'categorical_columns': [],
        'has_headers': True,
        'row_count': len(df),
        'col_count': len(df.columns),
        'empty_cells_ratio': 0,
        'data_density': 0
    }

    # åˆ†ææ¯åˆ—çš„æ•°æ®ç±»å‹å’Œç‰¹å¾
    for col in df.columns:
        col_data = df[col].dropna()
        if len(col_data) == 0:
            continue

        # æ£€æµ‹æ•°æ®ç±»å‹
        if pd.api.types.is_numeric_dtype(col_data):
            structure_info['numeric_columns'].append(col)
            structure_info['column_types'][col] = 'numeric'
        elif pd.api.types.is_datetime64_any_dtype(col_data):
            structure_info['date_columns'].append(col)
            structure_info['column_types'][col] = 'date'
        else:
            # æ–‡æœ¬åˆ—è¿›ä¸€æ­¥åˆ†æ
            unique_ratio = len(col_data.unique()) / len(col_data)
            if unique_ratio < 0.5:  # é‡å¤å€¼è¾ƒå¤šï¼Œå¯èƒ½æ˜¯åˆ†ç±»åˆ—
                structure_info['categorical_columns'].append(col)
                structure_info['column_types'][col] = 'categorical'
            else:
                structure_info['text_columns'].append(col)
                structure_info['column_types'][col] = 'text'

    # è®¡ç®—æ•°æ®å¯†åº¦
    total_cells = len(df) * len(df.columns)
    non_null_cells = df.count().sum()
    structure_info['data_density'] = non_null_cells / total_cells if total_cells > 0 else 0
    structure_info['empty_cells_ratio'] = 1 - structure_info['data_density']

    return structure_info


def create_table_summary(df: pd.DataFrame, sheet_name: str, structure_info: Dict[str, Any]) -> str:
    """åˆ›å»ºè¡¨æ ¼çš„ç»“æ„åŒ–æ‘˜è¦"""
    summary_parts = [
        f"=== è¡¨æ ¼æ‘˜è¦ï¼š{sheet_name} ===",
        f"æ•°æ®ç»´åº¦ï¼š{structure_info['row_count']}è¡Œ Ã— {structure_info['col_count']}åˆ—",
        f"æ•°æ®å®Œæ•´æ€§ï¼š{structure_info['data_density']:.1%} ï¼ˆ{structure_info['row_count'] * structure_info['col_count'] - df.isnull().sum().sum()}/{structure_info['row_count'] * structure_info['col_count']}ä¸ªéç©ºå•å…ƒæ ¼ï¼‰"
    ]

    # æ·»åŠ åˆ—ä¿¡æ¯
    if structure_info['numeric_columns']:
        summary_parts.append(
            f"æ•°å€¼åˆ— ({len(structure_info['numeric_columns'])}ä¸ª)ï¼š{', '.join(structure_info['numeric_columns'])}")

    if structure_info['categorical_columns']:
        summary_parts.append(
            f"åˆ†ç±»åˆ— ({len(structure_info['categorical_columns'])}ä¸ª)ï¼š{', '.join(structure_info['categorical_columns'])}")

    if structure_info['text_columns']:
        summary_parts.append(
            f"æ–‡æœ¬åˆ— ({len(structure_info['text_columns'])}ä¸ª)ï¼š{', '.join(structure_info['text_columns'])}")

    if structure_info['date_columns']:
        summary_parts.append(
            f"æ—¥æœŸåˆ— ({len(structure_info['date_columns'])}ä¸ª)ï¼š{', '.join(structure_info['date_columns'])}")

    # æ·»åŠ æ•°æ®ç»Ÿè®¡
    numeric_stats = []
    for col in structure_info['numeric_columns'][:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªæ•°å€¼åˆ—çš„ç»Ÿè®¡
        col_data = df[col].dropna()
        if len(col_data) > 0:
            stats = f"{col}ï¼šæœ€å°å€¼{col_data.min():.2f}, æœ€å¤§å€¼{col_data.max():.2f}, å¹³å‡å€¼{col_data.mean():.2f}"
            numeric_stats.append(stats)

    if numeric_stats:
        summary_parts.append("ä¸»è¦æ•°å€¼åˆ—ç»Ÿè®¡ï¼š")
        summary_parts.extend([f"  {stat}" for stat in numeric_stats])

    return "\n".join(summary_parts)


def format_table_data_intelligently(df: pd.DataFrame, structure_info: Dict[str, Any], start_row: int = 0,
                                    max_rows: int = 20) -> str:
    """æ™ºèƒ½æ ¼å¼åŒ–è¡¨æ ¼æ•°æ®ï¼Œä¿æŒè¯­ä¹‰å…³ç³»"""
    formatted_parts = []

    # è·å–è¦å¤„ç†çš„è¡ŒèŒƒå›´
    end_row = min(start_row + max_rows, len(df))
    subset_df = df.iloc[start_row:end_row]

    formatted_parts.append(f"=== æ•°æ®è®°å½• (ç¬¬{start_row + 1}è¡Œåˆ°ç¬¬{end_row}è¡Œ) ===")

    for idx, (_, row) in enumerate(subset_df.iterrows(), start=start_row + 1):
        # æ„å»ºæ›´è‡ªç„¶çš„æè¿°
        record_description = f"è®°å½•{idx}ï¼š"

        # ä¼˜å…ˆæ˜¾ç¤ºé‡è¦çš„è¯†åˆ«ä¿¡æ¯ï¼ˆé€šå¸¸æ˜¯å‰å‡ åˆ—æˆ–æ–‡æœ¬åˆ—ï¼‰
        primary_fields = []
        secondary_fields = []

        for col_name, value in row.items():
            if pd.isna(value) or str(value).strip() == '':
                continue

            formatted_value = format_cell_value(value, structure_info['column_types'].get(col_name, 'text'))
            field_text = f"{col_name}ä¸º{formatted_value}"

            # åˆ†ç±»åˆ—å’Œä¸»è¦æ–‡æœ¬åˆ—ä½œä¸ºä¸»è¦å­—æ®µ
            if (col_name in structure_info.get('categorical_columns', []) or
                    col_name in structure_info.get('text_columns', [])[:2]):  # å‰ä¸¤ä¸ªæ–‡æœ¬åˆ—
                primary_fields.append(field_text)
            else:
                secondary_fields.append(field_text)

        # ç»„åˆå­—æ®µæè¿°
        if primary_fields:
            record_description += "ï¼Œ".join(primary_fields)
            if secondary_fields:
                record_description += "ï¼›å…¶ä»–ä¿¡æ¯ï¼š" + "ï¼Œ".join(secondary_fields[:5])  # é™åˆ¶æ˜¾ç¤ºæ•°é‡
        else:
            record_description += "ï¼Œ".join(secondary_fields[:8])  # å¦‚æœæ²¡æœ‰ä¸»è¦å­—æ®µï¼Œæ˜¾ç¤ºæ›´å¤šæ¬¡è¦å­—æ®µ

        formatted_parts.append(record_description)

    return "\n".join(formatted_parts)


def format_cell_value(value: Any, col_type: str) -> str:
    """æ ¹æ®åˆ—ç±»å‹æ™ºèƒ½æ ¼å¼åŒ–å•å…ƒæ ¼å€¼"""
    if pd.isna(value):
        return "ç©ºå€¼"

    if col_type == 'numeric':
        if isinstance(value, (int, float)):
            if float(value).is_integer():
                return f"{int(value)}"
            else:
                return f"{float(value):.2f}".rstrip('0').rstrip('.')
    elif col_type == 'date':
        if isinstance(value, pd.Timestamp):
            return value.strftime('%Yå¹´%mæœˆ%dæ—¥')
        elif isinstance(value, str):
            # å°è¯•è§£ææ—¥æœŸå­—ç¬¦ä¸²
            try:
                parsed_date = pd.to_datetime(value)
                return parsed_date.strftime('%Yå¹´%mæœˆ%dæ—¥')
            except:
                pass

    # é»˜è®¤è½¬ä¸ºå­—ç¬¦ä¸²ï¼Œå¹¶æ¸…ç†
    str_value = str(value).strip()
    # ç§»é™¤è¿‡é•¿çš„æ–‡æœ¬ï¼Œä¿ç•™é‡è¦ä¿¡æ¯
    if len(str_value) > 50:
        str_value = str_value[:47] + "..."

    return str_value


def create_column_relationship_text(df: pd.DataFrame, structure_info: Dict[str, Any]) -> str:
    """åˆ›å»ºåˆ—ä¹‹é—´å…³ç³»çš„æè¿°æ–‡æœ¬"""
    relationship_parts = ["=== æ•°æ®å…³ç³»åˆ†æ ==="]

    # åˆ†æåˆ†ç±»åˆ—çš„å€¼åˆ†å¸ƒ
    for col in structure_info.get('categorical_columns', [])[:3]:  # æœ€å¤šåˆ†æ3ä¸ªåˆ†ç±»åˆ—
        value_counts = df[col].value_counts().head(5)
        if len(value_counts) > 0:
            value_desc = "ã€".join([f"{val}({count}æ¬¡)" for val, count in value_counts.items()])
            relationship_parts.append(f"{col}å­—æ®µçš„ä¸»è¦å€¼åŒ…æ‹¬ï¼š{value_desc}")

    # åˆ†ææ•°å€¼åˆ—çš„èŒƒå›´å’Œåˆ†å¸ƒ
    for col in structure_info.get('numeric_columns', [])[:3]:  # æœ€å¤šåˆ†æ3ä¸ªæ•°å€¼åˆ—
        col_data = df[col].dropna()
        if len(col_data) > 0:
            q25, q75 = col_data.quantile([0.25, 0.75])
            relationship_parts.append(f"{col}å­—æ®µæ•°å€¼åˆ†å¸ƒï¼š25%åˆ†ä½æ•°ä¸º{q25:.2f}ï¼Œ75%åˆ†ä½æ•°ä¸º{q75:.2f}")

    # å°è¯•å‘ç°åˆ—ä¹‹é—´çš„å…³è”ï¼ˆç®€å•çš„ç›¸å…³æ€§åˆ†æï¼‰
    numeric_cols = structure_info.get('numeric_columns', [])
    if len(numeric_cols) >= 2:
        correlations = []
        for i in range(len(numeric_cols)):
            for j in range(i + 1, min(i + 3, len(numeric_cols))):  # åªæ£€æŸ¥ä¸´è¿‘çš„å‡ åˆ—
                col1, col2 = numeric_cols[i], numeric_cols[j]
                corr = df[col1].corr(df[col2])
                if not pd.isna(corr) and abs(corr) > 0.5:
                    correlation_desc = "å¼ºæ­£ç›¸å…³" if corr > 0.5 else "å¼ºè´Ÿç›¸å…³"
                    correlations.append(f"{col1}ä¸{col2}å­˜åœ¨{correlation_desc}(ç›¸å…³ç³»æ•°{corr:.2f})")

        if correlations:
            relationship_parts.append("å‘ç°çš„æ•°æ®å…³è”ï¼š")
            relationship_parts.extend([f"  {corr}" for corr in correlations])

    return "\n".join(relationship_parts)


def split_excel_data_semantically(df: pd.DataFrame, structure_info: Dict[str, Any], metadata: dict) -> List[Document]:
    """åŸºäºè¯­ä¹‰ç›¸å…³æ€§æ™ºèƒ½åˆ†å—Excelæ•°æ®"""
    documents = []

    # ç­–ç•¥1ï¼šåˆ›å»ºæ•´ä½“æ‘˜è¦æ–‡æ¡£
    summary_text = create_table_summary(df, metadata.get('sheet_name', ''), structure_info)
    summary_doc = Document(
        page_content=summary_text,
        metadata={
            **metadata,
            'content_type': 'table_summary',
            'chunk_strategy': 'summary',
            'file_type': 'excel'
        }
    )
    documents.append(summary_doc)

    # ç­–ç•¥2ï¼šåˆ›å»ºåˆ—å…³ç³»åˆ†ææ–‡æ¡£
    if len(df) > 5:  # åªæœ‰è¶³å¤Ÿæ•°æ®æ—¶æ‰åˆ†æå…³ç³»
        relationship_text = create_column_relationship_text(df, structure_info)
        if len(relationship_text) > 100:  # ç¡®ä¿æœ‰è¶³å¤Ÿå†…å®¹
            relationship_doc = Document(
                page_content=relationship_text,
                metadata={
                    **metadata,
                    'content_type': 'column_relationships',
                    'chunk_strategy': 'relationship_analysis',
                    'file_type': 'excel'
                }
            )
            documents.append(relationship_doc)

    # ç­–ç•¥3ï¼šæŒ‰è¯­ä¹‰ç»„åˆ†å—æ•°æ®è¡Œ
    rows_per_chunk = calculate_optimal_chunk_size(structure_info)
    total_rows = len(df)

    for start_row in range(0, total_rows, rows_per_chunk):
        # åˆ›å»ºæ•°æ®å—
        data_text = format_table_data_intelligently(df, structure_info, start_row, rows_per_chunk)

        if is_valid_content(data_text, min_length=50):
            chunk_metadata = {
                **metadata,
                'content_type': 'table_data',
                'chunk_strategy': 'semantic_rows',
                'start_row': start_row + 1,
                'end_row': min(start_row + rows_per_chunk, total_rows),
                'chunk_size': min(rows_per_chunk, total_rows - start_row),
                'file_type': 'excel'
            }

            data_doc = Document(
                page_content=data_text,
                metadata=chunk_metadata
            )
            documents.append(data_doc)

    # ç­–ç•¥4ï¼šå¦‚æœæœ‰å¤šä¸ªé‡è¦çš„åˆ†ç±»åˆ—ï¼Œåˆ›å»ºåˆ†ç±»èšåˆæ–‡æ¡£
    for cat_col in structure_info.get('categorical_columns', [])[:2]:  # æœ€å¤šå¤„ç†2ä¸ªåˆ†ç±»åˆ—
        category_docs = create_category_aggregation_docs(df, cat_col, structure_info, metadata)
        documents.extend(category_docs)

    return documents


def calculate_optimal_chunk_size(structure_info: Dict[str, Any]) -> int:
    """æ ¹æ®è¡¨æ ¼ç»“æ„è®¡ç®—æœ€ä¼˜çš„åˆ†å—å¤§å°"""
    base_chunk_size = 15  # åŸºç¡€è¡Œæ•°

    # æ ¹æ®åˆ—æ•°è°ƒæ•´
    col_count = structure_info['col_count']
    if col_count > 10:
        base_chunk_size = max(8, base_chunk_size - (col_count - 10) * 2)
    elif col_count < 5:
        base_chunk_size = min(25, base_chunk_size + (5 - col_count) * 3)

    # æ ¹æ®æ•°æ®å¯†åº¦è°ƒæ•´
    density = structure_info['data_density']
    if density < 0.7:  # æ•°æ®ç¨€ç–æ—¶å¢åŠ è¡Œæ•°
        base_chunk_size = int(base_chunk_size * 1.5)
    elif density > 0.95:  # æ•°æ®å¯†é›†æ—¶å‡å°‘è¡Œæ•°
        base_chunk_size = int(base_chunk_size * 0.8)

    return max(5, min(30, base_chunk_size))  # é™åˆ¶åœ¨5-30è¡Œä¹‹é—´


def create_category_aggregation_docs(df: pd.DataFrame, cat_col: str, structure_info: Dict[str, Any], metadata: dict) -> \
List[Document]:
    """ä¸ºé‡è¦åˆ†ç±»åˆ—åˆ›å»ºèšåˆæ–‡æ¡£"""
    documents = []

    # è·å–åˆ†ç±»å€¼çš„åˆ†å¸ƒ
    category_counts = df[cat_col].value_counts()

    # åªå¤„ç†å‡ºç°é¢‘ç‡è¾ƒé«˜çš„åˆ†ç±»
    for category, count in category_counts.head(5).items():
        if pd.isna(category) or count < 2:
            continue

        # è·å–è¯¥åˆ†ç±»ä¸‹çš„æ‰€æœ‰è®°å½•
        category_data = df[df[cat_col] == category]

        # åˆ›å»ºè¯¥åˆ†ç±»çš„æè¿°æ–‡æ¡£
        category_text_parts = [
            f"=== {cat_col}ï¼š{category} çš„ç›¸å…³è®°å½• ===",
            f"è¯¥åˆ†ç±»åŒ…å«{count}æ¡è®°å½•ï¼Œå æ€»æ•°æ®çš„{count / len(df):.1%}"
        ]

        # æ·»åŠ è¯¥åˆ†ç±»ä¸‹å…¶ä»–åˆ—çš„ç‰¹å¾æè¿°
        for col in category_data.columns:
            if col == cat_col:
                continue

            col_type = structure_info['column_types'].get(col, 'text')
            if col_type == 'numeric':
                numeric_data = category_data[col].dropna()
                if len(numeric_data) > 0:
                    mean_val = numeric_data.mean()
                    category_text_parts.append(f"åœ¨{col}æ–¹é¢ï¼šå¹³å‡å€¼ä¸º{mean_val:.2f}")
            elif col_type in ['text', 'categorical']:
                text_data = category_data[col].dropna()
                if len(text_data) > 0:
                    most_common = text_data.mode()
                    if len(most_common) > 0:
                        category_text_parts.append(f"åœ¨{col}æ–¹é¢ï¼šæœ€å¸¸è§çš„æ˜¯{most_common.iloc[0]}")

        # æ·»åŠ å‡ ä¸ªå…·ä½“çš„è®°å½•ç¤ºä¾‹
        sample_records = []
        for idx, (_, row) in enumerate(category_data.head(3).iterrows()):
            record_parts = []
            for col, val in row.items():
                if col != cat_col and not pd.isna(val) and str(val).strip():
                    formatted_val = format_cell_value(val, structure_info['column_types'].get(col, 'text'))
                    record_parts.append(f"{col}ä¸º{formatted_val}")

            if record_parts:
                sample_records.append(f"ç¤ºä¾‹{idx + 1}ï¼š" + "ï¼Œ".join(record_parts[:4]))

        if sample_records:
            category_text_parts.append("å…·ä½“è®°å½•ç¤ºä¾‹ï¼š")
            category_text_parts.extend(sample_records)

        category_text = "\n".join(category_text_parts)

        if is_valid_content(category_text, min_length=80):
            category_doc = Document(
                page_content=category_text,
                metadata={
                    **metadata,
                    'content_type': 'category_aggregation',
                    'chunk_strategy': 'category_based',
                    'category_column': cat_col,
                    'category_value': str(category),
                    'record_count': count,
                    'file_type': 'excel'
                }
            )
            documents.append(category_doc)

    return documents


def load_excel_with_enhanced_strategies(excel_path: str) -> List[Document]:
    """ä½¿ç”¨å¢å¼ºç­–ç•¥åŠ è½½Excelæ–‡ä»¶ï¼Œä¼˜åŒ–embeddingæ•ˆæœ"""
    documents = []

    try:
        print(f"  æ­£åœ¨ä½¿ç”¨å¢å¼ºç­–ç•¥åŠ è½½Excelæ–‡ä»¶: {os.path.basename(excel_path)}")

        # è¯»å–Excelæ–‡ä»¶ï¼Œè·å–æ‰€æœ‰å·¥ä½œè¡¨
        excel_file = pd.ExcelFile(excel_path)
        print(f"  å‘ç° {len(excel_file.sheet_names)} ä¸ªå·¥ä½œè¡¨: {excel_file.sheet_names}")

        for sheet_name in excel_file.sheet_names:
            try:
                print(f"    å¤„ç†å·¥ä½œè¡¨: {sheet_name}")

                # è¯»å–å·¥ä½œè¡¨æ•°æ®ï¼Œä¿æŒæ•°æ®ç±»å‹
                df = pd.read_excel(excel_path, sheet_name=sheet_name)

                if df.empty:
                    print(f"    å·¥ä½œè¡¨ {sheet_name} ä¸ºç©ºï¼Œè·³è¿‡")
                    continue

                # æ¸…ç†åˆ—å
                df.columns = [str(col).strip() for col in df.columns]

                # åˆ†æè¡¨æ ¼ç»“æ„
                structure_info = analyze_excel_structure(df)
                print(f"    è¡¨æ ¼ç»“æ„åˆ†æï¼š{structure_info['row_count']}è¡ŒÃ—{structure_info['col_count']}åˆ—ï¼Œ"
                      f"æ•°æ®å¯†åº¦{structure_info['data_density']:.1%}")
                print(f"    åˆ—ç±»å‹åˆ†å¸ƒï¼šæ•°å€¼{len(structure_info['numeric_columns'])}ä¸ªï¼Œ"
                      f"æ–‡æœ¬{len(structure_info['text_columns'])}ä¸ªï¼Œ"
                      f"åˆ†ç±»{len(structure_info['categorical_columns'])}ä¸ª")

                # åŸºç¡€å…ƒæ•°æ®ï¼ˆåªåŒ…å«ç®€å•æ•°æ®ç±»å‹ï¼‰
                base_metadata = {
                    "source": excel_path,
                    "source_file": os.path.basename(excel_path),
                    "sheet_name": sheet_name,
                    "extraction_method": "enhanced_pandas_excel",
                    "rows_count": structure_info['row_count'],
                    "columns_count": structure_info['col_count'],
                    "data_density": structure_info['data_density'],
                    "numeric_columns_count": len(structure_info['numeric_columns']),
                    "text_columns_count": len(structure_info['text_columns']),
                    "categorical_columns_count": len(structure_info['categorical_columns']),
                    "date_columns_count": len(structure_info['date_columns']),
                    "file_type": "excel"
                }

                # ä½¿ç”¨è¯­ä¹‰åˆ†å—ç­–ç•¥
                sheet_documents = split_excel_data_semantically(df, structure_info, base_metadata)

                print(f"    å·¥ä½œè¡¨ {sheet_name} ç”Ÿæˆäº† {len(sheet_documents)} ä¸ªè¯­ä¹‰æ–‡æ¡£å—")
                documents.extend(sheet_documents)

            except Exception as e:
                print(f"    å¤„ç†å·¥ä½œè¡¨ {sheet_name} æ—¶å‡ºé”™: {e}")
                continue

        if documents:
            print(f"  æˆåŠŸä»Excelæ–‡ä»¶æå–äº† {len(documents)} ä¸ªå¢å¼ºå‹æ–‡æ¡£å—")
        else:
            print(f"  Excelæ–‡ä»¶æœªæå–åˆ°æœ‰æ•ˆå†…å®¹")

    except Exception as e:
        print(f"  åŠ è½½Excelæ–‡ä»¶å¤±è´¥: {e}")

    return documents


# =============================================================================
# ç»Ÿä¸€çš„æ–‡æ¡£å¤„ç†å’Œå‘é‡åŒ–ä¸»å‡½æ•°
# =============================================================================

def process_all_documents():
    """QAå¢å¼ºçš„æ–‡æ¡£å¤„ç†ä¸»å‡½æ•°"""

    print("ğŸš€ å¼€å§‹QAå¢å¼ºçš„æ–‡æ¡£å¤„ç†æµç¨‹")
    print("=" * 80)

    # åˆå§‹åŒ–embeddingså’Œå‘é‡æ•°æ®åº“
    print("ğŸŒ æ­£åœ¨åˆå§‹åŒ–é˜¿é‡Œäº‘DashScope Embedding APIè¿æ¥...")
    
    # ä½¿ç”¨é˜¿é‡Œäº‘çš„text-embedding-v4æ¨¡å‹ï¼ˆè‡ªå®šä¹‰ç›´è¿DashScopeä»¥é¿å…å…¼å®¹å±‚æ ¼å¼é—®é¢˜ï¼‰
    embeddings = DashScopeEmbeddings(
        api_key=dashscope_api_key,
        model="text-embedding-v4",
        api_base="https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings",
        request_timeout_seconds=30,
        sleep_between_requests_seconds=0.05,
    )
    print("âœ… æˆåŠŸåˆå§‹åŒ–é˜¿é‡Œäº‘text-embedding-v4ï¼ˆç›´è¿DashScopeå…¼å®¹ç«¯ç‚¹ï¼‰")

    # åˆ›å»ºå‘é‡æ•°æ®åº“
    vector_store = Chroma(
        collection_name="aliyun_text_embedding_v4_collection",
        embedding_function=embeddings,
        persist_directory="./chroma_langchain_db",
    )
    print(f"ğŸ“¦ å‘é‡æ•°æ®åº“åˆå§‹åŒ–å®Œæˆï¼šaliyun_text_embedding_v4_collection")
    print(f"ğŸ’¾ å­˜å‚¨ä½ç½®ï¼š./chroma_langchain_db")
    print(f"ğŸ¯ ä½¿ç”¨æ¨¡å‹ï¼štext-embedding-v4")

    all_documents = []
    all_qa_documents = []

    # =============================================================================
    # æ­¥éª¤1ï¼šåˆ›å»ºé¢„å®šä¹‰ç›®æ ‡QAæ–‡æ¡£
    # =============================================================================

    print("\n" + "=" * 60)
    print("ğŸ“‹ æ­¥éª¤1ï¼šåˆ›å»ºé¢„å®šä¹‰ç›®æ ‡QAæ–‡æ¡£é›†åˆ")
    print("=" * 60)

    target_qa_docs = create_target_qa_documents()
    all_qa_documents.extend(target_qa_docs)

    # =============================================================================
    # æ­¥éª¤2ï¼šå¤„ç†PDFæ–‡ä»¶å¹¶ç”ŸæˆQA
    # =============================================================================

    print("\n" + "=" * 60)
    print("ğŸ“„ æ­¥éª¤2ï¼šå¤„ç†PDFæ–‡ä»¶å¹¶ç”ŸæˆQAå¯¹")
    print("=" * 60)

    pdf_folder = "./datas/"
    if os.path.exists(pdf_folder):
        pdf_files = glob.glob(os.path.join(pdf_folder, "*.pdf"))
        print(f"ğŸ” æ‰¾åˆ° {len(pdf_files)} ä¸ªPDFæ–‡ä»¶:")
        for pdf_file in pdf_files:
            print(f"  - {pdf_file}")

        pdf_documents = []

        # å¤„ç†æ¯ä¸ªPDFæ–‡ä»¶
        for pdf_index, pdf_file in enumerate(pdf_files, 1):
            print(f"\nğŸ“„ æ­£åœ¨å¤„ç†ç¬¬ {pdf_index}/{len(pdf_files)} ä¸ªPDFæ–‡ä»¶: {os.path.basename(pdf_file)}")

            # ä½¿ç”¨æ”¹è¿›çš„åŠ è½½ç­–ç•¥
            documents = load_pdf_with_multiple_strategies(pdf_file)

            if not documents:
                print(f"  è·³è¿‡PDFæ–‡ä»¶ {os.path.basename(pdf_file)}ï¼šæ— æ³•æå–æœ‰æ•ˆå†…å®¹")
                continue

            # ä½¿ç”¨ç« èŠ‚åˆ‡åˆ†ç­–ç•¥
            for doc in documents:
                splits = split_by_chapters(doc.page_content, {
                    'source': pdf_file,
                    'source_file': os.path.basename(pdf_file),
                    'extraction_method': doc.metadata.get('extraction_method', 'unknown'),
                    'file_type': 'pdf'
                })

                print(f"  æŒ‰ç« èŠ‚åˆ†å‰²æˆ {len(splits)} ä¸ªæ–‡æ¡£å—")

                # è¿‡æ»¤å’Œæ¸…ç†æ–‡æ¡£
                valid_splits = []
                for split_doc in splits:
                    # é¦–å…ˆæ¸…ç†å†…å®¹
                    cleaned_content = clean_text(split_doc.page_content)
                    split_doc.page_content = cleaned_content

                    # ç„¶åéªŒè¯æ˜¯å¦æœ‰æ•ˆ
                    if is_valid_content(cleaned_content) and validate_document_before_embedding(split_doc):
                        split_doc.metadata.update({
                            'content_length': len(cleaned_content),
                        })
                        valid_splits.append(split_doc)

                print(f"  è¿‡æ»¤åè·å¾— {len(valid_splits)} ä¸ªé«˜è´¨é‡PDFç« èŠ‚å—")
                all_documents.extend(valid_splits)

                # ä¸ºæ¯ä¸ªæœ‰æ•ˆçš„æ–‡æ¡£å—ç”ŸæˆQAå¯¹
                print(f"  å¼€å§‹ä¸ºPDFç« èŠ‚ç”ŸæˆQAå¯¹...")
                pdf_qa_docs = []

                for i, split_doc in enumerate(valid_splits[:3]):  # é™åˆ¶å‰3ä¸ªæ–‡æ¡£å—ç”ŸæˆQA
                    print(f"    ä¸ºç¬¬ {i + 1}/{min(3, len(valid_splits))} ä¸ªæ–‡æ¡£å—ç”ŸæˆQAå¯¹")
                    qa_docs = generate_qa_pairs_from_document(split_doc)
                    pdf_qa_docs.extend(qa_docs)

                    # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
                    if qa_docs:
                        time.sleep(1)

                print(f"  PDFæ–‡ä»¶å…±ç”Ÿæˆ {len(pdf_qa_docs)} ä¸ªQAæ–‡æ¡£")
                all_qa_documents.extend(pdf_qa_docs)

        print(
            f"\nğŸ“Š PDFå¤„ç†å®Œæˆï¼Œå…±è·å¾— {len([d for d in all_documents if d.metadata.get('file_type') == 'pdf'])} ä¸ªPDFæ–‡æ¡£å—")
    else:
        print(f"âš ï¸ PDFæ–‡ä»¶å¤¹ {pdf_folder} ä¸å­˜åœ¨ï¼Œè·³è¿‡PDFå¤„ç†")

    # =============================================================================
    # æ­¥éª¤3ï¼šå¤„ç†Excelæ–‡ä»¶å¹¶ç”ŸæˆQA
    # =============================================================================

    print("\n" + "=" * 60)
    print("ğŸ“Š æ­¥éª¤3ï¼šå¤„ç†Excelæ–‡ä»¶å¹¶ç”ŸæˆQAå¯¹")
    print("=" * 60)

    excel_folder = "./datas/"
    if os.path.exists(excel_folder):
        excel_files = []
        for ext in ['*.xlsx', '*.xls']:
            excel_files.extend(glob.glob(os.path.join(excel_folder, ext)))

        print(f"\nğŸ“Š æ‰¾åˆ° {len(excel_files)} ä¸ªExcelæ–‡ä»¶:")
        for excel_file in excel_files:
            print(f"  - {excel_file}")

        excel_documents = []

        # ä½¿ç”¨å¢å¼ºç­–ç•¥å¤„ç†æ¯ä¸ªExcelæ–‡ä»¶
        for excel_index, excel_file in enumerate(excel_files, 1):
            print(f"\nğŸ“ˆ æ­£åœ¨å¤„ç†ç¬¬ {excel_index}/{len(excel_files)} ä¸ªExcelæ–‡ä»¶: {os.path.basename(excel_file)}")

            # ä½¿ç”¨å¢å¼ºçš„ExcelåŠ è½½ç­–ç•¥
            documents = load_excel_with_enhanced_strategies(excel_file)

            if not documents:
                print(f"  è·³è¿‡Excelæ–‡ä»¶ {os.path.basename(excel_file)}ï¼šæ— æ³•æå–æœ‰æ•ˆå†…å®¹")
                continue

            # è¿‡æ»¤å’ŒéªŒè¯æ–‡æ¡£
            valid_documents = []
            for doc in documents:
                # æ¸…ç†å†…å®¹
                cleaned_content = clean_text(doc.page_content)
                doc.page_content = cleaned_content

                # éªŒè¯å†…å®¹è´¨é‡
                if is_valid_content(cleaned_content) and validate_document_before_embedding(doc, min_length=50):
                    doc.metadata.update({
                        'content_length': len(cleaned_content),
                        'processing_version': 'enhanced_v2'
                    })
                    valid_documents.append(doc)

            print(f"  æ–‡ä»¶å¤„ç†å®Œæˆï¼Œè·å¾— {len(valid_documents)} ä¸ªé«˜è´¨é‡Excelæ–‡æ¡£å—")
            all_documents.extend(valid_documents)

            # ä¸ºExcelæ–‡æ¡£ç”ŸæˆQAå¯¹
            print(f"  å¼€å§‹ä¸ºExcelæ–‡æ¡£ç”ŸæˆQAå¯¹...")
            excel_qa_docs = []

            for i, doc in enumerate(valid_documents):
                print(f"    ä¸ºç¬¬ {i + 1}/{len(valid_documents)} ä¸ªExcelæ–‡æ¡£ç”ŸæˆQAå¯¹")
                qa_docs = generate_qa_pairs_from_document(doc)
                excel_qa_docs.extend(qa_docs)

                # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
                if qa_docs:
                    time.sleep(1)

            print(f"  Excelæ–‡ä»¶å…±ç”Ÿæˆ {len(excel_qa_docs)} ä¸ªQAæ–‡æ¡£")
            all_qa_documents.extend(excel_qa_docs)

        print(
            f"\nğŸ“Š Excelå¤„ç†å®Œæˆï¼Œå…±è·å¾— {len([d for d in all_documents if d.metadata.get('file_type') == 'excel'])} ä¸ªExcelæ–‡æ¡£å—")
    else:
        print(f"âš ï¸ Excelæ–‡ä»¶å¤¹ {excel_folder} ä¸å­˜åœ¨ï¼Œè·³è¿‡Excelå¤„ç†")

    # =============================================================================
    # æ­¥éª¤4ï¼šç»Ÿä¸€å…¥åº“åˆ°å‘é‡æ•°æ®åº“
    # =============================================================================

    print("\n" + "=" * 60)
    print("ğŸ¯ æ­¥éª¤4ï¼šç»Ÿä¸€å…¥åº“åˆ°å‘é‡æ•°æ®åº“")
    print("=" * 60)

    # åˆå¹¶æ‰€æœ‰æ–‡æ¡£
    all_final_documents = all_documents + all_qa_documents

    print(f"\nğŸ“Š æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯ï¼š")
    print(f"  - åŸå§‹æ–‡æ¡£å—: {len(all_documents)} ä¸ª")
    print(f"  - QAæ–‡æ¡£å—: {len(all_qa_documents)} ä¸ª")
    print(f"    - é¢„å®šä¹‰ç›®æ ‡QA: {len(target_qa_docs)} ä¸ª")
    print(f"    - ç”Ÿæˆçš„QA: {len(all_qa_documents) - len(target_qa_docs)} ä¸ª")
    print(f"  - æ–‡æ¡£æ€»è®¡: {len(all_final_documents)} ä¸ª")

    if len(all_final_documents) > 0:
        # æ˜¾ç¤ºå¤„ç†æ ·æœ¬
        print(f"\n=== ğŸ“‹ QAå¢å¼ºæ–‡æ¡£å¤„ç†æ ·æœ¬é¢„è§ˆ ===")

        # æŒ‰æ–‡æ¡£ç±»å‹åˆ†ç»„æ˜¾ç¤ºæ ·æœ¬
        doc_types = {}
        for doc in all_final_documents:
            doc_type = doc.metadata.get('content_type', doc.metadata.get('file_type', 'unknown'))
            if doc_type not in doc_types:
                doc_types[doc_type] = []
            doc_types[doc_type].append(doc)

        for doc_type, docs in list(doc_types.items())[:5]:  # æ˜¾ç¤ºå‰5ç§ç±»å‹
            print(f"\n--- {doc_type} ç±»å‹æ ·æœ¬ ---")
            sample_doc = docs[0]
            print(f"æ¥æº: {sample_doc.metadata.get('source_file', 'unknown')}")
            print(f"å†…å®¹ç±»å‹: {sample_doc.metadata.get('content_type', 'unknown')}")

            if sample_doc.metadata.get('content_type') in ['generated_qa', 'target_qa']:
                print(f"é—®é¢˜: {sample_doc.metadata.get('question', 'æœªçŸ¥')[:50]}...")
                print(f"ç”Ÿæˆæ–¹æ³•: {sample_doc.metadata.get('generation_method', 'unknown')}")
            else:
                print(f"æ–‡ä»¶ç±»å‹: {sample_doc.metadata.get('file_type', 'unknown')}")

            print(f"é•¿åº¦: {sample_doc.metadata.get('content_length', len(sample_doc.page_content))}")
            print(f"å†…å®¹é¢„è§ˆ: {sample_doc.page_content[:150]}...")
            print("-" * 50)

        # åˆ†æ‰¹æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
        batch_size = 5
        total_batches = (len(all_final_documents) + batch_size - 1) // batch_size
        successful_docs = 0

        for i in range(0, len(all_final_documents), batch_size):
            batch = all_final_documents[i:i + batch_size]
            batch_num = i // batch_size + 1
            print(f"\nğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ {batch_num}/{total_batches} æ‰¹æ–‡æ¡£ï¼ŒåŒ…å« {len(batch)} ä¸ªæ–‡æ¡£")

            # æœ€ç»ˆéªŒè¯
            validated_batch = []
            for j, doc in enumerate(batch):
                content_length = len(doc.page_content.strip())
                content_type = doc.metadata.get('content_type', doc.metadata.get('file_type', 'unknown'))
                print(
                    f"    æ–‡æ¡£ {j + 1}: {content_type}, é•¿åº¦={content_length}, æ¥æº={doc.metadata.get('source_file', 'unknown')}")

                min_length = 30 if content_type in ['generated_qa', 'target_qa'] else 50
                if validate_document_before_embedding(doc, min_length=min_length):
                    validated_batch.append(doc)
                else:
                    print(f"    âš ï¸ è·³è¿‡æ— æ•ˆæ–‡æ¡£ {j + 1}")

            if not validated_batch:
                print(f"    æ‰¹æ¬¡ {batch_num} æ²¡æœ‰æœ‰æ•ˆæ–‡æ¡£ï¼Œè·³è¿‡")
                continue

            print(f"    éªŒè¯åæ‰¹æ¬¡åŒ…å« {len(validated_batch)} ä¸ªæœ‰æ•ˆæ–‡æ¡£")

            try:
                # ç¡®ä¿æ‰€æœ‰æ–‡æ¡£å†…å®¹éƒ½æ˜¯å­—ç¬¦ä¸²æ ¼å¼
                cleaned_batch = []
                for doc in validated_batch:
                    # ç¡®ä¿page_contentæ˜¯å­—ç¬¦ä¸²
                    if not isinstance(doc.page_content, str):
                        doc.page_content = str(doc.page_content)
                    # æ¸…ç†å†…å®¹ï¼Œç§»é™¤å¯èƒ½çš„ç‰¹æ®Šå­—ç¬¦
                    doc.page_content = doc.page_content.strip()
                    if doc.page_content:
                        cleaned_batch.append(doc)
                
                if cleaned_batch:
                    _ = vector_store.add_documents(documents=cleaned_batch)
                    successful_docs += len(cleaned_batch)
                    print(f"âœ… æˆåŠŸæ·»åŠ ç¬¬ {batch_num} æ‰¹æ–‡æ¡£")
                else:
                    print(f"âš ï¸ æ‰¹æ¬¡ {batch_num} æ¸…ç†åæ²¡æœ‰æœ‰æ•ˆæ–‡æ¡£")
            except Exception as e:
                print(f"âŒ æ·»åŠ ç¬¬ {batch_num} æ‰¹æ–‡æ¡£æ—¶å‡ºé”™: {e}")
                # é€ä¸ªå°è¯•æ·»åŠ 
                for k, doc in enumerate(validated_batch):
                    try:
                        # ç¡®ä¿å•ä¸ªæ–‡æ¡£å†…å®¹æ ¼å¼æ­£ç¡®
                        if not isinstance(doc.page_content, str):
                            doc.page_content = str(doc.page_content)
                        doc.page_content = doc.page_content.strip()
                        
                        if doc.page_content:
                            vector_store.add_documents(documents=[doc])
                            successful_docs += 1
                            print(f"      âœ… æˆåŠŸæ·»åŠ å•ä¸ªæ–‡æ¡£ {k + 1}")
                        else:
                            print(f"      âš ï¸ æ–‡æ¡£ {k + 1} å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡")
                    except Exception as single_e:
                        print(f"      âŒ æ–‡æ¡£ {k + 1} æ·»åŠ å¤±è´¥: {single_e}")

        print(f"\nğŸ‰ QAå¢å¼ºå¤„ç†å®Œæˆï¼")
        print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯ï¼š")
        pdf_count = len(glob.glob(os.path.join('./datas/', '*.pdf'))) if os.path.exists('./datas/') else 0
        excel_count = len([f for ext in ['*.xlsx', '*.xls'] for f in
                           glob.glob(os.path.join('./datas/', ext))]) if os.path.exists('./datas/') else 0
        print(f"  - å¤„ç†çš„PDFæ–‡ä»¶æ•°: {pdf_count}")
        print(f"  - å¤„ç†çš„Excelæ–‡ä»¶æ•°: {excel_count}")
        print(f"  - åŸå§‹æ–‡æ¡£å—æ€»æ•°: {len(all_documents)}")
        print(f"  - QAæ–‡æ¡£å—æ€»æ•°: {len(all_qa_documents)}")
        print(f"  - ç”Ÿæˆçš„æ–‡æ¡£å—æ€»æ•°: {len(all_final_documents)}")
        print(f"  - æˆåŠŸæ·»åŠ åˆ°å‘é‡åº“: {successful_docs}")
        print(f"ğŸ” ä½¿ç”¨äº†QAæ•°æ®å¢å¼ºç­–ç•¥ï¼šåŸå§‹æ–‡æ¡£ + ç”ŸæˆQA + ç›®æ ‡QA")
        print(f"ğŸ’¾ å‘é‡åº“ä¿å­˜ä½ç½®: ./chroma_langchain_db")
        print(f"ğŸ“ˆ å‘é‡åº“é›†åˆåç§°: aliyun_text_embedding_v4_collection")
        print(f"ğŸ¯ ä½¿ç”¨æ¨¡å‹: text-embedding-v4")
        print(f"ğŸŒ è¿è¡Œæ¨¡å¼: é˜¿é‡Œäº‘DashScope API")
        print(f"ğŸ† æ¨¡å‹æ€§èƒ½: é˜¿é‡Œäº‘text-embedding-v4ï¼Œé«˜è´¨é‡å‘é‡è¡¨ç¤º")
        
        print(f"\nğŸ‰ åŸºäºé˜¿é‡Œäº‘text-embedding-v4çš„QAå¢å¼ºå¤„ç†å®Œæˆï¼")
        print(f"ğŸ¯ ä½¿ç”¨æ¨¡å‹ï¼štext-embedding-v4")
        print(f"ğŸŒ æœåŠ¡æ¨¡å¼ï¼šé˜¿é‡Œäº‘DashScope API")
        print(f"ğŸ† æ¨¡å‹æ€§èƒ½ï¼šé˜¿é‡Œäº‘text-embedding-v4ï¼Œé«˜è´¨é‡å‘é‡è¡¨ç¤º")
        print(f"ğŸ“ æŠ€æœ¯è§„æ ¼ï¼šé«˜ç»´åº¦å‘é‡åµŒå…¥ï¼Œå¤šè¯­è¨€æ”¯æŒ")
        print(f"ğŸ’ª ä¼˜åŠ¿ç‰¹è‰²ï¼šç¨³å®šäº‘ç«¯æœåŠ¡ï¼Œé«˜ç²¾åº¦å‘é‡è¡¨ç¤ºï¼ŒåŒ»å­¦é¢†åŸŸä¼˜åŒ–")
        
    else:
        print("âŒ æ²¡æœ‰æå–åˆ°æœ‰æ•ˆçš„æ–‡æ¡£å†…å®¹ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„å’Œæ–‡ä»¶æ ¼å¼")




# =============================================================================
# æ‰§è¡Œä¸»å‡½æ•°
# =============================================================================

if __name__ == "__main__":
    # è¿è¡Œæ–‡æ¡£å¤„ç†æµç¨‹
    process_all_documents() 